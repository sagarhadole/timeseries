{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c143492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff4e7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02f94ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "960f35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-sales-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9dfb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba9aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ae18143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=2, parse_dates=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "031cf6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2017-138688</td>\n",
       "      <td>16/06/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036.0</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-10</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-10</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-21</th>\n",
       "      <td>9796</td>\n",
       "      <td>CA-2017-125920</td>\n",
       "      <td>28/05/2017</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SH-19975</td>\n",
       "      <td>Sally Hughsby</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>60610.0</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10003429</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Cardinal HOLDit! Binder Insert Strips,Extra St...</td>\n",
       "      <td>3.7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>9797</td>\n",
       "      <td>CA-2016-128608</td>\n",
       "      <td>17/01/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>CS-12490</td>\n",
       "      <td>Cindy Schnelling</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43615.0</td>\n",
       "      <td>East</td>\n",
       "      <td>OFF-AR-10001374</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "      <td>BIC Brite Liner Highlighters, Chisel Tip</td>\n",
       "      <td>10.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>9798</td>\n",
       "      <td>CA-2016-128608</td>\n",
       "      <td>17/01/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>CS-12490</td>\n",
       "      <td>Cindy Schnelling</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43615.0</td>\n",
       "      <td>East</td>\n",
       "      <td>TEC-PH-10004977</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>GE 30524EE4</td>\n",
       "      <td>235.1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>9799</td>\n",
       "      <td>CA-2016-128608</td>\n",
       "      <td>17/01/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>CS-12490</td>\n",
       "      <td>Cindy Schnelling</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43615.0</td>\n",
       "      <td>East</td>\n",
       "      <td>TEC-PH-10000912</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>Anker 24W Portable Micro USB Car Charger</td>\n",
       "      <td>26.3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>9800</td>\n",
       "      <td>CA-2016-128608</td>\n",
       "      <td>17/01/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>CS-12490</td>\n",
       "      <td>Cindy Schnelling</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43615.0</td>\n",
       "      <td>East</td>\n",
       "      <td>TEC-AC-10000487</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>SanDisk Cruzer 4 GB USB Flash Drive</td>\n",
       "      <td>10.3840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Row ID        Order ID   Ship Date       Ship Mode Customer ID  \\\n",
       "Order Date                                                                   \n",
       "2017-08-11       1  CA-2017-152156  11/11/2017    Second Class    CG-12520   \n",
       "2017-08-11       2  CA-2017-152156  11/11/2017    Second Class    CG-12520   \n",
       "2017-12-06       3  CA-2017-138688  16/06/2017    Second Class    DV-13045   \n",
       "2016-11-10       4  US-2016-108966  18/10/2016  Standard Class    SO-20335   \n",
       "2016-11-10       5  US-2016-108966  18/10/2016  Standard Class    SO-20335   \n",
       "...            ...             ...         ...             ...         ...   \n",
       "2017-05-21    9796  CA-2017-125920  28/05/2017  Standard Class    SH-19975   \n",
       "2016-12-01    9797  CA-2016-128608  17/01/2016  Standard Class    CS-12490   \n",
       "2016-12-01    9798  CA-2016-128608  17/01/2016  Standard Class    CS-12490   \n",
       "2016-12-01    9799  CA-2016-128608  17/01/2016  Standard Class    CS-12490   \n",
       "2016-12-01    9800  CA-2016-128608  17/01/2016  Standard Class    CS-12490   \n",
       "\n",
       "               Customer Name    Segment        Country             City  \\\n",
       "Order Date                                                                \n",
       "2017-08-11       Claire Gute   Consumer  United States        Henderson   \n",
       "2017-08-11       Claire Gute   Consumer  United States        Henderson   \n",
       "2017-12-06   Darrin Van Huff  Corporate  United States      Los Angeles   \n",
       "2016-11-10    Sean O'Donnell   Consumer  United States  Fort Lauderdale   \n",
       "2016-11-10    Sean O'Donnell   Consumer  United States  Fort Lauderdale   \n",
       "...                      ...        ...            ...              ...   \n",
       "2017-05-21     Sally Hughsby  Corporate  United States          Chicago   \n",
       "2016-12-01  Cindy Schnelling  Corporate  United States           Toledo   \n",
       "2016-12-01  Cindy Schnelling  Corporate  United States           Toledo   \n",
       "2016-12-01  Cindy Schnelling  Corporate  United States           Toledo   \n",
       "2016-12-01  Cindy Schnelling  Corporate  United States           Toledo   \n",
       "\n",
       "                 State  Postal Code   Region       Product ID  \\\n",
       "Order Date                                                      \n",
       "2017-08-11    Kentucky      42420.0    South  FUR-BO-10001798   \n",
       "2017-08-11    Kentucky      42420.0    South  FUR-CH-10000454   \n",
       "2017-12-06  California      90036.0     West  OFF-LA-10000240   \n",
       "2016-11-10     Florida      33311.0    South  FUR-TA-10000577   \n",
       "2016-11-10     Florida      33311.0    South  OFF-ST-10000760   \n",
       "...                ...          ...      ...              ...   \n",
       "2017-05-21    Illinois      60610.0  Central  OFF-BI-10003429   \n",
       "2016-12-01        Ohio      43615.0     East  OFF-AR-10001374   \n",
       "2016-12-01        Ohio      43615.0     East  TEC-PH-10004977   \n",
       "2016-12-01        Ohio      43615.0     East  TEC-PH-10000912   \n",
       "2016-12-01        Ohio      43615.0     East  TEC-AC-10000487   \n",
       "\n",
       "                   Category Sub-Category  \\\n",
       "Order Date                                 \n",
       "2017-08-11        Furniture    Bookcases   \n",
       "2017-08-11        Furniture       Chairs   \n",
       "2017-12-06  Office Supplies       Labels   \n",
       "2016-11-10        Furniture       Tables   \n",
       "2016-11-10  Office Supplies      Storage   \n",
       "...                     ...          ...   \n",
       "2017-05-21  Office Supplies      Binders   \n",
       "2016-12-01  Office Supplies          Art   \n",
       "2016-12-01       Technology       Phones   \n",
       "2016-12-01       Technology       Phones   \n",
       "2016-12-01       Technology  Accessories   \n",
       "\n",
       "                                                 Product Name     Sales  \n",
       "Order Date                                                               \n",
       "2017-08-11                  Bush Somerset Collection Bookcase  261.9600  \n",
       "2017-08-11  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
       "2017-12-06  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
       "2016-11-10      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
       "2016-11-10                     Eldon Fold 'N Roll Cart System   22.3680  \n",
       "...                                                       ...       ...  \n",
       "2017-05-21  Cardinal HOLDit! Binder Insert Strips,Extra St...    3.7980  \n",
       "2016-12-01           BIC Brite Liner Highlighters, Chisel Tip   10.3680  \n",
       "2016-12-01                                        GE 30524EE4  235.1880  \n",
       "2016-12-01           Anker 24W Portable Micro USB Car Charger   26.3760  \n",
       "2016-12-01                SanDisk Cruzer 4 GB USB Flash Drive   10.3840  \n",
       "\n",
       "[9800 rows x 17 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "204d38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deb3a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9620a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02 00:00:00</th>\n",
       "      <td>67.625</td>\n",
       "      <td>6787.75</td>\n",
       "      <td>58.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 02:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 06:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 08:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 10:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 14:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 16:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 18:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 20:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 22:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 00:00:00</th>\n",
       "      <td>8423.500</td>\n",
       "      <td>87762.75</td>\n",
       "      <td>275.393875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 02:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 04:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 06:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 08:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 10:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 12:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 14:00:00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Row ID  Postal Code       Sales\n",
       "Order Date                                            \n",
       "2015-01-02 00:00:00    67.625      6787.75   58.612500\n",
       "2015-01-02 02:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 04:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 06:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 08:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 10:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 12:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 14:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 16:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 18:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 20:00:00     0.000         0.00    0.000000\n",
       "2015-01-02 22:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 00:00:00  8423.500     87762.75  275.393875\n",
       "2015-01-03 02:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 04:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 06:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 08:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 10:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 12:00:00     0.000         0.00    0.000000\n",
       "2015-01-03 14:00:00     0.000         0.00    0.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sales = data.resample(\"2H\").sum() / 8\n",
    "data_sales.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e49cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2c6b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_sales.iloc[:, i], trim=\"f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8f48278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Order Date\n",
       " 2015-01-02 00:00:00      67.625\n",
       " 2015-01-02 02:00:00       0.000\n",
       " 2015-01-02 04:00:00       0.000\n",
       " 2015-01-02 06:00:00       0.000\n",
       " 2015-01-02 08:00:00       0.000\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00       0.000\n",
       " 2018-12-29 18:00:00       0.000\n",
       " 2018-12-29 20:00:00       0.000\n",
       " 2018-12-29 22:00:00       0.000\n",
       " 2018-12-30 00:00:00    1382.125\n",
       " Freq: 2H, Name: Row ID, Length: 17497, dtype: float64,\n",
       " Order Date\n",
       " 2015-01-02 00:00:00     6787.75\n",
       " 2015-01-02 02:00:00        0.00\n",
       " 2015-01-02 04:00:00        0.00\n",
       " 2015-01-02 06:00:00        0.00\n",
       " 2015-01-02 08:00:00        0.00\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00        0.00\n",
       " 2018-12-29 18:00:00        0.00\n",
       " 2018-12-29 20:00:00        0.00\n",
       " 2018-12-29 22:00:00        0.00\n",
       " 2018-12-30 00:00:00    43354.00\n",
       " Freq: 2H, Name: Postal Code, Length: 17497, dtype: float64,\n",
       " Order Date\n",
       " 2015-01-02 00:00:00    58.61250\n",
       " 2015-01-02 02:00:00     0.00000\n",
       " 2015-01-02 04:00:00     0.00000\n",
       " 2015-01-02 06:00:00     0.00000\n",
       " 2015-01-02 08:00:00     0.00000\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00     0.00000\n",
       " 2018-12-29 18:00:00     0.00000\n",
       " 2018-12-29 20:00:00     0.00000\n",
       " 2018-12-29 22:00:00     0.00000\n",
       " 2018-12-30 00:00:00    89.22375\n",
       " Freq: 2H, Name: Sales, Length: 17497, dtype: float64]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedda15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88841151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 3):\n",
    "    timeseries[i].loc[\"2015-01-01\":\"2015-01-14\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")\n",
    "    axx[i].set_ylabel(\"sales\")\n",
    "    axx[i].grid(which=\"minor\", axis=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5508a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7471b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = \"2H\"\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4d9b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4272/3397719331.py:1: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  start_dataset = pd.Timestamp(\"2015-01-02 00:00:00\", freq=freq)\n",
      "/tmp/ipykernel_4272/3397719331.py:2: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  end_training = pd.Timestamp(\"2018-03-13 00:00:00\", freq=freq)\n"
     ]
    }
   ],
   "source": [
    "start_dataset = pd.Timestamp(\"2015-01-02 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2018-03-13 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82345945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9f949b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[\n",
    "            start_dataset : end_training - timedelta(days=1)\n",
    "        ].tolist(),  # We use -1, because pandas indexing includes the upper bound\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc6382a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53a118dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a0685bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.4 ms, sys: 0 ns, total: 41.4 ms\n",
      "Wall time: 40.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c290c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c868342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-1-101718508260/deepar-sales-demo-notebook/data/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-1-101718508260/deepar-sales-demo-notebook/data/test/test.json\n",
      "CPU times: user 40.3 ms, sys: 0 ns, total: 40.3 ms\n",
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2576ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2015-01-02 00:00:00\", \"target\": [67.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n"
     ]
    }
   ],
   "source": [
    "s3_sample = s3.Object(s3_bucket, s3_prefix + \"/data/train/train.json\").get()[\"Body\"].read()\n",
    "StringVariable = s3_sample.decode(\"UTF-8\", \"ignore\")\n",
    "lines = StringVariable.split(\"\\n\")\n",
    "print(lines[0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0091db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.2xlarge\",\n",
    "    base_job_name=\"deepar-sales-demo\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7e7c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ead9aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "176882b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-18 16:34:12 Starting - Starting the training job...\n",
      "2022-09-18 16:34:37 Starting - Preparing the instances for trainingProfilerReport-1663518852: InProgress\n",
      "......\n",
      "2022-09-18 16:35:38 Downloading - Downloading input data...\n",
      "2022-09-18 16:35:58 Training - Downloading the training image......\n",
      "2022-09-18 16:37:09 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '84', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '84', 'time_freq': '2H'}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '84', 'epochs': '400', 'prediction_length': '84', 'time_freq': '2H'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Real time series\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] number of time series: 3\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] number of observations: 41943\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] mean target length: 13981.0\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] min/mean/max target: 0.0/1275.328575760556/277080.125\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] mean abs(target): 1275.328575760556\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Test set statistics:\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Real time series\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] number of time series: 12\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] number of observations: 196572\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] mean target length: 16381.0\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] min/mean/max target: 0.0/1357.511110909743/279997.25\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] mean abs(target): 1357.511110909743\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] #memory_usage::<batchbuffer> = 291.4404296875 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:15 INFO 140718936323904] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519035.4619288, \"EndTime\": 1663519037.4698262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 2005.2306652069092, \"count\": 1, \"min\": 2005.2306652069092, \"max\": 2005.2306652069092}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:17 INFO 140718936323904] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:18 INFO 140718936323904] #memory_usage::<model> = 89 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519037.4699142, \"EndTime\": 1663519038.3931656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 2931.107759475708, \"count\": 1, \"min\": 2931.107759475708, \"max\": 2931.107759475708}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:20 INFO 140718936323904] Epoch[0] Batch[0] avg_epoch_loss=6.550551\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=6.550550937652588\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:22 INFO 140718936323904] Epoch[0] Batch[5] avg_epoch_loss=6.466557\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.466557105382283\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:22 INFO 140718936323904] Epoch[0] Batch [5]#011Speed: 198.74 samples/sec#011loss=6.466557\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519038.39326, \"EndTime\": 1663519043.51182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 5118.448734283447, \"count\": 1, \"min\": 5118.448734283447, \"max\": 5118.448734283447}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=115.46106239410022 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=0, train loss <loss>=6.35531210899353\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:23 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_decd6c9b-913e-4485-9c0a-c34753d43937-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519043.511916, \"EndTime\": 1663519043.602617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.88428115844727, \"count\": 1, \"min\": 89.88428115844727, \"max\": 89.88428115844727}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:25 INFO 140718936323904] Epoch[1] Batch[0] avg_epoch_loss=5.654580\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=5.6545796394348145\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:26 INFO 140718936323904] Epoch[1] Batch[5] avg_epoch_loss=5.951650\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=5.9516495068868\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:26 INFO 140718936323904] Epoch[1] Batch [5]#011Speed: 226.56 samples/sec#011loss=5.951650\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] Epoch[1] Batch[10] avg_epoch_loss=5.975750\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=6.004670333862305\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] Epoch[1] Batch [10]#011Speed: 202.14 samples/sec#011loss=6.004670\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519043.6027334, \"EndTime\": 1663519048.540662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4937.7686977386475, \"count\": 1, \"min\": 4937.7686977386475, \"max\": 4937.7686977386475}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.65870088855996 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=1, train loss <loss>=5.975749882784757\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:28 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_deafd0c0-0712-483a-a0db-46b77098cb9a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519048.540805, \"EndTime\": 1663519048.6293786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 87.94403076171875, \"count\": 1, \"min\": 87.94403076171875, \"max\": 87.94403076171875}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:30 INFO 140718936323904] Epoch[2] Batch[0] avg_epoch_loss=5.794445\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=5.794445037841797\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:31 INFO 140718936323904] Epoch[2] Batch[5] avg_epoch_loss=5.506381\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=5.506381432215373\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:31 INFO 140718936323904] Epoch[2] Batch [5]#011Speed: 200.89 samples/sec#011loss=5.506381\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519048.6294549, \"EndTime\": 1663519053.259915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4630.388259887695, \"count\": 1, \"min\": 4630.388259887695, \"max\": 4630.388259887695}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.56591566694587 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=2, train loss <loss>=5.5531316757202145\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:33 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_272fed80-5bd7-4e5b-80f6-52973236aef7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519053.2599885, \"EndTime\": 1663519053.330251, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 69.7486400604248, \"count\": 1, \"min\": 69.7486400604248, \"max\": 69.7486400604248}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:34 INFO 140718936323904] Epoch[3] Batch[0] avg_epoch_loss=6.115549\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.115549087524414\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:36 INFO 140718936323904] Epoch[3] Batch[5] avg_epoch_loss=5.627263\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=5.627263466517131\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:36 INFO 140718936323904] Epoch[3] Batch [5]#011Speed: 215.69 samples/sec#011loss=5.627263\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] Epoch[3] Batch[10] avg_epoch_loss=5.567844\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=5.496540641784668\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] Epoch[3] Batch [10]#011Speed: 187.84 samples/sec#011loss=5.496541\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519053.330319, \"EndTime\": 1663519058.0699966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4739.621877670288, \"count\": 1, \"min\": 4739.621877670288, \"max\": 4739.621877670288}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.45922073942336 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=3, train loss <loss>=5.5678440007296475\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:38 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:39 INFO 140718936323904] Epoch[4] Batch[0] avg_epoch_loss=4.882012\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.882011890411377\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:41 INFO 140718936323904] Epoch[4] Batch[5] avg_epoch_loss=5.071170\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.071170171101888\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:41 INFO 140718936323904] Epoch[4] Batch [5]#011Speed: 207.37 samples/sec#011loss=5.071170\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519058.0700774, \"EndTime\": 1663519062.6982403, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4627.696514129639, \"count\": 1, \"min\": 4627.696514129639, \"max\": 4627.696514129639}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.96870179122524 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.984636449813843\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:42 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_095a412c-67d3-4c1e-9a62-462b37a78bfd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519062.6984525, \"EndTime\": 1663519062.787625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 88.6068344116211, \"count\": 1, \"min\": 88.6068344116211, \"max\": 88.6068344116211}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:44 INFO 140718936323904] Epoch[5] Batch[0] avg_epoch_loss=5.264729\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=5.264729022979736\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:45 INFO 140718936323904] Epoch[5] Batch[5] avg_epoch_loss=4.863954\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.863954305648804\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:45 INFO 140718936323904] Epoch[5] Batch [5]#011Speed: 210.59 samples/sec#011loss=4.863954\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519062.7877126, \"EndTime\": 1663519067.264268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4476.4885902404785, \"count\": 1, \"min\": 4476.4885902404785, \"max\": 4476.4885902404785}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.73944913642737 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.767069005966187\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:47 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_9a335ad5-90b2-4659-b29b-bae3f3527669-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519067.264429, \"EndTime\": 1663519067.353713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 88.6836051940918, \"count\": 1, \"min\": 88.6836051940918, \"max\": 88.6836051940918}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:49 INFO 140718936323904] Epoch[6] Batch[0] avg_epoch_loss=4.651251\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=4.651251316070557\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:50 INFO 140718936323904] Epoch[6] Batch[5] avg_epoch_loss=4.540086\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.540085633595784\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:50 INFO 140718936323904] Epoch[6] Batch [5]#011Speed: 209.21 samples/sec#011loss=4.540086\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519067.3537908, \"EndTime\": 1663519071.8593214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4505.459547042847, \"count\": 1, \"min\": 4505.459547042847, \"max\": 4505.459547042847}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.6113720722568 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.688699340820312\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:51 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_799d40f4-a710-4400-b9a8-55d1b31659f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519071.8594267, \"EndTime\": 1663519071.930005, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 69.92053985595703, \"count\": 1, \"min\": 69.92053985595703, \"max\": 69.92053985595703}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:53 INFO 140718936323904] Epoch[7] Batch[0] avg_epoch_loss=4.496511\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.4965105056762695\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:55 INFO 140718936323904] Epoch[7] Batch[5] avg_epoch_loss=4.390898\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.39089830716451\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:55 INFO 140718936323904] Epoch[7] Batch [5]#011Speed: 221.85 samples/sec#011loss=4.390898\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519071.9301195, \"EndTime\": 1663519076.3692074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4439.011573791504, \"count\": 1, \"min\": 4439.011573791504, \"max\": 4439.011573791504}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.37004587658063 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.327381682395935\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:56 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_ebbdf616-9188-40f2-a12f-4587581cf422-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519076.3692958, \"EndTime\": 1663519076.4557488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 86.00950241088867, \"count\": 1, \"min\": 86.00950241088867, \"max\": 86.00950241088867}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:58 INFO 140718936323904] Epoch[8] Batch[0] avg_epoch_loss=4.430946\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.430946350097656\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:59 INFO 140718936323904] Epoch[8] Batch[5] avg_epoch_loss=4.278720\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.278720140457153\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:37:59 INFO 140718936323904] Epoch[8] Batch [5]#011Speed: 210.31 samples/sec#011loss=4.278720\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519076.4558358, \"EndTime\": 1663519080.867054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4411.149501800537, \"count\": 1, \"min\": 4411.149501800537, \"max\": 4411.149501800537}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.72249852504376 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.326554250717163\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:00 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_784d663f-1eb9-41a2-91c8-e91322624689-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519080.8671436, \"EndTime\": 1663519080.9585216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 90.88659286499023, \"count\": 1, \"min\": 90.88659286499023, \"max\": 90.88659286499023}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:02 INFO 140718936323904] Epoch[9] Batch[0] avg_epoch_loss=4.386748\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=4.38674783706665\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:04 INFO 140718936323904] Epoch[9] Batch[5] avg_epoch_loss=4.380668\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=4.38066840171814\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:04 INFO 140718936323904] Epoch[9] Batch [5]#011Speed: 216.49 samples/sec#011loss=4.380668\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519080.958641, \"EndTime\": 1663519085.406839, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4448.1260776519775, \"count\": 1, \"min\": 4448.1260776519775, \"max\": 4448.1260776519775}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.83040349632677 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=9, train loss <loss>=4.2131949186325075\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:05 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_0e2d004c-2249-4ce2-8ce0-d47ffd5cbcab-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519085.4069204, \"EndTime\": 1663519085.4782078, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 70.65105438232422, \"count\": 1, \"min\": 70.65105438232422, \"max\": 70.65105438232422}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:07 INFO 140718936323904] Epoch[10] Batch[0] avg_epoch_loss=4.737940\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=4.737940311431885\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:08 INFO 140718936323904] Epoch[10] Batch[5] avg_epoch_loss=4.244828\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.244827508926392\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:08 INFO 140718936323904] Epoch[10] Batch [5]#011Speed: 211.83 samples/sec#011loss=4.244828\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519085.4782887, \"EndTime\": 1663519090.0179064, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4539.548397064209, \"count\": 1, \"min\": 4539.548397064209, \"max\": 4539.548397064209}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.65409081293603 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=10, train loss <loss>=4.087951016426087\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:10 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_6cf87b71-2d67-4a92-9209-34b5ae1e6d9d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519090.0181055, \"EndTime\": 1663519090.1078296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.01476860046387, \"count\": 1, \"min\": 89.01476860046387, \"max\": 89.01476860046387}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:11 INFO 140718936323904] Epoch[11] Batch[0] avg_epoch_loss=4.301270\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=4.301270484924316\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:13 INFO 140718936323904] Epoch[11] Batch[5] avg_epoch_loss=4.251935\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=4.251934925715129\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:13 INFO 140718936323904] Epoch[11] Batch [5]#011Speed: 219.19 samples/sec#011loss=4.251935\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:14 INFO 140718936323904] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519090.1079152, \"EndTime\": 1663519094.619221, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4511.2364292144775, \"count\": 1, \"min\": 4511.2364292144775, \"max\": 4511.2364292144775}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:14 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.33181914485618 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:14 INFO 140718936323904] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.106072497367859\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:14 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:16 INFO 140718936323904] Epoch[12] Batch[0] avg_epoch_loss=4.835083\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.8350830078125\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:17 INFO 140718936323904] Epoch[12] Batch[5] avg_epoch_loss=4.105874\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.105874141057332\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:17 INFO 140718936323904] Epoch[12] Batch [5]#011Speed: 207.39 samples/sec#011loss=4.105874\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519094.619329, \"EndTime\": 1663519099.0488548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4428.9257526397705, \"count\": 1, \"min\": 4428.9257526397705, \"max\": 4428.9257526397705}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.69387417617816 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.081317591667175\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:19 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_6c747aed-ac48-4955-a742-3e93a8c2142a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519099.048952, \"EndTime\": 1663519099.1380844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 88.65857124328613, \"count\": 1, \"min\": 88.65857124328613, \"max\": 88.65857124328613}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:20 INFO 140718936323904] Epoch[13] Batch[0] avg_epoch_loss=3.957279\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.9572794437408447\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:22 INFO 140718936323904] Epoch[13] Batch[5] avg_epoch_loss=3.691158\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.6911584933598838\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:22 INFO 140718936323904] Epoch[13] Batch [5]#011Speed: 213.87 samples/sec#011loss=3.691158\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519099.1381698, \"EndTime\": 1663519103.6126893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4474.457263946533, \"count\": 1, \"min\": 4474.457263946533, \"max\": 4474.457263946533}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.20853615292867 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.5459225177764893\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:23 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_cced66a2-7d26-473e-8b87-3578d8359ba9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519103.61277, \"EndTime\": 1663519103.6741977, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 60.842275619506836, \"count\": 1, \"min\": 60.842275619506836, \"max\": 60.842275619506836}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:25 INFO 140718936323904] Epoch[14] Batch[0] avg_epoch_loss=3.686861\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.686861038208008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:26 INFO 140718936323904] Epoch[14] Batch[5] avg_epoch_loss=3.823577\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.823577125867208\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:26 INFO 140718936323904] Epoch[14] Batch [5]#011Speed: 206.50 samples/sec#011loss=3.823577\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] Epoch[14] Batch[10] avg_epoch_loss=3.666135\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=3.477205514907837\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] Epoch[14] Batch [10]#011Speed: 174.50 samples/sec#011loss=3.477206\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519103.6742911, \"EndTime\": 1663519108.6249044, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4950.545310974121, \"count\": 1, \"min\": 4950.545310974121, \"max\": 4950.545310974121}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.36521002090095 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.666135484522039\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:28 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:30 INFO 140718936323904] Epoch[15] Batch[0] avg_epoch_loss=3.988235\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.988234519958496\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:31 INFO 140718936323904] Epoch[15] Batch[5] avg_epoch_loss=3.836258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.8362576166788735\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:31 INFO 140718936323904] Epoch[15] Batch [5]#011Speed: 224.10 samples/sec#011loss=3.836258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:32 INFO 140718936323904] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519108.6249914, \"EndTime\": 1663519112.906906, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4281.480073928833, \"count\": 1, \"min\": 4281.480073928833, \"max\": 4281.480073928833}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:32 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=149.24326528525782 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:32 INFO 140718936323904] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.6768820762634276\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:32 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:34 INFO 140718936323904] Epoch[16] Batch[0] avg_epoch_loss=3.713822\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.7138216495513916\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:36 INFO 140718936323904] Epoch[16] Batch[5] avg_epoch_loss=3.840851\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.8408508698145547\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:36 INFO 140718936323904] Epoch[16] Batch [5]#011Speed: 213.28 samples/sec#011loss=3.840851\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] Epoch[16] Batch[10] avg_epoch_loss=3.824125\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=3.8040539264678954\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] Epoch[16] Batch [10]#011Speed: 183.74 samples/sec#011loss=3.804054\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519112.9069872, \"EndTime\": 1663519117.767908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4860.360622406006, \"count\": 1, \"min\": 4860.360622406006, \"max\": 4860.360622406006}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.9372621386369 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.824124986475164\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:37 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:39 INFO 140718936323904] Epoch[17] Batch[0] avg_epoch_loss=4.036358\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=4.03635835647583\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:40 INFO 140718936323904] Epoch[17] Batch[5] avg_epoch_loss=3.630869\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.6308693091074624\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:40 INFO 140718936323904] Epoch[17] Batch [5]#011Speed: 224.77 samples/sec#011loss=3.630869\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] Epoch[17] Batch[10] avg_epoch_loss=3.935409\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=4.300857639312744\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] Epoch[17] Batch [10]#011Speed: 208.30 samples/sec#011loss=4.300858\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519117.767995, \"EndTime\": 1663519122.3497198, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4581.203937530518, \"count\": 1, \"min\": 4581.203937530518, \"max\": 4581.203937530518}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.3523926968851 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.9354094592007725\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:42 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:43 INFO 140718936323904] Epoch[18] Batch[0] avg_epoch_loss=3.628598\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.628598213195801\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:45 INFO 140718936323904] Epoch[18] Batch[5] avg_epoch_loss=3.622582\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.622582038243612\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:45 INFO 140718936323904] Epoch[18] Batch [5]#011Speed: 204.36 samples/sec#011loss=3.622582\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] Epoch[18] Batch[10] avg_epoch_loss=3.457978\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=3.2604530811309815\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] Epoch[18] Batch [10]#011Speed: 192.54 samples/sec#011loss=3.260453\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519122.3498056, \"EndTime\": 1663519127.198549, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4848.266124725342, \"count\": 1, \"min\": 4848.266124725342, \"max\": 4848.266124725342}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.44520278968153 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.45797796682878\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:47 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_403148c9-0658-4ff1-b66e-6581392b89af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519127.1986425, \"EndTime\": 1663519127.251548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 52.32691764831543, \"count\": 1, \"min\": 52.32691764831543, \"max\": 52.32691764831543}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:48 INFO 140718936323904] Epoch[19] Batch[0] avg_epoch_loss=3.893731\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.8937313556671143\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:50 INFO 140718936323904] Epoch[19] Batch[5] avg_epoch_loss=3.605302\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.6053016980489097\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:50 INFO 140718936323904] Epoch[19] Batch [5]#011Speed: 225.29 samples/sec#011loss=3.605302\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:51 INFO 140718936323904] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519127.251616, \"EndTime\": 1663519131.6138759, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4362.187385559082, \"count\": 1, \"min\": 4362.187385559082, \"max\": 4362.187385559082}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.29236924963752 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.490838813781738\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:51 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:53 INFO 140718936323904] Epoch[20] Batch[0] avg_epoch_loss=3.903740\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.9037396907806396\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:54 INFO 140718936323904] Epoch[20] Batch[5] avg_epoch_loss=3.624696\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.624695897102356\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:54 INFO 140718936323904] Epoch[20] Batch [5]#011Speed: 205.19 samples/sec#011loss=3.624696\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:56 INFO 140718936323904] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519131.6139712, \"EndTime\": 1663519136.0890377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4474.6057987213135, \"count\": 1, \"min\": 4474.6057987213135, \"max\": 4474.6057987213135}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:56 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.8845543023029 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:56 INFO 140718936323904] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.74509596824646\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:56 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:57 INFO 140718936323904] Epoch[21] Batch[0] avg_epoch_loss=3.204948\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.2049484252929688\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:59 INFO 140718936323904] Epoch[21] Batch[5] avg_epoch_loss=3.556048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.5560476382573447\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:38:59 INFO 140718936323904] Epoch[21] Batch [5]#011Speed: 217.90 samples/sec#011loss=3.556048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:00 INFO 140718936323904] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519136.0891411, \"EndTime\": 1663519140.5238554, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4434.1065883636475, \"count\": 1, \"min\": 4434.1065883636475, \"max\": 4434.1065883636475}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.49631009533505 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.5750526666641234\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:02 INFO 140718936323904] Epoch[22] Batch[0] avg_epoch_loss=3.791403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.791402816772461\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:03 INFO 140718936323904] Epoch[22] Batch[5] avg_epoch_loss=3.676863\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.6768627564112344\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:03 INFO 140718936323904] Epoch[22] Batch [5]#011Speed: 210.06 samples/sec#011loss=3.676863\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] Epoch[22] Batch[10] avg_epoch_loss=3.613979\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=3.538519096374512\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] Epoch[22] Batch [10]#011Speed: 173.79 samples/sec#011loss=3.538519\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519140.5239909, \"EndTime\": 1663519145.5748885, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5050.405979156494, \"count\": 1, \"min\": 5050.405979156494, \"max\": 5050.405979156494}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=127.51076322916276 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.6139792745763604\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:05 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:07 INFO 140718936323904] Epoch[23] Batch[0] avg_epoch_loss=3.273571\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.2735707759857178\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:08 INFO 140718936323904] Epoch[23] Batch[5] avg_epoch_loss=3.318856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.318856438000997\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:08 INFO 140718936323904] Epoch[23] Batch [5]#011Speed: 202.77 samples/sec#011loss=3.318856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519145.5749984, \"EndTime\": 1663519150.1577208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4582.2906494140625, \"count\": 1, \"min\": 4582.2906494140625, \"max\": 4582.2906494140625}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.57319480781652 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.314206123352051\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:10 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_d61daace-a2ce-45cf-86b2-c26678c7fc54-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519150.1578083, \"EndTime\": 1663519150.244413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 86.14850044250488, \"count\": 1, \"min\": 86.14850044250488, \"max\": 86.14850044250488}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:11 INFO 140718936323904] Epoch[24] Batch[0] avg_epoch_loss=3.453419\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.4534189701080322\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:13 INFO 140718936323904] Epoch[24] Batch[5] avg_epoch_loss=3.361676\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.3616755406061807\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:13 INFO 140718936323904] Epoch[24] Batch [5]#011Speed: 203.10 samples/sec#011loss=3.361676\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] Epoch[24] Batch[10] avg_epoch_loss=3.383798\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=3.4103445529937746\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] Epoch[24] Batch [10]#011Speed: 185.01 samples/sec#011loss=3.410345\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519150.2444925, \"EndTime\": 1663519155.140766, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4896.202802658081, \"count\": 1, \"min\": 4896.202802658081, \"max\": 4896.202802658081}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.02022058071648 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.3837978189641778\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:15 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:16 INFO 140718936323904] Epoch[25] Batch[0] avg_epoch_loss=3.207670\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.207669973373413\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:18 INFO 140718936323904] Epoch[25] Batch[5] avg_epoch_loss=3.141667\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.1416670083999634\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:18 INFO 140718936323904] Epoch[25] Batch [5]#011Speed: 217.47 samples/sec#011loss=3.141667\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] Epoch[25] Batch[10] avg_epoch_loss=3.229236\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=3.334318017959595\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] Epoch[25] Batch [10]#011Speed: 174.75 samples/sec#011loss=3.334318\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519155.140852, \"EndTime\": 1663519160.0929163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4951.63106918335, \"count\": 1, \"min\": 4951.63106918335, \"max\": 4951.63106918335}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.0547482111724 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.2292356491088867\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:20 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_5b897ef1-e6a4-4e3c-90b5-41055a807d28-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519160.0930078, \"EndTime\": 1663519160.1505814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 57.0225715637207, \"count\": 1, \"min\": 57.0225715637207, \"max\": 57.0225715637207}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:21 INFO 140718936323904] Epoch[26] Batch[0] avg_epoch_loss=3.505025\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.5050246715545654\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:23 INFO 140718936323904] Epoch[26] Batch[5] avg_epoch_loss=3.391038\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.3910377422968545\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:23 INFO 140718936323904] Epoch[26] Batch [5]#011Speed: 211.01 samples/sec#011loss=3.391038\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] Epoch[26] Batch[10] avg_epoch_loss=3.206293\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.9845995187759398\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] Epoch[26] Batch [10]#011Speed: 185.13 samples/sec#011loss=2.984600\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519160.1506705, \"EndTime\": 1663519164.9339368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4783.197402954102, \"count\": 1, \"min\": 4783.197402954102, \"max\": 4783.197402954102}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.5615524753186 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.2062930952418935\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:24 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:25 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_4a2b3c8a-eca0-46dd-8823-1aaa0432ae09-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519164.93402, \"EndTime\": 1663519165.0204248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 85.9675407409668, \"count\": 1, \"min\": 85.9675407409668, \"max\": 85.9675407409668}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:26 INFO 140718936323904] Epoch[27] Batch[0] avg_epoch_loss=3.256111\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.256110668182373\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:28 INFO 140718936323904] Epoch[27] Batch[5] avg_epoch_loss=3.123245\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.1232454776763916\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:28 INFO 140718936323904] Epoch[27] Batch [5]#011Speed: 202.17 samples/sec#011loss=3.123245\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] Epoch[27] Batch[10] avg_epoch_loss=3.384017\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=3.6969420433044435\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] Epoch[27] Batch [10]#011Speed: 186.55 samples/sec#011loss=3.696942\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519165.0205057, \"EndTime\": 1663519169.918636, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4898.068189620972, \"count\": 1, \"min\": 4898.068189620972, \"max\": 4898.068189620972}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.3773772809531 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.3840166438709605\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:29 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:31 INFO 140718936323904] Epoch[28] Batch[0] avg_epoch_loss=3.193735\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.193735122680664\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:33 INFO 140718936323904] Epoch[28] Batch[5] avg_epoch_loss=3.152947\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.152947028477987\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:33 INFO 140718936323904] Epoch[28] Batch [5]#011Speed: 214.38 samples/sec#011loss=3.152947\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:34 INFO 140718936323904] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519169.9187126, \"EndTime\": 1663519174.3547108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4435.579776763916, \"count\": 1, \"min\": 4435.579776763916, \"max\": 4435.579776763916}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:34 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.676836408843 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:34 INFO 140718936323904] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.211586427688599\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:34 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:35 INFO 140718936323904] Epoch[29] Batch[0] avg_epoch_loss=3.732428\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.7324280738830566\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:37 INFO 140718936323904] Epoch[29] Batch[5] avg_epoch_loss=3.154864\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.154863953590393\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:37 INFO 140718936323904] Epoch[29] Batch [5]#011Speed: 217.44 samples/sec#011loss=3.154864\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] Epoch[29] Batch[10] avg_epoch_loss=3.398440\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=3.6907309532165526\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] Epoch[29] Batch [10]#011Speed: 189.86 samples/sec#011loss=3.690731\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519174.3547943, \"EndTime\": 1663519179.0577376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4702.406406402588, \"count\": 1, \"min\": 4702.406406402588, \"max\": 4702.406406402588}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.28699547886356 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.3984398625113745\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:39 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:40 INFO 140718936323904] Epoch[30] Batch[0] avg_epoch_loss=3.393040\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.3930397033691406\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:42 INFO 140718936323904] Epoch[30] Batch[5] avg_epoch_loss=3.042756\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.0427563985188804\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:42 INFO 140718936323904] Epoch[30] Batch [5]#011Speed: 207.91 samples/sec#011loss=3.042756\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519179.0578136, \"EndTime\": 1663519183.4505591, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4392.29154586792, \"count\": 1, \"min\": 4392.29154586792, \"max\": 4392.29154586792}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.79496563451522 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.101998805999756\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:43 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_576eb17e-51f2-444a-8246-6f92e22a0c34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519183.4506476, \"EndTime\": 1663519183.538941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 87.75067329406738, \"count\": 1, \"min\": 87.75067329406738, \"max\": 87.75067329406738}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:45 INFO 140718936323904] Epoch[31] Batch[0] avg_epoch_loss=3.617512\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.6175124645233154\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:46 INFO 140718936323904] Epoch[31] Batch[5] avg_epoch_loss=3.322771\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.3227710326512656\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:46 INFO 140718936323904] Epoch[31] Batch [5]#011Speed: 214.85 samples/sec#011loss=3.322771\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] Epoch[31] Batch[10] avg_epoch_loss=3.208709\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=3.071834897994995\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] Epoch[31] Batch [10]#011Speed: 179.02 samples/sec#011loss=3.071835\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519183.539027, \"EndTime\": 1663519188.3645208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4825.427770614624, \"count\": 1, \"min\": 4825.427770614624, \"max\": 4825.427770614624}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.81813475514082 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.2087091532620517\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:48 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:49 INFO 140718936323904] Epoch[32] Batch[0] avg_epoch_loss=2.413279\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.4132792949676514\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:51 INFO 140718936323904] Epoch[32] Batch[5] avg_epoch_loss=3.037209\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.0372087160746255\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:51 INFO 140718936323904] Epoch[32] Batch [5]#011Speed: 211.85 samples/sec#011loss=3.037209\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:52 INFO 140718936323904] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519188.3645964, \"EndTime\": 1663519192.8001199, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4435.1255893707275, \"count\": 1, \"min\": 4435.1255893707275, \"max\": 4435.1255893707275}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:52 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.89764969466722 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:52 INFO 140718936323904] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.2669785976409913\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:52 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:54 INFO 140718936323904] Epoch[33] Batch[0] avg_epoch_loss=3.262237\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.2622365951538086\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:55 INFO 140718936323904] Epoch[33] Batch[5] avg_epoch_loss=3.086917\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.0869174003601074\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:55 INFO 140718936323904] Epoch[33] Batch [5]#011Speed: 215.47 samples/sec#011loss=3.086917\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:57 INFO 140718936323904] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519192.8002157, \"EndTime\": 1663519197.276595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4475.8782386779785, \"count\": 1, \"min\": 4475.8782386779785, \"max\": 4475.8782386779785}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:57 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.40977705052157 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:57 INFO 140718936323904] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.193695592880249\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:57 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:58 INFO 140718936323904] Epoch[34] Batch[0] avg_epoch_loss=2.965280\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:39:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.9652798175811768\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:00 INFO 140718936323904] Epoch[34] Batch[5] avg_epoch_loss=2.920872\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.9208720922470093\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:00 INFO 140718936323904] Epoch[34] Batch [5]#011Speed: 205.34 samples/sec#011loss=2.920872\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] Epoch[34] Batch[10] avg_epoch_loss=3.177759\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.4860222339630127\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] Epoch[34] Batch [10]#011Speed: 172.81 samples/sec#011loss=3.486022\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519197.2766862, \"EndTime\": 1663519202.2773218, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5000.057697296143, \"count\": 1, \"min\": 5000.057697296143, \"max\": 5000.057697296143}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.3951785304391 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.1777585202997383\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:02 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:03 INFO 140718936323904] Epoch[35] Batch[0] avg_epoch_loss=3.099244\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.0992443561553955\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:05 INFO 140718936323904] Epoch[35] Batch[5] avg_epoch_loss=3.156262\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.156261682510376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:05 INFO 140718936323904] Epoch[35] Batch [5]#011Speed: 207.68 samples/sec#011loss=3.156262\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] Epoch[35] Batch[10] avg_epoch_loss=3.309471\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=3.4933231830596925\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] Epoch[35] Batch [10]#011Speed: 167.90 samples/sec#011loss=3.493323\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519202.2774057, \"EndTime\": 1663519207.326517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5048.6602783203125, \"count\": 1, \"min\": 5048.6602783203125, \"max\": 5048.6602783203125}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=128.54584634169194 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.309471455487338\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:07 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:08 INFO 140718936323904] Epoch[36] Batch[0] avg_epoch_loss=2.674350\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.6743500232696533\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:10 INFO 140718936323904] Epoch[36] Batch[5] avg_epoch_loss=2.976464\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.976463953653971\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:10 INFO 140718936323904] Epoch[36] Batch [5]#011Speed: 200.82 samples/sec#011loss=2.976464\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519207.3266022, \"EndTime\": 1663519211.7393613, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4412.308931350708, \"count\": 1, \"min\": 4412.308931350708, \"max\": 4412.308931350708}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.87146222254282 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.9519126176834107\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:11 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_0309f173-0108-4a14-b28a-77adb36367b8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519211.7394552, \"EndTime\": 1663519211.8288372, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 88.9139175415039, \"count\": 1, \"min\": 88.9139175415039, \"max\": 88.9139175415039}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:13 INFO 140718936323904] Epoch[37] Batch[0] avg_epoch_loss=3.471965\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.4719650745391846\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:14 INFO 140718936323904] Epoch[37] Batch[5] avg_epoch_loss=3.123000\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.1229997078577676\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:14 INFO 140718936323904] Epoch[37] Batch [5]#011Speed: 225.41 samples/sec#011loss=3.123000\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519211.8289225, \"EndTime\": 1663519216.196759, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4367.7709102630615, \"count\": 1, \"min\": 4367.7709102630615, \"max\": 4367.7709102630615}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.716003718506 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.935537242889404\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:16 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_75ee4ab7-aa50-4c9c-8c2d-aea2c4fe0447-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519216.1968412, \"EndTime\": 1663519216.267491, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 70.08028030395508, \"count\": 1, \"min\": 70.08028030395508, \"max\": 70.08028030395508}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:17 INFO 140718936323904] Epoch[38] Batch[0] avg_epoch_loss=3.033614\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=3.033613920211792\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:19 INFO 140718936323904] Epoch[38] Batch[5] avg_epoch_loss=3.110787\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.11078679561615\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:19 INFO 140718936323904] Epoch[38] Batch [5]#011Speed: 227.31 samples/sec#011loss=3.110787\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:20 INFO 140718936323904] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519216.2675874, \"EndTime\": 1663519220.6251428, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4357.490062713623, \"count\": 1, \"min\": 4357.490062713623, \"max\": 4357.490062713623}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:20 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=146.1808812549782 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:20 INFO 140718936323904] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.992464232444763\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:20 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:22 INFO 140718936323904] Epoch[39] Batch[0] avg_epoch_loss=2.794531\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.7945311069488525\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:23 INFO 140718936323904] Epoch[39] Batch[5] avg_epoch_loss=2.904868\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.9048682848612466\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:23 INFO 140718936323904] Epoch[39] Batch [5]#011Speed: 205.06 samples/sec#011loss=2.904868\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] Epoch[39] Batch[10] avg_epoch_loss=3.161972\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=3.4704959869384764\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] Epoch[39] Batch [10]#011Speed: 195.08 samples/sec#011loss=3.470496\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519220.625233, \"EndTime\": 1663519225.609839, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4984.01665687561, \"count\": 1, \"min\": 4984.01665687561, \"max\": 4984.01665687561}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.63690149940564 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.161971785805442\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:25 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:27 INFO 140718936323904] Epoch[40] Batch[0] avg_epoch_loss=2.789002\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.789001941680908\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:28 INFO 140718936323904] Epoch[40] Batch[5] avg_epoch_loss=2.800726\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.8007256984710693\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:28 INFO 140718936323904] Epoch[40] Batch [5]#011Speed: 203.77 samples/sec#011loss=2.800726\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] Epoch[40] Batch[10] avg_epoch_loss=2.968592\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=3.170030927658081\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] Epoch[40] Batch [10]#011Speed: 189.81 samples/sec#011loss=3.170031\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519225.609914, \"EndTime\": 1663519230.552848, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4942.510366439819, \"count\": 1, \"min\": 4942.510366439819, \"max\": 4942.510366439819}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.10368212534385 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.968591711737893\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:30 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:32 INFO 140718936323904] Epoch[41] Batch[0] avg_epoch_loss=3.233240\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=3.2332398891448975\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:33 INFO 140718936323904] Epoch[41] Batch[5] avg_epoch_loss=2.799848\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.799848477045695\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:33 INFO 140718936323904] Epoch[41] Batch [5]#011Speed: 203.91 samples/sec#011loss=2.799848\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] Epoch[41] Batch[10] avg_epoch_loss=2.838660\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.885233497619629\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] Epoch[41] Batch [10]#011Speed: 177.42 samples/sec#011loss=2.885233\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519230.5529506, \"EndTime\": 1663519235.5504699, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4997.100591659546, \"count\": 1, \"min\": 4997.100591659546, \"max\": 4997.100591659546}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.67596469137396 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.8386598500338467\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:35 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_c9644dd8-3685-4016-8131-0e70e24490e2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519235.550552, \"EndTime\": 1663519235.640499, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.4770622253418, \"count\": 1, \"min\": 89.4770622253418, \"max\": 89.4770622253418}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:37 INFO 140718936323904] Epoch[42] Batch[0] avg_epoch_loss=2.849918\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.8499176502227783\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:38 INFO 140718936323904] Epoch[42] Batch[5] avg_epoch_loss=2.636272\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.636271834373474\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:38 INFO 140718936323904] Epoch[42] Batch [5]#011Speed: 208.78 samples/sec#011loss=2.636272\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] Epoch[42] Batch[10] avg_epoch_loss=2.681389\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.735528564453125\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] Epoch[42] Batch [10]#011Speed: 179.44 samples/sec#011loss=2.735529\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519235.6405845, \"EndTime\": 1663519240.5823524, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4941.709518432617, \"count\": 1, \"min\": 4941.709518432617, \"max\": 4941.709518432617}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.99417787344896 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.6813885298642246\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:40 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_d719f33b-f689-4d3e-977e-afcf970dfccd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519240.582427, \"EndTime\": 1663519240.6712499, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 88.38462829589844, \"count\": 1, \"min\": 88.38462829589844, \"max\": 88.38462829589844}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:42 INFO 140718936323904] Epoch[43] Batch[0] avg_epoch_loss=3.325678\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.3256776332855225\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:43 INFO 140718936323904] Epoch[43] Batch[5] avg_epoch_loss=2.765960\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.7659597396850586\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:43 INFO 140718936323904] Epoch[43] Batch [5]#011Speed: 218.72 samples/sec#011loss=2.765960\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] Epoch[43] Batch[10] avg_epoch_loss=2.564143\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.321962571144104\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] Epoch[43] Batch [10]#011Speed: 190.06 samples/sec#011loss=2.321963\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519240.6713352, \"EndTime\": 1663519245.3956926, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4724.2889404296875, \"count\": 1, \"min\": 4724.2889404296875, \"max\": 4724.2889404296875}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.7366904650809 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.564142844893716\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:45 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_6b96aaa4-6766-44a5-9f4b-e5f25225086c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519245.395777, \"EndTime\": 1663519245.4853506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.09463882446289, \"count\": 1, \"min\": 89.09463882446289, \"max\": 89.09463882446289}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:47 INFO 140718936323904] Epoch[44] Batch[0] avg_epoch_loss=2.591540\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.5915403366088867\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:48 INFO 140718936323904] Epoch[44] Batch[5] avg_epoch_loss=2.618398\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.6183977127075195\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:48 INFO 140718936323904] Epoch[44] Batch [5]#011Speed: 208.00 samples/sec#011loss=2.618398\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] Epoch[44] Batch[10] avg_epoch_loss=2.828819\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=3.0813241004943848\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] Epoch[44] Batch [10]#011Speed: 186.48 samples/sec#011loss=3.081324\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519245.4854345, \"EndTime\": 1663519250.3624399, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4876.935720443726, \"count\": 1, \"min\": 4876.935720443726, \"max\": 4876.935720443726}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.2520160501842 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.8288187980651855\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:50 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:51 INFO 140718936323904] Epoch[45] Batch[0] avg_epoch_loss=2.579060\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.579059600830078\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:53 INFO 140718936323904] Epoch[45] Batch[5] avg_epoch_loss=3.089408\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=3.0894080797831216\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:53 INFO 140718936323904] Epoch[45] Batch [5]#011Speed: 209.51 samples/sec#011loss=3.089408\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:54 INFO 140718936323904] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519250.3625233, \"EndTime\": 1663519254.873209, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4510.2434158325195, \"count\": 1, \"min\": 4510.2434158325195, \"max\": 4510.2434158325195}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:54 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.6780378175687 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:54 INFO 140718936323904] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.9862956285476683\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:54 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:56 INFO 140718936323904] Epoch[46] Batch[0] avg_epoch_loss=1.870031\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.8700309991836548\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:58 INFO 140718936323904] Epoch[46] Batch[5] avg_epoch_loss=2.562860\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.56285959482193\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:58 INFO 140718936323904] Epoch[46] Batch [5]#011Speed: 202.92 samples/sec#011loss=2.562860\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:59 INFO 140718936323904] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519254.8732972, \"EndTime\": 1663519259.3814158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4507.665395736694, \"count\": 1, \"min\": 4507.665395736694, \"max\": 4507.665395736694}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:59 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.7617208090077 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:59 INFO 140718936323904] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.568840539455414\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:40:59 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:01 INFO 140718936323904] Epoch[47] Batch[0] avg_epoch_loss=2.922642\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.9226417541503906\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:02 INFO 140718936323904] Epoch[47] Batch[5] avg_epoch_loss=2.728441\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.728440523147583\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:02 INFO 140718936323904] Epoch[47] Batch [5]#011Speed: 201.25 samples/sec#011loss=2.728441\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] Epoch[47] Batch[10] avg_epoch_loss=2.793797\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=2.8722253322601317\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] Epoch[47] Batch [10]#011Speed: 202.83 samples/sec#011loss=2.872225\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519259.3814974, \"EndTime\": 1663519264.1751873, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4793.214321136475, \"count\": 1, \"min\": 4793.214321136475, \"max\": 4793.214321136475}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.02191601814238 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.793797254562378\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:04 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:05 INFO 140718936323904] Epoch[48] Batch[0] avg_epoch_loss=3.091178\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=3.0911784172058105\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:07 INFO 140718936323904] Epoch[48] Batch[5] avg_epoch_loss=2.757251\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.7572511037190757\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:07 INFO 140718936323904] Epoch[48] Batch [5]#011Speed: 203.67 samples/sec#011loss=2.757251\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:08 INFO 140718936323904] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519264.1752787, \"EndTime\": 1663519268.721256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4545.445919036865, \"count\": 1, \"min\": 4545.445919036865, \"max\": 4545.445919036865}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:08 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.1559794714798 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:08 INFO 140718936323904] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.7740166425704955\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:08 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:10 INFO 140718936323904] Epoch[49] Batch[0] avg_epoch_loss=2.603812\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.6038124561309814\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:11 INFO 140718936323904] Epoch[49] Batch[5] avg_epoch_loss=2.559556\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.559556265672048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:11 INFO 140718936323904] Epoch[49] Batch [5]#011Speed: 205.88 samples/sec#011loss=2.559556\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:13 INFO 140718936323904] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519268.7213511, \"EndTime\": 1663519273.1474655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4425.559282302856, \"count\": 1, \"min\": 4425.559282302856, \"max\": 4425.559282302856}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:13 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.0863789943704 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:13 INFO 140718936323904] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.7943052887916564\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:13 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:14 INFO 140718936323904] Epoch[50] Batch[0] avg_epoch_loss=2.407821\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.4078209400177\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:16 INFO 140718936323904] Epoch[50] Batch[5] avg_epoch_loss=2.624098\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.6240984201431274\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:16 INFO 140718936323904] Epoch[50] Batch [5]#011Speed: 212.44 samples/sec#011loss=2.624098\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519273.1475625, \"EndTime\": 1663519277.557423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4409.257173538208, \"count\": 1, \"min\": 4409.257173538208, \"max\": 4409.257173538208}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.70212121710227 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.5135169625282288\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:17 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_6848a1fe-50f9-4449-a453-0efb1a3750ac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519277.5575075, \"EndTime\": 1663519277.612533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 54.43978309631348, \"count\": 1, \"min\": 54.43978309631348, \"max\": 54.43978309631348}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:19 INFO 140718936323904] Epoch[51] Batch[0] avg_epoch_loss=2.603085\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.6030845642089844\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:20 INFO 140718936323904] Epoch[51] Batch[5] avg_epoch_loss=2.661284\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.6612837314605713\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:20 INFO 140718936323904] Epoch[51] Batch [5]#011Speed: 206.73 samples/sec#011loss=2.661284\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] Epoch[51] Batch[10] avg_epoch_loss=2.758782\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=2.8757796764373778\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] Epoch[51] Batch [10]#011Speed: 177.99 samples/sec#011loss=2.875780\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519277.6126196, \"EndTime\": 1663519282.5491924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4936.50221824646, \"count\": 1, \"min\": 4936.50221824646, \"max\": 4936.50221824646}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.4794549432904 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.7587818882682105\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:22 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:24 INFO 140718936323904] Epoch[52] Batch[0] avg_epoch_loss=2.657049\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.6570489406585693\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:25 INFO 140718936323904] Epoch[52] Batch[5] avg_epoch_loss=2.510340\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.5103402535120645\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:25 INFO 140718936323904] Epoch[52] Batch [5]#011Speed: 218.31 samples/sec#011loss=2.510340\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] Epoch[52] Batch[10] avg_epoch_loss=2.932856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=3.439875841140747\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] Epoch[52] Batch [10]#011Speed: 194.08 samples/sec#011loss=3.439876\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519282.5492682, \"EndTime\": 1663519287.339349, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4789.558410644531, \"count\": 1, \"min\": 4789.558410644531, \"max\": 4789.558410644531}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.8297861410513 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.9328564297069204\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:27 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:29 INFO 140718936323904] Epoch[53] Batch[0] avg_epoch_loss=2.263274\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.2632744312286377\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:30 INFO 140718936323904] Epoch[53] Batch[5] avg_epoch_loss=2.575786\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.575785676638285\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:30 INFO 140718936323904] Epoch[53] Batch [5]#011Speed: 206.56 samples/sec#011loss=2.575786\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:31 INFO 140718936323904] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519287.3394232, \"EndTime\": 1663519291.839602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4499.759197235107, \"count\": 1, \"min\": 4499.759197235107, \"max\": 4499.759197235107}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:31 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.2257374845077 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:31 INFO 140718936323904] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.65977566242218\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:31 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:33 INFO 140718936323904] Epoch[54] Batch[0] avg_epoch_loss=2.494631\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.494631290435791\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:34 INFO 140718936323904] Epoch[54] Batch[5] avg_epoch_loss=2.549545\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.5495445330937705\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:34 INFO 140718936323904] Epoch[54] Batch [5]#011Speed: 219.29 samples/sec#011loss=2.549545\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] Epoch[54] Batch[10] avg_epoch_loss=2.649543\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=2.7695404052734376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] Epoch[54] Batch [10]#011Speed: 187.12 samples/sec#011loss=2.769540\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519291.8396933, \"EndTime\": 1663519296.5739686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4733.8128089904785, \"count\": 1, \"min\": 4733.8128089904785, \"max\": 4733.8128089904785}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.573980199799 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.649542656811801\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:36 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:38 INFO 140718936323904] Epoch[55] Batch[0] avg_epoch_loss=2.844894\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.8448939323425293\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:39 INFO 140718936323904] Epoch[55] Batch[5] avg_epoch_loss=2.653193\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.6531929969787598\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:39 INFO 140718936323904] Epoch[55] Batch [5]#011Speed: 217.39 samples/sec#011loss=2.653193\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] Epoch[55] Batch[10] avg_epoch_loss=2.748656\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.8632111072540285\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] Epoch[55] Batch [10]#011Speed: 185.96 samples/sec#011loss=2.863211\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519296.5740519, \"EndTime\": 1663519301.3671393, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4792.619705200195, \"count\": 1, \"min\": 4792.619705200195, \"max\": 4792.619705200195}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.67328658435062 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.748655774376609\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:41 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:42 INFO 140718936323904] Epoch[56] Batch[0] avg_epoch_loss=2.348517\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.3485167026519775\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:44 INFO 140718936323904] Epoch[56] Batch[5] avg_epoch_loss=2.443337\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.4433372815450034\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:44 INFO 140718936323904] Epoch[56] Batch [5]#011Speed: 207.74 samples/sec#011loss=2.443337\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] Epoch[56] Batch[10] avg_epoch_loss=2.043617\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=1.5639517188072205\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] Epoch[56] Batch [10]#011Speed: 183.04 samples/sec#011loss=1.563952\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519301.367206, \"EndTime\": 1663519306.212447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4844.818353652954, \"count\": 1, \"min\": 4844.818353652954, \"max\": 4844.818353652954}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.7156722487605 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.0436165712096472\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:46 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_ace589ff-3c49-40e0-9a2a-ba44aea1550d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519306.2125294, \"EndTime\": 1663519306.2738142, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 60.65773963928223, \"count\": 1, \"min\": 60.65773963928223, \"max\": 60.65773963928223}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:47 INFO 140718936323904] Epoch[57] Batch[0] avg_epoch_loss=2.939594\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.939593553543091\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:49 INFO 140718936323904] Epoch[57] Batch[5] avg_epoch_loss=2.692075\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.6920751333236694\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:49 INFO 140718936323904] Epoch[57] Batch [5]#011Speed: 221.52 samples/sec#011loss=2.692075\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] Epoch[57] Batch[10] avg_epoch_loss=2.338228\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=1.9136113226413727\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] Epoch[57] Batch [10]#011Speed: 187.17 samples/sec#011loss=1.913611\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519306.273904, \"EndTime\": 1663519310.9850154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4711.042404174805, \"count\": 1, \"min\": 4711.042404174805, \"max\": 4711.042404174805}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.5788644311389 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.3382279466498983\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:50 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:52 INFO 140718936323904] Epoch[58] Batch[0] avg_epoch_loss=2.496235\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.4962353706359863\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:54 INFO 140718936323904] Epoch[58] Batch[5] avg_epoch_loss=2.383856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.383855621019999\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:54 INFO 140718936323904] Epoch[58] Batch [5]#011Speed: 204.70 samples/sec#011loss=2.383856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:55 INFO 140718936323904] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519310.9850898, \"EndTime\": 1663519315.4471893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4461.5797996521, \"count\": 1, \"min\": 4461.5797996521, \"max\": 4461.5797996521}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:55 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.9948085170416 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:55 INFO 140718936323904] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.446709084510803\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:55 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:57 INFO 140718936323904] Epoch[59] Batch[0] avg_epoch_loss=2.289191\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.2891907691955566\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:58 INFO 140718936323904] Epoch[59] Batch[5] avg_epoch_loss=2.437092\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.437092105547587\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:41:58 INFO 140718936323904] Epoch[59] Batch [5]#011Speed: 207.11 samples/sec#011loss=2.437092\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:00 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519315.4472737, \"EndTime\": 1663519320.037867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4590.118408203125, \"count\": 1, \"min\": 4590.118408203125, \"max\": 4590.118408203125}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.11898186266535 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.329793655872345\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:01 INFO 140718936323904] Epoch[60] Batch[0] avg_epoch_loss=1.917066\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.9170656204223633\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:03 INFO 140718936323904] Epoch[60] Batch[5] avg_epoch_loss=2.306216\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.3062157034873962\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:03 INFO 140718936323904] Epoch[60] Batch [5]#011Speed: 205.70 samples/sec#011loss=2.306216\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:04 INFO 140718936323904] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519320.0379524, \"EndTime\": 1663519324.6515007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4612.959146499634, \"count\": 1, \"min\": 4612.959146499634, \"max\": 4612.959146499634}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:04 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.52322687717898 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:04 INFO 140718936323904] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.397011196613312\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:04 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:06 INFO 140718936323904] Epoch[61] Batch[0] avg_epoch_loss=2.490664\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.490663528442383\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:07 INFO 140718936323904] Epoch[61] Batch[5] avg_epoch_loss=2.582156\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.5821559826533\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:07 INFO 140718936323904] Epoch[61] Batch [5]#011Speed: 211.97 samples/sec#011loss=2.582156\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:09 INFO 140718936323904] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519324.651927, \"EndTime\": 1663519329.088974, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4436.164617538452, \"count\": 1, \"min\": 4436.164617538452, \"max\": 4436.164617538452}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:09 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.7274434287017 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:09 INFO 140718936323904] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.503579545021057\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:09 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:10 INFO 140718936323904] Epoch[62] Batch[0] avg_epoch_loss=2.679334\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.6793339252471924\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:12 INFO 140718936323904] Epoch[62] Batch[5] avg_epoch_loss=2.260923\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.260922829310099\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:12 INFO 140718936323904] Epoch[62] Batch [5]#011Speed: 205.24 samples/sec#011loss=2.260923\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:13 INFO 140718936323904] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519329.0890644, \"EndTime\": 1663519333.5730498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4483.382701873779, \"count\": 1, \"min\": 4483.382701873779, \"max\": 4483.382701873779}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:13 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.47804609104196 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:13 INFO 140718936323904] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.5180174112319946\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:13 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:15 INFO 140718936323904] Epoch[63] Batch[0] avg_epoch_loss=2.378903\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.3789029121398926\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:16 INFO 140718936323904] Epoch[63] Batch[5] avg_epoch_loss=2.402490\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.4024903376897178\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:16 INFO 140718936323904] Epoch[63] Batch [5]#011Speed: 206.27 samples/sec#011loss=2.402490\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:18 INFO 140718936323904] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519333.5731406, \"EndTime\": 1663519338.0536346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4479.977130889893, \"count\": 1, \"min\": 4479.977130889893, \"max\": 4479.977130889893}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:18 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.28240776613055 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:18 INFO 140718936323904] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.540097975730896\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:18 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:19 INFO 140718936323904] Epoch[64] Batch[0] avg_epoch_loss=2.615874\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.6158738136291504\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:21 INFO 140718936323904] Epoch[64] Batch[5] avg_epoch_loss=2.645902\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.6459020376205444\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:21 INFO 140718936323904] Epoch[64] Batch [5]#011Speed: 221.14 samples/sec#011loss=2.645902\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] Epoch[64] Batch[10] avg_epoch_loss=2.324712\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=1.9392850153148173\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] Epoch[64] Batch [10]#011Speed: 190.95 samples/sec#011loss=1.939285\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519338.053725, \"EndTime\": 1663519342.778273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4724.075794219971, \"count\": 1, \"min\": 4724.075794219971, \"max\": 4724.075794219971}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.9179753068376 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.324712482027032\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:22 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:24 INFO 140718936323904] Epoch[65] Batch[0] avg_epoch_loss=2.745901\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.745901346206665\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:26 INFO 140718936323904] Epoch[65] Batch[5] avg_epoch_loss=2.559048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.559048334757487\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:26 INFO 140718936323904] Epoch[65] Batch [5]#011Speed: 206.92 samples/sec#011loss=2.559048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:27 INFO 140718936323904] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519342.7783582, \"EndTime\": 1663519347.361286, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4582.46111869812, \"count\": 1, \"min\": 4582.46111869812, \"max\": 4582.46111869812}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:27 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.4398430833675 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:27 INFO 140718936323904] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.472766363620758\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:27 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:28 INFO 140718936323904] Epoch[66] Batch[0] avg_epoch_loss=2.523453\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.5234534740448\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:30 INFO 140718936323904] Epoch[66] Batch[5] avg_epoch_loss=2.364403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.3644028107325235\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:30 INFO 140718936323904] Epoch[66] Batch [5]#011Speed: 212.90 samples/sec#011loss=2.364403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] Epoch[66] Batch[10] avg_epoch_loss=2.218571\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=2.0435727715492247\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] Epoch[66] Batch [10]#011Speed: 174.00 samples/sec#011loss=2.043573\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519347.361409, \"EndTime\": 1663519352.2540817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4892.186641693115, \"count\": 1, \"min\": 4892.186641693115, \"max\": 4892.186641693115}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.94915644102642 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.218570974740115\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:32 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:33 INFO 140718936323904] Epoch[67] Batch[0] avg_epoch_loss=2.303884\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.3038835525512695\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:35 INFO 140718936323904] Epoch[67] Batch[5] avg_epoch_loss=2.383269\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.383269468943278\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:35 INFO 140718936323904] Epoch[67] Batch [5]#011Speed: 202.24 samples/sec#011loss=2.383269\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:36 INFO 140718936323904] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519352.254182, \"EndTime\": 1663519356.7531223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4498.455286026001, \"count\": 1, \"min\": 4498.455286026001, \"max\": 4498.455286026001}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:36 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.82187036792067 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:36 INFO 140718936323904] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.2985080003738405\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:36 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:38 INFO 140718936323904] Epoch[68] Batch[0] avg_epoch_loss=2.509106\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.509105920791626\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:39 INFO 140718936323904] Epoch[68] Batch[5] avg_epoch_loss=2.164162\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.164161741733551\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:39 INFO 140718936323904] Epoch[68] Batch [5]#011Speed: 213.48 samples/sec#011loss=2.164162\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:41 INFO 140718936323904] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519356.7532103, \"EndTime\": 1663519361.2384531, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4484.760046005249, \"count\": 1, \"min\": 4484.760046005249, \"max\": 4484.760046005249}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:41 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.2419567111635 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:41 INFO 140718936323904] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.2007504224777223\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:41 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:42 INFO 140718936323904] Epoch[69] Batch[0] avg_epoch_loss=2.133053\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.1330525875091553\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:44 INFO 140718936323904] Epoch[69] Batch[5] avg_epoch_loss=2.336904\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.336903711160024\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:44 INFO 140718936323904] Epoch[69] Batch [5]#011Speed: 210.48 samples/sec#011loss=2.336904\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:45 INFO 140718936323904] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519361.2385426, \"EndTime\": 1663519365.7951238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4556.073904037476, \"count\": 1, \"min\": 4556.073904037476, \"max\": 4556.073904037476}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:45 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.8831833352494 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:45 INFO 140718936323904] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.3639745235443117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:45 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:47 INFO 140718936323904] Epoch[70] Batch[0] avg_epoch_loss=2.698203\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.6982030868530273\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:48 INFO 140718936323904] Epoch[70] Batch[5] avg_epoch_loss=2.529953\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.529953400293986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:48 INFO 140718936323904] Epoch[70] Batch [5]#011Speed: 206.07 samples/sec#011loss=2.529953\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:49 INFO 140718936323904] processed a total of 567 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519365.79522, \"EndTime\": 1663519369.9559696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4160.224437713623, \"count\": 1, \"min\": 4160.224437713623, \"max\": 4160.224437713623}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:49 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.28652182743352 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:49 INFO 140718936323904] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.4439168903562756\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:49 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:51 INFO 140718936323904] Epoch[71] Batch[0] avg_epoch_loss=2.045759\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.0457592010498047\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:53 INFO 140718936323904] Epoch[71] Batch[5] avg_epoch_loss=2.641530\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.6415298779805503\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:53 INFO 140718936323904] Epoch[71] Batch [5]#011Speed: 208.78 samples/sec#011loss=2.641530\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] Epoch[71] Batch[10] avg_epoch_loss=2.534171\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=2.4053405284881593\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] Epoch[71] Batch [10]#011Speed: 180.40 samples/sec#011loss=2.405341\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519369.9560556, \"EndTime\": 1663519374.9048505, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4948.220014572144, \"count\": 1, \"min\": 4948.220014572144, \"max\": 4948.220014572144}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.7406802567694 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.534171082756736\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:54 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:56 INFO 140718936323904] Epoch[72] Batch[0] avg_epoch_loss=2.422089\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.422088861465454\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:58 INFO 140718936323904] Epoch[72] Batch[5] avg_epoch_loss=2.456747\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.4567474126815796\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:58 INFO 140718936323904] Epoch[72] Batch [5]#011Speed: 208.82 samples/sec#011loss=2.456747\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] Epoch[72] Batch[10] avg_epoch_loss=2.235010\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=1.968924629688263\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] Epoch[72] Batch [10]#011Speed: 181.98 samples/sec#011loss=1.968925\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519374.904925, \"EndTime\": 1663519379.8661177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4960.679292678833, \"count\": 1, \"min\": 4960.679292678833, \"max\": 4960.679292678833}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.83329824122453 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.2350097840482537\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:42:59 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:01 INFO 140718936323904] Epoch[73] Batch[0] avg_epoch_loss=2.058844\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.0588440895080566\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:03 INFO 140718936323904] Epoch[73] Batch[5] avg_epoch_loss=2.373247\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.373246709505717\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:03 INFO 140718936323904] Epoch[73] Batch [5]#011Speed: 211.20 samples/sec#011loss=2.373247\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] Epoch[73] Batch[10] avg_epoch_loss=2.595503\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.862209606170654\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] Epoch[73] Batch [10]#011Speed: 184.19 samples/sec#011loss=2.862210\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519379.8662093, \"EndTime\": 1663519384.8781543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5011.539459228516, \"count\": 1, \"min\": 5011.539459228516, \"max\": 5011.539459228516}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=128.50057919963982 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.595502571626143\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:04 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:06 INFO 140718936323904] Epoch[74] Batch[0] avg_epoch_loss=2.196566\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.196565628051758\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:08 INFO 140718936323904] Epoch[74] Batch[5] avg_epoch_loss=2.444825\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.4448245565096536\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:08 INFO 140718936323904] Epoch[74] Batch [5]#011Speed: 209.41 samples/sec#011loss=2.444825\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:09 INFO 140718936323904] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519384.878231, \"EndTime\": 1663519389.476835, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4598.199844360352, \"count\": 1, \"min\": 4598.199844360352, \"max\": 4598.199844360352}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:09 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.96178096923285 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:09 INFO 140718936323904] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.4805168986320494\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:09 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:11 INFO 140718936323904] Epoch[75] Batch[0] avg_epoch_loss=1.993819\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=1.993818759918213\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:12 INFO 140718936323904] Epoch[75] Batch[5] avg_epoch_loss=2.348881\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.348881483078003\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:12 INFO 140718936323904] Epoch[75] Batch [5]#011Speed: 208.40 samples/sec#011loss=2.348881\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] Epoch[75] Batch[10] avg_epoch_loss=2.132639\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=1.8731484070420266\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] Epoch[75] Batch [10]#011Speed: 176.08 samples/sec#011loss=1.873148\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519389.476924, \"EndTime\": 1663519394.4502964, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4972.833633422852, \"count\": 1, \"min\": 4972.833633422852, \"max\": 4972.833633422852}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.51682170731064 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.1326391757889227\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:14 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:16 INFO 140718936323904] Epoch[76] Batch[0] avg_epoch_loss=2.218976\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.2189764976501465\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:17 INFO 140718936323904] Epoch[76] Batch[5] avg_epoch_loss=2.121357\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.1213569045066833\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:17 INFO 140718936323904] Epoch[76] Batch [5]#011Speed: 203.16 samples/sec#011loss=2.121357\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] Epoch[76] Batch[10] avg_epoch_loss=2.160791\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=2.20811288356781\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] Epoch[76] Batch [10]#011Speed: 180.44 samples/sec#011loss=2.208113\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519394.4503736, \"EndTime\": 1663519399.4152281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4964.346885681152, \"count\": 1, \"min\": 4964.346885681152, \"max\": 4964.346885681152}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.31773169951953 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.160791440443559\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:19 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:21 INFO 140718936323904] Epoch[77] Batch[0] avg_epoch_loss=2.175842\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.175841808319092\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:22 INFO 140718936323904] Epoch[77] Batch[5] avg_epoch_loss=2.250283\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.25028258562088\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:22 INFO 140718936323904] Epoch[77] Batch [5]#011Speed: 210.34 samples/sec#011loss=2.250283\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:23 INFO 140718936323904] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519399.415359, \"EndTime\": 1663519403.9299078, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4514.108180999756, \"count\": 1, \"min\": 4514.108180999756, \"max\": 4514.108180999756}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:23 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.5518786191278 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:23 INFO 140718936323904] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.287654495239258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:23 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:25 INFO 140718936323904] Epoch[78] Batch[0] avg_epoch_loss=2.203518\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.2035176753997803\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:26 INFO 140718936323904] Epoch[78] Batch[5] avg_epoch_loss=2.323626\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.323626001675924\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:26 INFO 140718936323904] Epoch[78] Batch [5]#011Speed: 220.26 samples/sec#011loss=2.323626\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] Epoch[78] Batch[10] avg_epoch_loss=1.997575\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=1.6063144087791443\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] Epoch[78] Batch [10]#011Speed: 192.50 samples/sec#011loss=1.606314\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519403.9300025, \"EndTime\": 1663519408.6368792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.311464309692, \"count\": 1, \"min\": 4706.311464309692, \"max\": 4706.311464309692}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.68380387597446 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=78, train loss <loss>=1.997575277631933\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:28 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_fcc5f1f5-c4ad-43ea-9f8e-329271ac61c8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519408.6369646, \"EndTime\": 1663519408.728385, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 90.9111499786377, \"count\": 1, \"min\": 90.9111499786377, \"max\": 90.9111499786377}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:30 INFO 140718936323904] Epoch[79] Batch[0] avg_epoch_loss=2.221978\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.221978187561035\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:31 INFO 140718936323904] Epoch[79] Batch[5] avg_epoch_loss=2.280088\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.280088166395823\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:31 INFO 140718936323904] Epoch[79] Batch [5]#011Speed: 207.60 samples/sec#011loss=2.280088\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] Epoch[79] Batch[10] avg_epoch_loss=2.494554\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=2.7519136905670165\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] Epoch[79] Batch [10]#011Speed: 190.11 samples/sec#011loss=2.751914\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519408.7284722, \"EndTime\": 1663519413.6051555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4876.612663269043, \"count\": 1, \"min\": 4876.612663269043, \"max\": 4876.612663269043}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.8758914728417 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.4945543137463657\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:33 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:35 INFO 140718936323904] Epoch[80] Batch[0] avg_epoch_loss=2.774714\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.7747139930725098\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:36 INFO 140718936323904] Epoch[80] Batch[5] avg_epoch_loss=2.394227\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.394227166970571\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:36 INFO 140718936323904] Epoch[80] Batch [5]#011Speed: 218.59 samples/sec#011loss=2.394227\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:38 INFO 140718936323904] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519413.6052325, \"EndTime\": 1663519418.0046985, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4398.947477340698, \"count\": 1, \"min\": 4398.947477340698, \"max\": 4398.947477340698}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:38 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.43857616633417 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:38 INFO 140718936323904] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.3935671925544737\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:38 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:39 INFO 140718936323904] Epoch[81] Batch[0] avg_epoch_loss=1.998612\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=1.998612403869629\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:41 INFO 140718936323904] Epoch[81] Batch[5] avg_epoch_loss=1.961084\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=1.961084008216858\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:41 INFO 140718936323904] Epoch[81] Batch [5]#011Speed: 225.28 samples/sec#011loss=1.961084\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:42 INFO 140718936323904] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519418.004786, \"EndTime\": 1663519422.2395556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4234.312295913696, \"count\": 1, \"min\": 4234.312295913696, \"max\": 4234.312295913696}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=146.1817445151459 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.091491997241974\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:42 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:43 INFO 140718936323904] Epoch[82] Batch[0] avg_epoch_loss=2.154146\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.1541459560394287\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:45 INFO 140718936323904] Epoch[82] Batch[5] avg_epoch_loss=2.030749\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.0307489236195884\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:45 INFO 140718936323904] Epoch[82] Batch [5]#011Speed: 213.00 samples/sec#011loss=2.030749\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:46 INFO 140718936323904] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519422.2396557, \"EndTime\": 1663519426.6143396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4374.0904331207275, \"count\": 1, \"min\": 4374.0904331207275, \"max\": 4374.0904331207275}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:46 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.2825351061565 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:46 INFO 140718936323904] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.0563876748085024\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:46 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:48 INFO 140718936323904] Epoch[83] Batch[0] avg_epoch_loss=2.031571\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.03157114982605\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:49 INFO 140718936323904] Epoch[83] Batch[5] avg_epoch_loss=2.173725\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.173724671204885\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:49 INFO 140718936323904] Epoch[83] Batch [5]#011Speed: 206.53 samples/sec#011loss=2.173725\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:51 INFO 140718936323904] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519426.614423, \"EndTime\": 1663519431.1070528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4492.074012756348, \"count\": 1, \"min\": 4492.074012756348, \"max\": 4492.074012756348}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.80094890563248 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.1342123627662657\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:51 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:52 INFO 140718936323904] Epoch[84] Batch[0] avg_epoch_loss=2.094399\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.0943992137908936\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:54 INFO 140718936323904] Epoch[84] Batch[5] avg_epoch_loss=2.174205\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.174205223719279\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:54 INFO 140718936323904] Epoch[84] Batch [5]#011Speed: 214.09 samples/sec#011loss=2.174205\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] Epoch[84] Batch[10] avg_epoch_loss=2.297578\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=2.445624828338623\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] Epoch[84] Batch [10]#011Speed: 195.13 samples/sec#011loss=2.445625\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519431.1071475, \"EndTime\": 1663519435.8518999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4744.250774383545, \"count\": 1, \"min\": 4744.250774383545, \"max\": 4744.250774383545}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.26745356458383 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.2975777712735264\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:55 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:57 INFO 140718936323904] Epoch[85] Batch[0] avg_epoch_loss=2.479064\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.4790635108947754\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:59 INFO 140718936323904] Epoch[85] Batch[5] avg_epoch_loss=2.327182\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.327182133992513\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:43:59 INFO 140718936323904] Epoch[85] Batch [5]#011Speed: 204.59 samples/sec#011loss=2.327182\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] Epoch[85] Batch[10] avg_epoch_loss=2.265010\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=2.190404272079468\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] Epoch[85] Batch [10]#011Speed: 177.81 samples/sec#011loss=2.190404\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519435.852039, \"EndTime\": 1663519440.8113153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4958.775997161865, \"count\": 1, \"min\": 4958.775997161865, \"max\": 4958.775997161865}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.27088948553794 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.2650103785774927\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:02 INFO 140718936323904] Epoch[86] Batch[0] avg_epoch_loss=1.806116\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=1.8061163425445557\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:04 INFO 140718936323904] Epoch[86] Batch[5] avg_epoch_loss=2.182749\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.182748774687449\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:04 INFO 140718936323904] Epoch[86] Batch [5]#011Speed: 202.89 samples/sec#011loss=2.182749\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] Epoch[86] Batch[10] avg_epoch_loss=2.256829\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=2.3457247257232665\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] Epoch[86] Batch [10]#011Speed: 169.46 samples/sec#011loss=2.345725\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519440.8114002, \"EndTime\": 1663519445.9374359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5125.5528926849365, \"count\": 1, \"min\": 5125.5528926849365, \"max\": 5125.5528926849365}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.7387574614461 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.2568287524310024\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:05 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:07 INFO 140718936323904] Epoch[87] Batch[0] avg_epoch_loss=2.025259\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.025259017944336\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:09 INFO 140718936323904] Epoch[87] Batch[5] avg_epoch_loss=2.016485\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.016485075155894\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:09 INFO 140718936323904] Epoch[87] Batch [5]#011Speed: 207.60 samples/sec#011loss=2.016485\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] Epoch[87] Batch[10] avg_epoch_loss=2.067005\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=2.1276280641555787\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] Epoch[87] Batch [10]#011Speed: 194.73 samples/sec#011loss=2.127628\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519445.937529, \"EndTime\": 1663519450.6487827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4710.762023925781, \"count\": 1, \"min\": 4710.762023925781, \"max\": 4710.762023925781}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.70950792133615 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.067004615610296\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:10 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:12 INFO 140718936323904] Epoch[88] Batch[0] avg_epoch_loss=2.183646\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.1836459636688232\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:13 INFO 140718936323904] Epoch[88] Batch[5] avg_epoch_loss=2.264185\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.2641854286193848\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:13 INFO 140718936323904] Epoch[88] Batch [5]#011Speed: 222.99 samples/sec#011loss=2.264185\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] Epoch[88] Batch[10] avg_epoch_loss=1.951638\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=1.5765808999538422\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] Epoch[88] Batch [10]#011Speed: 185.13 samples/sec#011loss=1.576581\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519450.6488721, \"EndTime\": 1663519455.390629, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4741.219282150269, \"count\": 1, \"min\": 4741.219282150269, \"max\": 4741.219282150269}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.03738429981902 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=88, train loss <loss>=1.9516379155895927\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:15 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_01d3a28f-0181-49be-87ae-117c5224b6bd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519455.3907187, \"EndTime\": 1663519455.4529095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 61.54966354370117, \"count\": 1, \"min\": 61.54966354370117, \"max\": 61.54966354370117}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:17 INFO 140718936323904] Epoch[89] Batch[0] avg_epoch_loss=2.015187\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.0151872634887695\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:18 INFO 140718936323904] Epoch[89] Batch[5] avg_epoch_loss=2.306249\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.3062486251195273\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:18 INFO 140718936323904] Epoch[89] Batch [5]#011Speed: 207.95 samples/sec#011loss=2.306249\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:19 INFO 140718936323904] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519455.4529903, \"EndTime\": 1663519459.852867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4399.810314178467, \"count\": 1, \"min\": 4399.810314178467, \"max\": 4399.810314178467}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:19 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.45668865353002 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:19 INFO 140718936323904] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.179285430908203\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:19 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:21 INFO 140718936323904] Epoch[90] Batch[0] avg_epoch_loss=2.067905\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.0679054260253906\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:22 INFO 140718936323904] Epoch[90] Batch[5] avg_epoch_loss=2.071258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.071257690588633\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:22 INFO 140718936323904] Epoch[90] Batch [5]#011Speed: 209.10 samples/sec#011loss=2.071258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:24 INFO 140718936323904] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519459.8529546, \"EndTime\": 1663519464.2747624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4421.255588531494, \"count\": 1, \"min\": 4421.255588531494, \"max\": 4421.255588531494}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:24 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.12895781168737 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:24 INFO 140718936323904] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.149878430366516\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:24 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:25 INFO 140718936323904] Epoch[91] Batch[0] avg_epoch_loss=1.905477\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=1.9054774045944214\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:27 INFO 140718936323904] Epoch[91] Batch[5] avg_epoch_loss=2.050111\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.0501113136609397\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:27 INFO 140718936323904] Epoch[91] Batch [5]#011Speed: 219.78 samples/sec#011loss=2.050111\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] Epoch[91] Batch[10] avg_epoch_loss=2.153037\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=2.2765477895736694\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] Epoch[91] Batch [10]#011Speed: 184.50 samples/sec#011loss=2.276548\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519464.2748532, \"EndTime\": 1663519468.9535084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4678.108930587769, \"count\": 1, \"min\": 4678.108930587769, \"max\": 4678.108930587769}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=146.63657910493916 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.1530369845303623\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:28 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:30 INFO 140718936323904] Epoch[92] Batch[0] avg_epoch_loss=2.356415\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.356415271759033\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:32 INFO 140718936323904] Epoch[92] Batch[5] avg_epoch_loss=2.314649\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.3146491845448813\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:32 INFO 140718936323904] Epoch[92] Batch [5]#011Speed: 207.29 samples/sec#011loss=2.314649\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:33 INFO 140718936323904] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519468.9535944, \"EndTime\": 1663519473.4415812, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4487.545490264893, \"count\": 1, \"min\": 4487.545490264893, \"max\": 4487.545490264893}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.37905032267744 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.122560679912567\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:33 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:35 INFO 140718936323904] Epoch[93] Batch[0] avg_epoch_loss=1.874777\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=1.8747767210006714\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:36 INFO 140718936323904] Epoch[93] Batch[5] avg_epoch_loss=1.990784\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=1.9907835721969604\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:36 INFO 140718936323904] Epoch[93] Batch [5]#011Speed: 228.58 samples/sec#011loss=1.990784\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] Epoch[93] Batch[10] avg_epoch_loss=2.140749\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=2.3207069873809814\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] Epoch[93] Batch [10]#011Speed: 189.11 samples/sec#011loss=2.320707\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519473.4416728, \"EndTime\": 1663519478.1485775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.473112106323, \"count\": 1, \"min\": 4706.473112106323, \"max\": 4706.473112106323}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.19160110491248 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.14074876091697\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:38 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:39 INFO 140718936323904] Epoch[94] Batch[0] avg_epoch_loss=2.150673\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.1506731510162354\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:41 INFO 140718936323904] Epoch[94] Batch[5] avg_epoch_loss=2.107117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.1071166396141052\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:41 INFO 140718936323904] Epoch[94] Batch [5]#011Speed: 219.25 samples/sec#011loss=2.107117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] Epoch[94] Batch[10] avg_epoch_loss=1.923409\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=1.7029606461524964\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] Epoch[94] Batch [10]#011Speed: 202.53 samples/sec#011loss=1.702961\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519478.1486652, \"EndTime\": 1663519482.723619, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4574.515342712402, \"count\": 1, \"min\": 4574.515342712402, \"max\": 4574.515342712402}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=148.20868370229783 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=94, train loss <loss>=1.9234093698588284\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:42 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_f0572bf2-7557-4aef-8403-628e99cb1bab-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519482.7236948, \"EndTime\": 1663519482.7831686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 58.989524841308594, \"count\": 1, \"min\": 58.989524841308594, \"max\": 58.989524841308594}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:44 INFO 140718936323904] Epoch[95] Batch[0] avg_epoch_loss=1.944648\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=1.9446476697921753\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:45 INFO 140718936323904] Epoch[95] Batch[5] avg_epoch_loss=1.955620\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=1.9556201299031575\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:45 INFO 140718936323904] Epoch[95] Batch [5]#011Speed: 219.27 samples/sec#011loss=1.955620\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] Epoch[95] Batch[10] avg_epoch_loss=1.997854\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.0485336542129517\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] Epoch[95] Batch [10]#011Speed: 187.11 samples/sec#011loss=2.048534\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519482.7832596, \"EndTime\": 1663519487.548059, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4764.730453491211, \"count\": 1, \"min\": 4764.730453491211, \"max\": 4764.730453491211}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.30424703006116 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=95, train loss <loss>=1.9978535500439731\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:47 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:49 INFO 140718936323904] Epoch[96] Batch[0] avg_epoch_loss=1.922676\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=1.9226758480072021\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:50 INFO 140718936323904] Epoch[96] Batch[5] avg_epoch_loss=2.197554\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.1975543896357217\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:50 INFO 140718936323904] Epoch[96] Batch [5]#011Speed: 214.52 samples/sec#011loss=2.197554\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] Epoch[96] Batch[10] avg_epoch_loss=2.230213\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=2.269402766227722\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] Epoch[96] Batch [10]#011Speed: 195.03 samples/sec#011loss=2.269403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519487.548148, \"EndTime\": 1663519492.327078, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4778.413772583008, \"count\": 1, \"min\": 4778.413772583008, \"max\": 4778.413772583008}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.5600319798286 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.2302127426320855\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:52 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:53 INFO 140718936323904] Epoch[97] Batch[0] avg_epoch_loss=1.767218\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=1.7672182321548462\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:55 INFO 140718936323904] Epoch[97] Batch[5] avg_epoch_loss=2.094576\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.094575881958008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:55 INFO 140718936323904] Epoch[97] Batch [5]#011Speed: 201.04 samples/sec#011loss=2.094576\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:56 INFO 140718936323904] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519492.3271613, \"EndTime\": 1663519496.9089227, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4581.272125244141, \"count\": 1, \"min\": 4581.272125244141, \"max\": 4581.272125244141}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:56 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.73097046778884 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:56 INFO 140718936323904] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.024816834926605\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:56 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:58 INFO 140718936323904] Epoch[98] Batch[0] avg_epoch_loss=1.822111\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=1.822110891342163\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:59 INFO 140718936323904] Epoch[98] Batch[5] avg_epoch_loss=1.933327\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=1.933326780796051\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:44:59 INFO 140718936323904] Epoch[98] Batch [5]#011Speed: 224.83 samples/sec#011loss=1.933327\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] Epoch[98] Batch[10] avg_epoch_loss=2.160119\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=2.4322696208953856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] Epoch[98] Batch [10]#011Speed: 184.33 samples/sec#011loss=2.432270\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519496.909007, \"EndTime\": 1663519501.6780124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4768.322944641113, \"count\": 1, \"min\": 4768.322944641113, \"max\": 4768.322944641113}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.05463726049157 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.160118980841203\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:01 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:03 INFO 140718936323904] Epoch[99] Batch[0] avg_epoch_loss=1.604680\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=1.6046797037124634\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:04 INFO 140718936323904] Epoch[99] Batch[5] avg_epoch_loss=1.941393\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=1.941392680009206\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:04 INFO 140718936323904] Epoch[99] Batch [5]#011Speed: 200.09 samples/sec#011loss=1.941393\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519501.6780982, \"EndTime\": 1663519506.2833633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4604.613780975342, \"count\": 1, \"min\": 4604.613780975342, \"max\": 4604.613780975342}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.4269876032677 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=99, train loss <loss>=1.8633374333381654\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:06 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_88a36668-f920-4545-8706-6809a1a04704-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519506.283446, \"EndTime\": 1663519506.350722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 66.74480438232422, \"count\": 1, \"min\": 66.74480438232422, \"max\": 66.74480438232422}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:07 INFO 140718936323904] Epoch[100] Batch[0] avg_epoch_loss=2.432352\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.432352066040039\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:09 INFO 140718936323904] Epoch[100] Batch[5] avg_epoch_loss=2.179892\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.1798921823501587\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:09 INFO 140718936323904] Epoch[100] Batch [5]#011Speed: 220.82 samples/sec#011loss=2.179892\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:10 INFO 140718936323904] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519506.350801, \"EndTime\": 1663519510.730944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4380.070924758911, \"count\": 1, \"min\": 4380.070924758911, \"max\": 4380.070924758911}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.6001264970496 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.1199588537216187\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:10 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:12 INFO 140718936323904] Epoch[101] Batch[0] avg_epoch_loss=1.415474\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=1.4154736995697021\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:13 INFO 140718936323904] Epoch[101] Batch[5] avg_epoch_loss=1.943128\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=1.9431283076604207\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:13 INFO 140718936323904] Epoch[101] Batch [5]#011Speed: 208.68 samples/sec#011loss=1.943128\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] Epoch[101] Batch[10] avg_epoch_loss=1.894286\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=1.8356743812561036\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] Epoch[101] Batch [10]#011Speed: 197.87 samples/sec#011loss=1.835674\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519510.731046, \"EndTime\": 1663519515.491374, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4759.735107421875, \"count\": 1, \"min\": 4759.735107421875, \"max\": 4759.735107421875}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.50034732107954 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=101, train loss <loss>=1.8942856138402766\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:15 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:17 INFO 140718936323904] Epoch[102] Batch[0] avg_epoch_loss=2.686378\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.686378002166748\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:18 INFO 140718936323904] Epoch[102] Batch[5] avg_epoch_loss=2.107807\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.107806622982025\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:18 INFO 140718936323904] Epoch[102] Batch [5]#011Speed: 211.67 samples/sec#011loss=2.107807\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] Epoch[102] Batch[10] avg_epoch_loss=1.980352\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=1.8274060010910034\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] Epoch[102] Batch [10]#011Speed: 193.70 samples/sec#011loss=1.827406\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519515.4914527, \"EndTime\": 1663519520.2710319, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.1712284088135, \"count\": 1, \"min\": 4779.1712284088135, \"max\": 4779.1712284088135}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.72348694255285 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=102, train loss <loss>=1.9803517948497424\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:20 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:21 INFO 140718936323904] Epoch[103] Batch[0] avg_epoch_loss=2.232638\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.232638359069824\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:23 INFO 140718936323904] Epoch[103] Batch[5] avg_epoch_loss=2.275482\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.2754819989204407\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:23 INFO 140718936323904] Epoch[103] Batch [5]#011Speed: 210.54 samples/sec#011loss=2.275482\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] Epoch[103] Batch[10] avg_epoch_loss=2.374742\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=2.4938543319702147\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] Epoch[103] Batch [10]#011Speed: 180.92 samples/sec#011loss=2.493854\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519520.2711158, \"EndTime\": 1663519525.1244953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4852.922439575195, \"count\": 1, \"min\": 4852.922439575195, \"max\": 4852.922439575195}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.32376777670632 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.3747421503067017\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:25 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:26 INFO 140718936323904] Epoch[104] Batch[0] avg_epoch_loss=2.746875\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.7468745708465576\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:28 INFO 140718936323904] Epoch[104] Batch[5] avg_epoch_loss=2.292986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.2929863135019937\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:28 INFO 140718936323904] Epoch[104] Batch [5]#011Speed: 234.12 samples/sec#011loss=2.292986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:29 INFO 140718936323904] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519525.1245959, \"EndTime\": 1663519529.3697062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4244.652986526489, \"count\": 1, \"min\": 4244.652986526489, \"max\": 4244.652986526489}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:29 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.99922011429973 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:29 INFO 140718936323904] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.006772351264954\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:29 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:31 INFO 140718936323904] Epoch[105] Batch[0] avg_epoch_loss=1.659528\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=1.659528374671936\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:32 INFO 140718936323904] Epoch[105] Batch[5] avg_epoch_loss=2.008444\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.0084437926610312\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:32 INFO 140718936323904] Epoch[105] Batch [5]#011Speed: 222.13 samples/sec#011loss=2.008444\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:33 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519529.3697965, \"EndTime\": 1663519533.6751354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4304.854154586792, \"count\": 1, \"min\": 4304.854154586792, \"max\": 4304.854154586792}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=147.26756295795286 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.123796534538269\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:33 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:35 INFO 140718936323904] Epoch[106] Batch[0] avg_epoch_loss=1.863714\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=1.8637144565582275\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:36 INFO 140718936323904] Epoch[106] Batch[5] avg_epoch_loss=1.877222\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=1.877221941947937\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:36 INFO 140718936323904] Epoch[106] Batch [5]#011Speed: 206.39 samples/sec#011loss=1.877222\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:38 INFO 140718936323904] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519533.675327, \"EndTime\": 1663519538.0937424, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4417.875051498413, \"count\": 1, \"min\": 4417.875051498413, \"max\": 4417.875051498413}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:38 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.05134019963182 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:38 INFO 140718936323904] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=106, train loss <loss>=1.9296784400939941\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:38 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:39 INFO 140718936323904] Epoch[107] Batch[0] avg_epoch_loss=1.322008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.322007656097412\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:41 INFO 140718936323904] Epoch[107] Batch[5] avg_epoch_loss=2.085616\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.0856159925460815\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:41 INFO 140718936323904] Epoch[107] Batch [5]#011Speed: 216.69 samples/sec#011loss=2.085616\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] Epoch[107] Batch[10] avg_epoch_loss=2.193155\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=2.322201681137085\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] Epoch[107] Batch [10]#011Speed: 212.38 samples/sec#011loss=2.322202\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519538.0938253, \"EndTime\": 1663519542.6697028, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4575.442552566528, \"count\": 1, \"min\": 4575.442552566528, \"max\": 4575.442552566528}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.68142430134606 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=107, train loss <loss>=2.1931549419056284\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:42 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:44 INFO 140718936323904] Epoch[108] Batch[0] avg_epoch_loss=2.024897\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.0248968601226807\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:45 INFO 140718936323904] Epoch[108] Batch[5] avg_epoch_loss=2.040286\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.0402855277061462\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:45 INFO 140718936323904] Epoch[108] Batch [5]#011Speed: 210.20 samples/sec#011loss=2.040286\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:47 INFO 140718936323904] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519542.6697924, \"EndTime\": 1663519547.0379255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4367.619037628174, \"count\": 1, \"min\": 4367.619037628174, \"max\": 4367.619037628174}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:47 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.3232080070425 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:47 INFO 140718936323904] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.188975822925568\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:47 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:48 INFO 140718936323904] Epoch[109] Batch[0] avg_epoch_loss=2.163359\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.163358688354492\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:50 INFO 140718936323904] Epoch[109] Batch[5] avg_epoch_loss=1.979117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=1.979116936524709\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:50 INFO 140718936323904] Epoch[109] Batch [5]#011Speed: 206.29 samples/sec#011loss=1.979117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] Epoch[109] Batch[10] avg_epoch_loss=1.878335\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=1.7573967278003693\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] Epoch[109] Batch [10]#011Speed: 187.11 samples/sec#011loss=1.757397\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519547.0380156, \"EndTime\": 1663519551.9231703, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4884.636878967285, \"count\": 1, \"min\": 4884.636878967285, \"max\": 4884.636878967285}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.13805662453575 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=109, train loss <loss>=1.878335023468191\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:51 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:53 INFO 140718936323904] Epoch[110] Batch[0] avg_epoch_loss=2.401706\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.4017062187194824\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:55 INFO 140718936323904] Epoch[110] Batch[5] avg_epoch_loss=2.072660\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.0726596315701804\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:55 INFO 140718936323904] Epoch[110] Batch [5]#011Speed: 203.83 samples/sec#011loss=2.072660\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] Epoch[110] Batch[10] avg_epoch_loss=2.002908\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=1.9192060470581054\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] Epoch[110] Batch [10]#011Speed: 175.97 samples/sec#011loss=1.919206\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519551.923245, \"EndTime\": 1663519556.9278123, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5004.1608810424805, \"count\": 1, \"min\": 5004.1608810424805, \"max\": 5004.1608810424805}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.08791764015172 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.0029080022465098\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:56 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:58 INFO 140718936323904] Epoch[111] Batch[0] avg_epoch_loss=1.825385\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:45:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=1.8253854513168335\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:00 INFO 140718936323904] Epoch[111] Batch[5] avg_epoch_loss=2.056818\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.056818167368571\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:00 INFO 140718936323904] Epoch[111] Batch [5]#011Speed: 207.30 samples/sec#011loss=2.056818\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:01 INFO 140718936323904] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519556.9278946, \"EndTime\": 1663519561.4391642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4510.80322265625, \"count\": 1, \"min\": 4510.80322265625, \"max\": 4510.80322265625}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:01 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.57203270372307 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:01 INFO 140718936323904] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=111, train loss <loss>=2.1544380664825438\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:01 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:03 INFO 140718936323904] Epoch[112] Batch[0] avg_epoch_loss=1.886111\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.8861113786697388\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:04 INFO 140718936323904] Epoch[112] Batch[5] avg_epoch_loss=1.878331\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=1.878330926100413\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:04 INFO 140718936323904] Epoch[112] Batch [5]#011Speed: 207.88 samples/sec#011loss=1.878331\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] Epoch[112] Batch[10] avg_epoch_loss=1.967396\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=2.074274706840515\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] Epoch[112] Batch [10]#011Speed: 178.55 samples/sec#011loss=2.074275\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519561.439241, \"EndTime\": 1663519566.3715653, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4931.889533996582, \"count\": 1, \"min\": 4931.889533996582, \"max\": 4931.889533996582}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.48297441498548 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=112, train loss <loss>=1.9673962809822776\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:06 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:07 INFO 140718936323904] Epoch[113] Batch[0] avg_epoch_loss=2.242387\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=2.242386817932129\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:09 INFO 140718936323904] Epoch[113] Batch[5] avg_epoch_loss=1.834391\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=1.8343907395998638\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:09 INFO 140718936323904] Epoch[113] Batch [5]#011Speed: 205.64 samples/sec#011loss=1.834391\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] Epoch[113] Batch[10] avg_epoch_loss=1.682607\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=1.5004673421382904\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] Epoch[113] Batch [10]#011Speed: 179.43 samples/sec#011loss=1.500467\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519566.371656, \"EndTime\": 1663519571.3270686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4954.962968826294, \"count\": 1, \"min\": 4954.962968826294, \"max\": 4954.962968826294}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.78421877923805 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=113, train loss <loss>=1.6826073771173304\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:11 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_8e1ba563-8f6a-4f1f-bead-034c23183358-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519571.3271372, \"EndTime\": 1663519571.3858304, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 58.145999908447266, \"count\": 1, \"min\": 58.145999908447266, \"max\": 58.145999908447266}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:12 INFO 140718936323904] Epoch[114] Batch[0] avg_epoch_loss=1.644915\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=1.6449146270751953\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:14 INFO 140718936323904] Epoch[114] Batch[5] avg_epoch_loss=2.092470\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=2.092470129330953\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:14 INFO 140718936323904] Epoch[114] Batch [5]#011Speed: 208.09 samples/sec#011loss=2.092470\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] Epoch[114] Batch[10] avg_epoch_loss=2.042424\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=1.9823680162429809\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] Epoch[114] Batch [10]#011Speed: 181.27 samples/sec#011loss=1.982368\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] processed a total of 700 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519571.385907, \"EndTime\": 1663519576.2260976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4840.1198387146, \"count\": 1, \"min\": 4840.1198387146, \"max\": 4840.1198387146}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.620628643823 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=114, train loss <loss>=2.042423714290966\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:16 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:17 INFO 140718936323904] Epoch[115] Batch[0] avg_epoch_loss=2.167253\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.167253017425537\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:19 INFO 140718936323904] Epoch[115] Batch[5] avg_epoch_loss=2.068355\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=2.068354586760203\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:19 INFO 140718936323904] Epoch[115] Batch [5]#011Speed: 212.21 samples/sec#011loss=2.068355\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:20 INFO 140718936323904] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519576.226187, \"EndTime\": 1663519580.524933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4298.2118129730225, \"count\": 1, \"min\": 4298.2118129730225, \"max\": 4298.2118129730225}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:20 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.54356895266025 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:20 INFO 140718936323904] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=115, train loss <loss>=2.1050650477409363\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:20 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:22 INFO 140718936323904] Epoch[116] Batch[0] avg_epoch_loss=1.510421\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=1.5104213953018188\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:23 INFO 140718936323904] Epoch[116] Batch[5] avg_epoch_loss=1.953185\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=1.9531847635904949\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:23 INFO 140718936323904] Epoch[116] Batch [5]#011Speed: 209.39 samples/sec#011loss=1.953185\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:24 INFO 140718936323904] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519580.5250287, \"EndTime\": 1663519584.9110506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4385.47420501709, \"count\": 1, \"min\": 4385.47420501709, \"max\": 4385.47420501709}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:24 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.73975662906767 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:24 INFO 140718936323904] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=116, train loss <loss>=2.059776520729065\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:24 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:26 INFO 140718936323904] Epoch[117] Batch[0] avg_epoch_loss=1.745784\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=1.745784044265747\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:28 INFO 140718936323904] Epoch[117] Batch[5] avg_epoch_loss=1.921845\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=1.9218453963597615\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:28 INFO 140718936323904] Epoch[117] Batch [5]#011Speed: 205.16 samples/sec#011loss=1.921845\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] Epoch[117] Batch[10] avg_epoch_loss=1.838829\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=1.7392086505889892\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] Epoch[117] Batch [10]#011Speed: 197.50 samples/sec#011loss=1.739209\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519584.9111426, \"EndTime\": 1663519589.6685042, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4756.861209869385, \"count\": 1, \"min\": 4756.861209869385, \"max\": 4756.861209869385}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.7947173745251 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=117, train loss <loss>=1.8388286937366833\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:29 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:31 INFO 140718936323904] Epoch[118] Batch[0] avg_epoch_loss=1.325089\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=1.3250887393951416\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:32 INFO 140718936323904] Epoch[118] Batch[5] avg_epoch_loss=1.904855\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=1.904854695002238\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:32 INFO 140718936323904] Epoch[118] Batch [5]#011Speed: 214.44 samples/sec#011loss=1.904855\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] Epoch[118] Batch[10] avg_epoch_loss=1.758293\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=1.5824192643165589\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] Epoch[118] Batch [10]#011Speed: 187.03 samples/sec#011loss=1.582419\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519589.668579, \"EndTime\": 1663519594.4909308, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4821.798324584961, \"count\": 1, \"min\": 4821.798324584961, \"max\": 4821.798324584961}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.04524703059934 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=118, train loss <loss>=1.7582931355996565\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:34 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:36 INFO 140718936323904] Epoch[119] Batch[0] avg_epoch_loss=1.679685\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=1.6796846389770508\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:37 INFO 140718936323904] Epoch[119] Batch[5] avg_epoch_loss=1.988248\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=1.9882475932439168\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:37 INFO 140718936323904] Epoch[119] Batch [5]#011Speed: 224.08 samples/sec#011loss=1.988248\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] Epoch[119] Batch[10] avg_epoch_loss=1.741752\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=1.4459566831588746\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] Epoch[119] Batch [10]#011Speed: 202.93 samples/sec#011loss=1.445957\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519594.4910197, \"EndTime\": 1663519599.0839162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4592.362642288208, \"count\": 1, \"min\": 4592.362642288208, \"max\": 4592.362642288208}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=140.6650838091787 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=119, train loss <loss>=1.741751725023443\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:39 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:40 INFO 140718936323904] Epoch[120] Batch[0] avg_epoch_loss=1.813293\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=1.813293218612671\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:42 INFO 140718936323904] Epoch[120] Batch[5] avg_epoch_loss=1.938164\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=1.9381644129753113\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:42 INFO 140718936323904] Epoch[120] Batch [5]#011Speed: 207.30 samples/sec#011loss=1.938164\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] Epoch[120] Batch[10] avg_epoch_loss=1.876893\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=1.8033669710159301\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] Epoch[120] Batch [10]#011Speed: 188.37 samples/sec#011loss=1.803367\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519599.083985, \"EndTime\": 1663519603.8708987, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4786.457538604736, \"count\": 1, \"min\": 4786.457538604736, \"max\": 4786.457538604736}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.56996718949364 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=120, train loss <loss>=1.8768928484483198\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:43 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:45 INFO 140718936323904] Epoch[121] Batch[0] avg_epoch_loss=1.982792\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=1.9827920198440552\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:47 INFO 140718936323904] Epoch[121] Batch[5] avg_epoch_loss=1.983075\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=1.9830747445424397\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:47 INFO 140718936323904] Epoch[121] Batch [5]#011Speed: 205.99 samples/sec#011loss=1.983075\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] Epoch[121] Batch[10] avg_epoch_loss=2.166695\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=2.3870397329330446\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] Epoch[121] Batch [10]#011Speed: 187.65 samples/sec#011loss=2.387040\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519603.871008, \"EndTime\": 1663519608.7348611, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4863.312005996704, \"count\": 1, \"min\": 4863.312005996704, \"max\": 4863.312005996704}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.70633936175622 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=121, train loss <loss>=2.1666951938108965\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:48 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:50 INFO 140718936323904] Epoch[122] Batch[0] avg_epoch_loss=2.529173\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=2.529172897338867\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:51 INFO 140718936323904] Epoch[122] Batch[5] avg_epoch_loss=2.037050\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=2.037050267060598\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:51 INFO 140718936323904] Epoch[122] Batch [5]#011Speed: 204.08 samples/sec#011loss=2.037050\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] Epoch[122] Batch[10] avg_epoch_loss=1.633354\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=1.1489189863204956\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] Epoch[122] Batch [10]#011Speed: 187.04 samples/sec#011loss=1.148919\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519608.7349494, \"EndTime\": 1663519613.6880531, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4952.583789825439, \"count\": 1, \"min\": 4952.583789825439, \"max\": 4952.583789825439}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.23185938426 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=122, train loss <loss>=1.6333542303605513\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:53 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_e4950936-5d7e-4f66-8839-7a1eed85d118-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519613.6881385, \"EndTime\": 1663519613.776209, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 87.56303787231445, \"count\": 1, \"min\": 87.56303787231445, \"max\": 87.56303787231445}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:55 INFO 140718936323904] Epoch[123] Batch[0] avg_epoch_loss=2.173691\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=2.1736905574798584\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:57 INFO 140718936323904] Epoch[123] Batch[5] avg_epoch_loss=2.020646\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=2.0206457376480103\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:57 INFO 140718936323904] Epoch[123] Batch [5]#011Speed: 201.85 samples/sec#011loss=2.020646\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] Epoch[123] Batch[10] avg_epoch_loss=1.845158\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=1.6345731019973755\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] Epoch[123] Batch [10]#011Speed: 191.19 samples/sec#011loss=1.634573\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519613.7762928, \"EndTime\": 1663519618.7165933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4940.212249755859, \"count\": 1, \"min\": 4940.212249755859, \"max\": 4940.212249755859}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.17776798426 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=123, train loss <loss>=1.8451581759886309\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:46:58 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:00 INFO 140718936323904] Epoch[124] Batch[0] avg_epoch_loss=2.019700\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=2.019699811935425\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:01 INFO 140718936323904] Epoch[124] Batch[5] avg_epoch_loss=1.897318\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=1.8973181049029033\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:01 INFO 140718936323904] Epoch[124] Batch [5]#011Speed: 199.59 samples/sec#011loss=1.897318\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] Epoch[124] Batch[10] avg_epoch_loss=2.036960\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=2.2045305490493776\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] Epoch[124] Batch [10]#011Speed: 181.90 samples/sec#011loss=2.204531\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519618.7166584, \"EndTime\": 1663519623.7537663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5036.650896072388, \"count\": 1, \"min\": 5036.650896072388, \"max\": 5036.650896072388}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.05045563423386 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=124, train loss <loss>=2.0369601249694824\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:03 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:05 INFO 140718936323904] Epoch[125] Batch[0] avg_epoch_loss=2.269194\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=2.2691943645477295\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:07 INFO 140718936323904] Epoch[125] Batch[5] avg_epoch_loss=2.077207\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=2.0772069493929544\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:07 INFO 140718936323904] Epoch[125] Batch [5]#011Speed: 209.06 samples/sec#011loss=2.077207\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:08 INFO 140718936323904] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519623.7538636, \"EndTime\": 1663519628.4040155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4649.6217250823975, \"count\": 1, \"min\": 4649.6217250823975, \"max\": 4649.6217250823975}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:08 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.265433425286 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:08 INFO 140718936323904] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=125, train loss <loss>=2.09396835565567\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:08 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:10 INFO 140718936323904] Epoch[126] Batch[0] avg_epoch_loss=1.716449\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=1.716449499130249\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:11 INFO 140718936323904] Epoch[126] Batch[5] avg_epoch_loss=1.661260\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=1.6612595319747925\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:11 INFO 140718936323904] Epoch[126] Batch [5]#011Speed: 218.26 samples/sec#011loss=1.661260\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:12 INFO 140718936323904] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519628.4040964, \"EndTime\": 1663519632.7259176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4321.263313293457, \"count\": 1, \"min\": 4321.263313293457, \"max\": 4321.263313293457}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:12 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=147.6378065288961 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:12 INFO 140718936323904] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=126, train loss <loss>=1.6932084679603576\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:12 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:14 INFO 140718936323904] Epoch[127] Batch[0] avg_epoch_loss=1.341026\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=1.3410263061523438\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:15 INFO 140718936323904] Epoch[127] Batch[5] avg_epoch_loss=1.756795\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=1.756794512271881\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:15 INFO 140718936323904] Epoch[127] Batch [5]#011Speed: 201.87 samples/sec#011loss=1.756795\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:17 INFO 140718936323904] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519632.7260058, \"EndTime\": 1663519637.204888, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4478.301286697388, \"count\": 1, \"min\": 4478.301286697388, \"max\": 4478.301286697388}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:17 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.7910246572297 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:17 INFO 140718936323904] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=127, train loss <loss>=1.6793064951896668\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:17 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:18 INFO 140718936323904] Epoch[128] Batch[0] avg_epoch_loss=2.103926\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=2.1039257049560547\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:20 INFO 140718936323904] Epoch[128] Batch[5] avg_epoch_loss=1.906370\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=1.9063702424367268\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:20 INFO 140718936323904] Epoch[128] Batch [5]#011Speed: 213.87 samples/sec#011loss=1.906370\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] Epoch[128] Batch[10] avg_epoch_loss=2.103580\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=2.3402310609817505\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] Epoch[128] Batch [10]#011Speed: 188.82 samples/sec#011loss=2.340231\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519637.204976, \"EndTime\": 1663519642.0141726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4808.728933334351, \"count\": 1, \"min\": 4808.728933334351, \"max\": 4808.728933334351}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.03935288762213 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=128, train loss <loss>=2.1035797054117378\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:22 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:23 INFO 140718936323904] Epoch[129] Batch[0] avg_epoch_loss=1.681697\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=1.6816973686218262\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:25 INFO 140718936323904] Epoch[129] Batch[5] avg_epoch_loss=2.054355\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=2.054355005423228\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:25 INFO 140718936323904] Epoch[129] Batch [5]#011Speed: 201.59 samples/sec#011loss=2.054355\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:26 INFO 140718936323904] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519642.0142477, \"EndTime\": 1663519646.5734377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4558.744192123413, \"count\": 1, \"min\": 4558.744192123413, \"max\": 4558.744192123413}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:26 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.46346667562005 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:26 INFO 140718936323904] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=129, train loss <loss>=2.0325332403182985\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:26 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:28 INFO 140718936323904] Epoch[130] Batch[0] avg_epoch_loss=1.928594\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=1.9285938739776611\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:29 INFO 140718936323904] Epoch[130] Batch[5] avg_epoch_loss=1.826563\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:29 INFO 140718936323904] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=1.8265625437100728\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:29 INFO 140718936323904] Epoch[130] Batch [5]#011Speed: 200.59 samples/sec#011loss=1.826563\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] Epoch[130] Batch[10] avg_epoch_loss=1.549616\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=1.217281174659729\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] Epoch[130] Batch [10]#011Speed: 205.27 samples/sec#011loss=1.217281\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519646.5735185, \"EndTime\": 1663519651.4121146, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4838.063716888428, \"count\": 1, \"min\": 4838.063716888428, \"max\": 4838.063716888428}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.52106386733325 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=130, train loss <loss>=1.5496164668690076\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:31 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_3cb38671-baa8-47cc-99d9-b5398feaa28c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519651.412203, \"EndTime\": 1663519651.502708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.9965763092041, \"count\": 1, \"min\": 89.9965763092041, \"max\": 89.9965763092041}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:33 INFO 140718936323904] Epoch[131] Batch[0] avg_epoch_loss=1.905365\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=1.905364751815796\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:34 INFO 140718936323904] Epoch[131] Batch[5] avg_epoch_loss=1.774006\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=1.7740063071250916\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:34 INFO 140718936323904] Epoch[131] Batch [5]#011Speed: 222.75 samples/sec#011loss=1.774006\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] Epoch[131] Batch[10] avg_epoch_loss=1.774786\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=1.7757218956947327\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] Epoch[131] Batch [10]#011Speed: 182.63 samples/sec#011loss=1.775722\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519651.5027962, \"EndTime\": 1663519656.3448722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4842.007637023926, \"count\": 1, \"min\": 4842.007637023926, \"max\": 4842.007637023926}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.60764717288248 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=131, train loss <loss>=1.774786120111292\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:36 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:37 INFO 140718936323904] Epoch[132] Batch[0] avg_epoch_loss=2.385898\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=2.3858983516693115\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:39 INFO 140718936323904] Epoch[132] Batch[5] avg_epoch_loss=2.090386\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=2.0903857747713723\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:39 INFO 140718936323904] Epoch[132] Batch [5]#011Speed: 204.90 samples/sec#011loss=2.090386\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] Epoch[132] Batch[10] avg_epoch_loss=2.301679\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=2.555230188369751\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] Epoch[132] Batch [10]#011Speed: 176.85 samples/sec#011loss=2.555230\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519656.344967, \"EndTime\": 1663519661.3332663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4987.755060195923, \"count\": 1, \"min\": 4987.755060195923, \"max\": 4987.755060195923}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=128.71196118713289 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=132, train loss <loss>=2.301678690043363\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:41 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:42 INFO 140718936323904] Epoch[133] Batch[0] avg_epoch_loss=1.903091\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=1.9030908346176147\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:44 INFO 140718936323904] Epoch[133] Batch[5] avg_epoch_loss=1.817367\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=1.8173667987187703\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:44 INFO 140718936323904] Epoch[133] Batch [5]#011Speed: 213.38 samples/sec#011loss=1.817367\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] Epoch[133] Batch[10] avg_epoch_loss=1.864564\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=1.9212010264396668\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] Epoch[133] Batch [10]#011Speed: 187.87 samples/sec#011loss=1.921201\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519661.3333511, \"EndTime\": 1663519666.2001753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4866.29581451416, \"count\": 1, \"min\": 4866.29581451416, \"max\": 4866.29581451416}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.5960138986289 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=133, train loss <loss>=1.8645641749555415\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:46 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:47 INFO 140718936323904] Epoch[134] Batch[0] avg_epoch_loss=2.087025\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=2.0870254039764404\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:49 INFO 140718936323904] Epoch[134] Batch[5] avg_epoch_loss=1.939030\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=1.939030408859253\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:49 INFO 140718936323904] Epoch[134] Batch [5]#011Speed: 223.35 samples/sec#011loss=1.939030\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] Epoch[134] Batch[10] avg_epoch_loss=2.055081\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=2.1943419694900514\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] Epoch[134] Batch [10]#011Speed: 192.34 samples/sec#011loss=2.194342\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519666.2002575, \"EndTime\": 1663519670.8464668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4645.682573318481, \"count\": 1, \"min\": 4645.682573318481, \"max\": 4645.682573318481}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=145.50713964531016 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=134, train loss <loss>=2.0550811182368887\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:50 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:52 INFO 140718936323904] Epoch[135] Batch[0] avg_epoch_loss=2.042845\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=2.0428454875946045\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:53 INFO 140718936323904] Epoch[135] Batch[5] avg_epoch_loss=1.762124\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=1.7621235847473145\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:53 INFO 140718936323904] Epoch[135] Batch [5]#011Speed: 204.88 samples/sec#011loss=1.762124\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:55 INFO 140718936323904] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519670.8465645, \"EndTime\": 1663519675.360949, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4513.814926147461, \"count\": 1, \"min\": 4513.814926147461, \"max\": 4513.814926147461}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:55 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.24409731458053 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:55 INFO 140718936323904] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=135, train loss <loss>=1.651607131958008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:55 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:56 INFO 140718936323904] Epoch[136] Batch[0] avg_epoch_loss=2.138437\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=2.1384365558624268\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:58 INFO 140718936323904] Epoch[136] Batch[5] avg_epoch_loss=1.815422\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=1.8154216607411702\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:47:58 INFO 140718936323904] Epoch[136] Batch [5]#011Speed: 205.69 samples/sec#011loss=1.815422\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] Epoch[136] Batch[10] avg_epoch_loss=1.898918\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=1.9991129398345948\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] Epoch[136] Batch [10]#011Speed: 180.02 samples/sec#011loss=1.999113\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519675.361049, \"EndTime\": 1663519680.2477582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4886.1985206604, \"count\": 1, \"min\": 4886.1985206604, \"max\": 4886.1985206604}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.93472586716376 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=136, train loss <loss>=1.8989176966927268\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:01 INFO 140718936323904] Epoch[137] Batch[0] avg_epoch_loss=2.410388\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:01 INFO 140718936323904] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=2.4103879928588867\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:03 INFO 140718936323904] Epoch[137] Batch[5] avg_epoch_loss=1.920735\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=1.920735200246175\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:03 INFO 140718936323904] Epoch[137] Batch [5]#011Speed: 200.28 samples/sec#011loss=1.920735\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:04 INFO 140718936323904] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519680.2478926, \"EndTime\": 1663519684.900332, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4651.915073394775, \"count\": 1, \"min\": 4651.915073394775, \"max\": 4651.915073394775}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:04 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.20861237672966 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:04 INFO 140718936323904] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=137, train loss <loss>=1.9600528717041015\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:04 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:06 INFO 140718936323904] Epoch[138] Batch[0] avg_epoch_loss=1.740696\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=1.7406961917877197\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:08 INFO 140718936323904] Epoch[138] Batch[5] avg_epoch_loss=1.656988\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=1.6569878458976746\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:08 INFO 140718936323904] Epoch[138] Batch [5]#011Speed: 215.75 samples/sec#011loss=1.656988\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:09 INFO 140718936323904] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519684.900422, \"EndTime\": 1663519689.3356378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4434.773206710815, \"count\": 1, \"min\": 4434.773206710815, \"max\": 4434.773206710815}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:09 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.22185969110248 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:09 INFO 140718936323904] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:09 INFO 140718936323904] #quality_metric: host=algo-1, epoch=138, train loss <loss>=1.8291149735450745\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:09 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:11 INFO 140718936323904] Epoch[139] Batch[0] avg_epoch_loss=2.387852\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=2.3878519535064697\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:12 INFO 140718936323904] Epoch[139] Batch[5] avg_epoch_loss=1.985691\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=1.9856913089752197\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:12 INFO 140718936323904] Epoch[139] Batch [5]#011Speed: 223.08 samples/sec#011loss=1.985691\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:13 INFO 140718936323904] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519689.3357198, \"EndTime\": 1663519693.6927495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4356.453418731689, \"count\": 1, \"min\": 4356.453418731689, \"max\": 4356.453418731689}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:13 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.14998122345165 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:13 INFO 140718936323904] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=139, train loss <loss>=1.9933990478515624\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:13 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:15 INFO 140718936323904] Epoch[140] Batch[0] avg_epoch_loss=1.326244\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=1.3262437582015991\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:16 INFO 140718936323904] Epoch[140] Batch[5] avg_epoch_loss=1.887226\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=1.8872261842091878\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:16 INFO 140718936323904] Epoch[140] Batch [5]#011Speed: 218.95 samples/sec#011loss=1.887226\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] Epoch[140] Batch[10] avg_epoch_loss=2.039503\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=2.222235107421875\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] Epoch[140] Batch [10]#011Speed: 198.47 samples/sec#011loss=2.222235\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519693.6928282, \"EndTime\": 1663519698.3684502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4675.068140029907, \"count\": 1, \"min\": 4675.068140029907, \"max\": 4675.068140029907}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.37910824907217 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=140, train loss <loss>=2.039502967487682\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:18 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:19 INFO 140718936323904] Epoch[141] Batch[0] avg_epoch_loss=2.196279\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=2.1962788105010986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:21 INFO 140718936323904] Epoch[141] Batch[5] avg_epoch_loss=1.643263\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=1.6432629625002544\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:21 INFO 140718936323904] Epoch[141] Batch [5]#011Speed: 203.93 samples/sec#011loss=1.643263\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:22 INFO 140718936323904] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519698.3685358, \"EndTime\": 1663519702.891135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4522.137880325317, \"count\": 1, \"min\": 4522.137880325317, \"max\": 4522.137880325317}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:22 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.32044656169668 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:22 INFO 140718936323904] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=141, train loss <loss>=1.7076716303825379\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:22 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:24 INFO 140718936323904] Epoch[142] Batch[0] avg_epoch_loss=1.966893\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=1.9668933153152466\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:25 INFO 140718936323904] Epoch[142] Batch[5] avg_epoch_loss=1.760444\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=1.760444442431132\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:25 INFO 140718936323904] Epoch[142] Batch [5]#011Speed: 222.38 samples/sec#011loss=1.760444\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:27 INFO 140718936323904] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519702.8912225, \"EndTime\": 1663519707.2466466, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4354.843378067017, \"count\": 1, \"min\": 4354.843378067017, \"max\": 4354.843378067017}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:27 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.4331103655127 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:27 INFO 140718936323904] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=142, train loss <loss>=1.719705057144165\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:27 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:28 INFO 140718936323904] Epoch[143] Batch[0] avg_epoch_loss=1.743859\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=1.743859052658081\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:30 INFO 140718936323904] Epoch[143] Batch[5] avg_epoch_loss=1.796137\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=1.7961368163426716\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:30 INFO 140718936323904] Epoch[143] Batch [5]#011Speed: 200.86 samples/sec#011loss=1.796137\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] Epoch[143] Batch[10] avg_epoch_loss=1.624897\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=1.4194087147712708\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] Epoch[143] Batch [10]#011Speed: 183.43 samples/sec#011loss=1.419409\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519707.2467265, \"EndTime\": 1663519712.2336152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4986.367225646973, \"count\": 1, \"min\": 4986.367225646973, \"max\": 4986.367225646973}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.3518312002783 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=143, train loss <loss>=1.624896770173853\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:32 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:33 INFO 140718936323904] Epoch[144] Batch[0] avg_epoch_loss=1.332707\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=1.332707405090332\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:35 INFO 140718936323904] Epoch[144] Batch[5] avg_epoch_loss=1.566115\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=1.5661151210467021\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:35 INFO 140718936323904] Epoch[144] Batch [5]#011Speed: 211.78 samples/sec#011loss=1.566115\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] Epoch[144] Batch[10] avg_epoch_loss=1.292283\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=0.9636855125427246\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] Epoch[144] Batch [10]#011Speed: 204.20 samples/sec#011loss=0.963686\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519712.2337103, \"EndTime\": 1663519716.953679, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4719.498157501221, \"count\": 1, \"min\": 4719.498157501221, \"max\": 4719.498157501221}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.51104332923728 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=144, train loss <loss>=1.2922834808176213\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:36 INFO 140718936323904] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:37 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/state_d20c2650-0e0a-4c9f-85d1-65a2d7ca6a95-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519716.9537644, \"EndTime\": 1663519717.0411487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 86.94601058959961, \"count\": 1, \"min\": 86.94601058959961, \"max\": 86.94601058959961}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:38 INFO 140718936323904] Epoch[145] Batch[0] avg_epoch_loss=1.487206\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=1.4872063398361206\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:40 INFO 140718936323904] Epoch[145] Batch[5] avg_epoch_loss=1.522722\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=1.52272230386734\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:40 INFO 140718936323904] Epoch[145] Batch [5]#011Speed: 218.79 samples/sec#011loss=1.522722\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] Epoch[145] Batch[10] avg_epoch_loss=1.775071\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=2.077890157699585\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] Epoch[145] Batch [10]#011Speed: 189.66 samples/sec#011loss=2.077890\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519717.041232, \"EndTime\": 1663519721.8240802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4782.774209976196, \"count\": 1, \"min\": 4782.774209976196, \"max\": 4782.774209976196}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.15503350709858 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=145, train loss <loss>=1.7750713283365422\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:41 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:43 INFO 140718936323904] Epoch[146] Batch[0] avg_epoch_loss=1.615856\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=1.615856409072876\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:44 INFO 140718936323904] Epoch[146] Batch[5] avg_epoch_loss=1.563376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=1.5633761485417683\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:44 INFO 140718936323904] Epoch[146] Batch [5]#011Speed: 205.28 samples/sec#011loss=1.563376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] Epoch[146] Batch[10] avg_epoch_loss=1.703802\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=1.8723137617111205\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] Epoch[146] Batch [10]#011Speed: 191.07 samples/sec#011loss=1.872314\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519721.824171, \"EndTime\": 1663519726.653537, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4828.782796859741, \"count\": 1, \"min\": 4828.782796859741, \"max\": 4828.782796859741}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.4398950602158 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=146, train loss <loss>=1.7038023363460193\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:46 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:48 INFO 140718936323904] Epoch[147] Batch[0] avg_epoch_loss=1.545295\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=1.5452947616577148\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:49 INFO 140718936323904] Epoch[147] Batch[5] avg_epoch_loss=1.684678\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=1.6846783359845479\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:49 INFO 140718936323904] Epoch[147] Batch [5]#011Speed: 204.22 samples/sec#011loss=1.684678\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:51 INFO 140718936323904] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519726.6536229, \"EndTime\": 1663519731.1442752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4490.195989608765, \"count\": 1, \"min\": 4490.195989608765, \"max\": 4490.195989608765}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.29223477795196 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=147, train loss <loss>=1.689120841026306\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:51 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:52 INFO 140718936323904] Epoch[148] Batch[0] avg_epoch_loss=1.748458\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=1.7484575510025024\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:54 INFO 140718936323904] Epoch[148] Batch[5] avg_epoch_loss=1.898008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=1.8980076313018799\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:54 INFO 140718936323904] Epoch[148] Batch [5]#011Speed: 222.01 samples/sec#011loss=1.898008\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] Epoch[148] Batch[10] avg_epoch_loss=1.861051\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=1.8167019844055177\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] Epoch[148] Batch [10]#011Speed: 186.16 samples/sec#011loss=1.816702\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519731.1443894, \"EndTime\": 1663519735.8509715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.132650375366, \"count\": 1, \"min\": 4706.132650375366, \"max\": 4706.132650375366}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=148.31335440889416 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=148, train loss <loss>=1.8610505190762607\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:55 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:57 INFO 140718936323904] Epoch[149] Batch[0] avg_epoch_loss=2.044284\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=2.0442843437194824\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:59 INFO 140718936323904] Epoch[149] Batch[5] avg_epoch_loss=1.786876\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=1.786876122156779\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:48:59 INFO 140718936323904] Epoch[149] Batch [5]#011Speed: 202.36 samples/sec#011loss=1.786876\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:00 INFO 140718936323904] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519735.8510556, \"EndTime\": 1663519740.5041764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4652.701377868652, \"count\": 1, \"min\": 4652.701377868652, \"max\": 4652.701377868652}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.54194227223797 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=149, train loss <loss>=1.8411182641983033\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:02 INFO 140718936323904] Epoch[150] Batch[0] avg_epoch_loss=2.299203\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=2.299203395843506\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:03 INFO 140718936323904] Epoch[150] Batch[5] avg_epoch_loss=1.973544\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=1.9735436638196309\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:03 INFO 140718936323904] Epoch[150] Batch [5]#011Speed: 201.52 samples/sec#011loss=1.973544\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:05 INFO 140718936323904] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519740.5042636, \"EndTime\": 1663519745.2114727, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.650018692017, \"count\": 1, \"min\": 4706.650018692017, \"max\": 4706.650018692017}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.54858222281027 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=150, train loss <loss>=1.8565961360931396\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:05 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:06 INFO 140718936323904] Epoch[151] Batch[0] avg_epoch_loss=1.693580\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:06 INFO 140718936323904] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=1.6935802698135376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:08 INFO 140718936323904] Epoch[151] Batch[5] avg_epoch_loss=1.680398\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=1.6803975701332092\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:08 INFO 140718936323904] Epoch[151] Batch [5]#011Speed: 210.29 samples/sec#011loss=1.680398\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] Epoch[151] Batch[10] avg_epoch_loss=1.854068\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=2.062472343444824\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] Epoch[151] Batch [10]#011Speed: 184.91 samples/sec#011loss=2.062472\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519745.2115703, \"EndTime\": 1663519750.0377288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4825.369596481323, \"count\": 1, \"min\": 4825.369596481323, \"max\": 4825.369596481323}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=139.46812902091008 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=151, train loss <loss>=1.8540679216384888\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:10 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:11 INFO 140718936323904] Epoch[152] Batch[0] avg_epoch_loss=1.867474\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=1.8674741983413696\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:13 INFO 140718936323904] Epoch[152] Batch[5] avg_epoch_loss=1.767245\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=1.7672449151674907\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:13 INFO 140718936323904] Epoch[152] Batch [5]#011Speed: 202.45 samples/sec#011loss=1.767245\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] Epoch[152] Batch[10] avg_epoch_loss=1.485988\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=1.1484805285930633\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] Epoch[152] Batch [10]#011Speed: 185.02 samples/sec#011loss=1.148481\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519750.037803, \"EndTime\": 1663519754.8919704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4853.729963302612, \"count\": 1, \"min\": 4853.729963302612, \"max\": 4853.729963302612}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.70776452588618 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] #quality_metric: host=algo-1, epoch=152, train loss <loss>=1.4859883758154782\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:14 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:16 INFO 140718936323904] Epoch[153] Batch[0] avg_epoch_loss=1.431729\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=1.4317293167114258\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:17 INFO 140718936323904] Epoch[153] Batch[5] avg_epoch_loss=1.633156\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=1.6331563591957092\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:17 INFO 140718936323904] Epoch[153] Batch [5]#011Speed: 223.31 samples/sec#011loss=1.633156\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] Epoch[153] Batch[10] avg_epoch_loss=1.938272\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=2.3044111490249635\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] Epoch[153] Batch [10]#011Speed: 195.73 samples/sec#011loss=2.304411\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519754.8920646, \"EndTime\": 1663519759.5657945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4673.182487487793, \"count\": 1, \"min\": 4673.182487487793, \"max\": 4673.182487487793}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.232273980581 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=153, train loss <loss>=1.9382721727544612\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:19 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:21 INFO 140718936323904] Epoch[154] Batch[0] avg_epoch_loss=1.162551\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=1.1625514030456543\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:22 INFO 140718936323904] Epoch[154] Batch[5] avg_epoch_loss=1.695371\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=1.6953713496526082\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:22 INFO 140718936323904] Epoch[154] Batch [5]#011Speed: 214.21 samples/sec#011loss=1.695371\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] Epoch[154] Batch[10] avg_epoch_loss=1.598126\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=1.4814319935627283\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] Epoch[154] Batch [10]#011Speed: 178.85 samples/sec#011loss=1.481432\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519759.5658672, \"EndTime\": 1663519764.4677758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4901.466608047485, \"count\": 1, \"min\": 4901.466608047485, \"max\": 4901.466608047485}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.97815909639118 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=154, train loss <loss>=1.5981261877935717\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:24 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:26 INFO 140718936323904] Epoch[155] Batch[0] avg_epoch_loss=1.564027\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=1.5640265941619873\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:27 INFO 140718936323904] Epoch[155] Batch[5] avg_epoch_loss=1.680419\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=1.6804190476735432\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:27 INFO 140718936323904] Epoch[155] Batch [5]#011Speed: 219.29 samples/sec#011loss=1.680419\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:28 INFO 140718936323904] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519764.4678507, \"EndTime\": 1663519768.7640343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4295.766830444336, \"count\": 1, \"min\": 4295.766830444336, \"max\": 4295.766830444336}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.92702887019777 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=155, train loss <loss>=1.6062715888023376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:28 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:30 INFO 140718936323904] Epoch[156] Batch[0] avg_epoch_loss=1.735446\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=1.7354463338851929\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:31 INFO 140718936323904] Epoch[156] Batch[5] avg_epoch_loss=1.604410\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=1.6044102311134338\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:31 INFO 140718936323904] Epoch[156] Batch [5]#011Speed: 204.94 samples/sec#011loss=1.604410\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:33 INFO 140718936323904] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519768.7641273, \"EndTime\": 1663519773.1376126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4372.902631759644, \"count\": 1, \"min\": 4372.902631759644, \"max\": 4372.902631759644}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=145.66584702633904 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=156, train loss <loss>=1.7282560825347901\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:33 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:34 INFO 140718936323904] Epoch[157] Batch[0] avg_epoch_loss=2.328748\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=2.328747510910034\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:36 INFO 140718936323904] Epoch[157] Batch[5] avg_epoch_loss=1.908986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=1.908985674381256\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:36 INFO 140718936323904] Epoch[157] Batch [5]#011Speed: 202.52 samples/sec#011loss=1.908986\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:37 INFO 140718936323904] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519773.1376965, \"EndTime\": 1663519777.7304513, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4592.229604721069, \"count\": 1, \"min\": 4592.229604721069, \"max\": 4592.229604721069}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:37 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.4908555075814 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:37 INFO 140718936323904] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=157, train loss <loss>=1.7784209966659545\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:37 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:39 INFO 140718936323904] Epoch[158] Batch[0] avg_epoch_loss=1.595431\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=1.595430612564087\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:40 INFO 140718936323904] Epoch[158] Batch[5] avg_epoch_loss=1.668920\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=1.668919861316681\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:40 INFO 140718936323904] Epoch[158] Batch [5]#011Speed: 205.89 samples/sec#011loss=1.668920\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] Epoch[158] Batch[10] avg_epoch_loss=1.758924\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=1.8669293642044067\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] Epoch[158] Batch [10]#011Speed: 182.07 samples/sec#011loss=1.866929\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519777.730541, \"EndTime\": 1663519782.5927327, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4861.745834350586, \"count\": 1, \"min\": 4861.745834350586, \"max\": 4861.745834350586}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.80663060586232 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=158, train loss <loss>=1.7589241808111018\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:42 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:44 INFO 140718936323904] Epoch[159] Batch[0] avg_epoch_loss=1.620050\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=1.620050072669983\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:45 INFO 140718936323904] Epoch[159] Batch[5] avg_epoch_loss=1.794298\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:45 INFO 140718936323904] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=1.7942982912063599\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:45 INFO 140718936323904] Epoch[159] Batch [5]#011Speed: 207.23 samples/sec#011loss=1.794298\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:47 INFO 140718936323904] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519782.5928276, \"EndTime\": 1663519787.0759377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4482.5708866119385, \"count\": 1, \"min\": 4482.5708866119385, \"max\": 4482.5708866119385}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:47 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=134.51660381657538 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:47 INFO 140718936323904] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=159, train loss <loss>=1.5567811585962772\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:47 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:48 INFO 140718936323904] Epoch[160] Batch[0] avg_epoch_loss=1.758721\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:48 INFO 140718936323904] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=1.7587214708328247\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:50 INFO 140718936323904] Epoch[160] Batch[5] avg_epoch_loss=1.786795\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=1.78679492076238\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:50 INFO 140718936323904] Epoch[160] Batch [5]#011Speed: 221.64 samples/sec#011loss=1.786795\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:51 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519787.0760381, \"EndTime\": 1663519791.4748278, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4398.132801055908, \"count\": 1, \"min\": 4398.132801055908, \"max\": 4398.132801055908}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:51 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=144.14830772736752 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:51 INFO 140718936323904] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:51 INFO 140718936323904] #quality_metric: host=algo-1, epoch=160, train loss <loss>=1.6072170972824096\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:51 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:53 INFO 140718936323904] Epoch[161] Batch[0] avg_epoch_loss=1.812268\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=1.8122683763504028\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:54 INFO 140718936323904] Epoch[161] Batch[5] avg_epoch_loss=1.620522\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:54 INFO 140718936323904] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=1.6205216646194458\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:54 INFO 140718936323904] Epoch[161] Batch [5]#011Speed: 203.31 samples/sec#011loss=1.620522\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] Epoch[161] Batch[10] avg_epoch_loss=1.490715\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=1.3349478781223296\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] Epoch[161] Batch [10]#011Speed: 178.07 samples/sec#011loss=1.334948\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519791.4749064, \"EndTime\": 1663519796.4847696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5009.438276290894, \"count\": 1, \"min\": 5009.438276290894, \"max\": 5009.438276290894}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.54827612300795 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] #quality_metric: host=algo-1, epoch=161, train loss <loss>=1.4907153980298475\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:56 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:58 INFO 140718936323904] Epoch[162] Batch[0] avg_epoch_loss=1.786473\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=1.7864727973937988\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:59 INFO 140718936323904] Epoch[162] Batch[5] avg_epoch_loss=1.520508\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:59 INFO 140718936323904] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=1.520508050918579\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:49:59 INFO 140718936323904] Epoch[162] Batch [5]#011Speed: 202.01 samples/sec#011loss=1.520508\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:00 INFO 140718936323904] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519796.4848616, \"EndTime\": 1663519800.9928722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4507.530212402344, \"count\": 1, \"min\": 4507.530212402344, \"max\": 4507.530212402344}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.758675469213 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=162, train loss <loss>=1.548373007774353\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:02 INFO 140718936323904] Epoch[163] Batch[0] avg_epoch_loss=1.743729\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=1.743728518486023\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:04 INFO 140718936323904] Epoch[163] Batch[5] avg_epoch_loss=1.693752\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:04 INFO 140718936323904] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=1.6937517523765564\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:04 INFO 140718936323904] Epoch[163] Batch [5]#011Speed: 201.55 samples/sec#011loss=1.693752\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:05 INFO 140718936323904] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519800.9929667, \"EndTime\": 1663519805.5962908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4602.854013442993, \"count\": 1, \"min\": 4602.854013442993, \"max\": 4602.854013442993}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.08660106682953 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=163, train loss <loss>=1.8178443312644958\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:05 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:07 INFO 140718936323904] Epoch[164] Batch[0] avg_epoch_loss=1.758246\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=1.7582461833953857\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:08 INFO 140718936323904] Epoch[164] Batch[5] avg_epoch_loss=1.650709\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=1.650708556175232\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:08 INFO 140718936323904] Epoch[164] Batch [5]#011Speed: 207.28 samples/sec#011loss=1.650709\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:10 INFO 140718936323904] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519805.59643, \"EndTime\": 1663519810.1251767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4528.1336307525635, \"count\": 1, \"min\": 4528.1336307525635, \"max\": 4528.1336307525635}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.02208846231082 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=164, train loss <loss>=1.6673932075500488\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:10 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:11 INFO 140718936323904] Epoch[165] Batch[0] avg_epoch_loss=1.896132\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:11 INFO 140718936323904] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=1.8961316347122192\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:13 INFO 140718936323904] Epoch[165] Batch[5] avg_epoch_loss=1.692348\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=1.6923481027285259\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:13 INFO 140718936323904] Epoch[165] Batch [5]#011Speed: 206.86 samples/sec#011loss=1.692348\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] Epoch[165] Batch[10] avg_epoch_loss=2.005143\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=2.380496788024902\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] Epoch[165] Batch [10]#011Speed: 182.21 samples/sec#011loss=2.380497\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519810.1252675, \"EndTime\": 1663519815.1177928, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4992.037534713745, \"count\": 1, \"min\": 4992.037534713745, \"max\": 4992.037534713745}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.00206398008737 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=165, train loss <loss>=2.0051429596814243\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:15 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:16 INFO 140718936323904] Epoch[166] Batch[0] avg_epoch_loss=1.508545\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:16 INFO 140718936323904] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=1.5085445642471313\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:18 INFO 140718936323904] Epoch[166] Batch[5] avg_epoch_loss=1.596337\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=1.596337080001831\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:18 INFO 140718936323904] Epoch[166] Batch [5]#011Speed: 208.81 samples/sec#011loss=1.596337\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:19 INFO 140718936323904] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519815.117889, \"EndTime\": 1663519819.428401, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4309.798240661621, \"count\": 1, \"min\": 4309.798240661621, \"max\": 4309.798240661621}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:19 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=145.47795765729157 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:19 INFO 140718936323904] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:19 INFO 140718936323904] #quality_metric: host=algo-1, epoch=166, train loss <loss>=1.5982484340667724\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:19 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:21 INFO 140718936323904] Epoch[167] Batch[0] avg_epoch_loss=2.033062\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=2.0330617427825928\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:22 INFO 140718936323904] Epoch[167] Batch[5] avg_epoch_loss=1.614048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:22 INFO 140718936323904] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=1.6140478452046711\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:22 INFO 140718936323904] Epoch[167] Batch [5]#011Speed: 230.35 samples/sec#011loss=1.614048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:23 INFO 140718936323904] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519819.428499, \"EndTime\": 1663519823.6760707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4247.049570083618, \"count\": 1, \"min\": 4247.049570083618, \"max\": 4247.049570083618}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:23 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=150.68799349904242 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:23 INFO 140718936323904] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=167, train loss <loss>=1.6968555569648742\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:23 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:25 INFO 140718936323904] Epoch[168] Batch[0] avg_epoch_loss=1.597358\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:25 INFO 140718936323904] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=1.5973575115203857\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:26 INFO 140718936323904] Epoch[168] Batch[5] avg_epoch_loss=1.653142\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=1.6531415780385335\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:26 INFO 140718936323904] Epoch[168] Batch [5]#011Speed: 203.39 samples/sec#011loss=1.653142\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] Epoch[168] Batch[10] avg_epoch_loss=1.515153\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=1.3495659459382296\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] Epoch[168] Batch [10]#011Speed: 188.43 samples/sec#011loss=1.349566\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519823.6761668, \"EndTime\": 1663519828.4817107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4804.9633502960205, \"count\": 1, \"min\": 4804.9633502960205, \"max\": 4804.9633502960205}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.18726541807706 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=168, train loss <loss>=1.5151526543565772\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:28 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:30 INFO 140718936323904] Epoch[169] Batch[0] avg_epoch_loss=1.772667\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=1.7726668119430542\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:31 INFO 140718936323904] Epoch[169] Batch[5] avg_epoch_loss=1.730532\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:31 INFO 140718936323904] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=1.7305318713188171\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:31 INFO 140718936323904] Epoch[169] Batch [5]#011Speed: 207.98 samples/sec#011loss=1.730532\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:32 INFO 140718936323904] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519828.4817863, \"EndTime\": 1663519832.8859015, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4403.679370880127, \"count\": 1, \"min\": 4403.679370880127, \"max\": 4403.679370880127}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:32 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.058213297112 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:32 INFO 140718936323904] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=169, train loss <loss>=1.855088233947754\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:32 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:34 INFO 140718936323904] Epoch[170] Batch[0] avg_epoch_loss=1.793913\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=1.7939132452011108\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:35 INFO 140718936323904] Epoch[170] Batch[5] avg_epoch_loss=1.751909\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:35 INFO 140718936323904] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=1.7519085605939229\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:35 INFO 140718936323904] Epoch[170] Batch [5]#011Speed: 214.90 samples/sec#011loss=1.751909\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:37 INFO 140718936323904] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519832.8859866, \"EndTime\": 1663519837.2656121, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4379.050493240356, \"count\": 1, \"min\": 4379.050493240356, \"max\": 4379.050493240356}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:37 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.94893573142068 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:37 INFO 140718936323904] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:37 INFO 140718936323904] #quality_metric: host=algo-1, epoch=170, train loss <loss>=1.6104018568992615\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:37 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:38 INFO 140718936323904] Epoch[171] Batch[0] avg_epoch_loss=2.177652\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=2.17765212059021\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:40 INFO 140718936323904] Epoch[171] Batch[5] avg_epoch_loss=1.866423\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:40 INFO 140718936323904] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=1.8664233287175496\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:40 INFO 140718936323904] Epoch[171] Batch [5]#011Speed: 217.66 samples/sec#011loss=1.866423\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:41 INFO 140718936323904] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519837.2657042, \"EndTime\": 1663519841.6067262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4340.579509735107, \"count\": 1, \"min\": 4340.579509735107, \"max\": 4340.579509735107}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:41 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=143.52338036666234 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:41 INFO 140718936323904] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=171, train loss <loss>=1.7638839840888978\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:41 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:43 INFO 140718936323904] Epoch[172] Batch[0] avg_epoch_loss=1.402361\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:43 INFO 140718936323904] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=1.4023610353469849\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:44 INFO 140718936323904] Epoch[172] Batch[5] avg_epoch_loss=1.582380\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:44 INFO 140718936323904] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=1.5823803742726643\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:44 INFO 140718936323904] Epoch[172] Batch [5]#011Speed: 207.79 samples/sec#011loss=1.582380\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:46 INFO 140718936323904] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519841.6068668, \"EndTime\": 1663519846.0810678, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4473.700284957886, \"count\": 1, \"min\": 4473.700284957886, \"max\": 4473.700284957886}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:46 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=141.7129538456242 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:46 INFO 140718936323904] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:46 INFO 140718936323904] #quality_metric: host=algo-1, epoch=172, train loss <loss>=1.5340175390243531\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:46 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:47 INFO 140718936323904] Epoch[173] Batch[0] avg_epoch_loss=1.797830\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:47 INFO 140718936323904] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=1.7978301048278809\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:49 INFO 140718936323904] Epoch[173] Batch[5] avg_epoch_loss=1.729981\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:49 INFO 140718936323904] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=1.7299810647964478\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:49 INFO 140718936323904] Epoch[173] Batch [5]#011Speed: 225.05 samples/sec#011loss=1.729981\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] Epoch[173] Batch[10] avg_epoch_loss=1.561838\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=1.3600662976503373\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] Epoch[173] Batch [10]#011Speed: 184.42 samples/sec#011loss=1.360066\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519846.081159, \"EndTime\": 1663519850.8462903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4764.652252197266, \"count\": 1, \"min\": 4764.652252197266, \"max\": 4764.652252197266}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=138.51657045572293 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] #quality_metric: host=algo-1, epoch=173, train loss <loss>=1.561837988820943\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:50 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:52 INFO 140718936323904] Epoch[174] Batch[0] avg_epoch_loss=1.634587\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:52 INFO 140718936323904] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=1.634587287902832\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:53 INFO 140718936323904] Epoch[174] Batch[5] avg_epoch_loss=1.887199\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:53 INFO 140718936323904] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=1.887198547522227\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:53 INFO 140718936323904] Epoch[174] Batch [5]#011Speed: 220.64 samples/sec#011loss=1.887199\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] Epoch[174] Batch[10] avg_epoch_loss=1.585903\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=1.2243483185768127\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] Epoch[174] Batch [10]#011Speed: 182.69 samples/sec#011loss=1.224348\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519850.846375, \"EndTime\": 1663519855.670214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4823.356628417969, \"count\": 1, \"min\": 4823.356628417969, \"max\": 4823.356628417969}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=136.62308374385066 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] #quality_metric: host=algo-1, epoch=174, train loss <loss>=1.585902988910675\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:55 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:57 INFO 140718936323904] Epoch[175] Batch[0] avg_epoch_loss=1.595275\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:57 INFO 140718936323904] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=1.5952750444412231\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:58 INFO 140718936323904] Epoch[175] Batch[5] avg_epoch_loss=1.546150\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:58 INFO 140718936323904] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=1.5461499094963074\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:50:58 INFO 140718936323904] Epoch[175] Batch [5]#011Speed: 204.58 samples/sec#011loss=1.546150\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] Epoch[175] Batch[10] avg_epoch_loss=1.811264\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=2.129401707649231\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] Epoch[175] Batch [10]#011Speed: 182.29 samples/sec#011loss=2.129402\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519855.6703074, \"EndTime\": 1663519860.6349444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4964.132308959961, \"count\": 1, \"min\": 4964.132308959961, \"max\": 4964.132308959961}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=133.3529498326166 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] #quality_metric: host=algo-1, epoch=175, train loss <loss>=1.8112643632021816\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:00 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:02 INFO 140718936323904] Epoch[176] Batch[0] avg_epoch_loss=1.480859\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:02 INFO 140718936323904] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=1.4808591604232788\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:03 INFO 140718936323904] Epoch[176] Batch[5] avg_epoch_loss=1.714403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:03 INFO 140718936323904] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=1.7144034107526143\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:03 INFO 140718936323904] Epoch[176] Batch [5]#011Speed: 210.29 samples/sec#011loss=1.714403\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] Epoch[176] Batch[10] avg_epoch_loss=1.584734\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=1.4291298329830169\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] Epoch[176] Batch [10]#011Speed: 176.92 samples/sec#011loss=1.429130\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519860.635038, \"EndTime\": 1663519865.6286972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4993.048906326294, \"count\": 1, \"min\": 4993.048906326294, \"max\": 4993.048906326294}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=129.77721319457308 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] #quality_metric: host=algo-1, epoch=176, train loss <loss>=1.5847336026755245\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:05 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:07 INFO 140718936323904] Epoch[177] Batch[0] avg_epoch_loss=1.321572\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:07 INFO 140718936323904] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=1.3215718269348145\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:08 INFO 140718936323904] Epoch[177] Batch[5] avg_epoch_loss=1.491787\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:08 INFO 140718936323904] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=1.4917871952056885\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:08 INFO 140718936323904] Epoch[177] Batch [5]#011Speed: 206.58 samples/sec#011loss=1.491787\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] Epoch[177] Batch[10] avg_epoch_loss=1.732443\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=2.021230387687683\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] Epoch[177] Batch [10]#011Speed: 180.78 samples/sec#011loss=2.021230\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519865.6287847, \"EndTime\": 1663519870.5877929, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4958.584547042847, \"count\": 1, \"min\": 4958.584547042847, \"max\": 4958.584547042847}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=130.880374444062 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] #quality_metric: host=algo-1, epoch=177, train loss <loss>=1.7324431917884133\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:10 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:12 INFO 140718936323904] Epoch[178] Batch[0] avg_epoch_loss=1.520674\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:12 INFO 140718936323904] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=1.5206735134124756\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:13 INFO 140718936323904] Epoch[178] Batch[5] avg_epoch_loss=1.460790\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:13 INFO 140718936323904] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=1.4607902765274048\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:13 INFO 140718936323904] Epoch[178] Batch [5]#011Speed: 217.70 samples/sec#011loss=1.460790\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] Epoch[178] Batch[10] avg_epoch_loss=1.562306\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=1.684124267101288\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] Epoch[178] Batch [10]#011Speed: 183.28 samples/sec#011loss=1.684124\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519870.5878994, \"EndTime\": 1663519875.3810499, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4792.668104171753, \"count\": 1, \"min\": 4792.668104171753, \"max\": 4792.668104171753}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=137.2890774515075 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] #quality_metric: host=algo-1, epoch=178, train loss <loss>=1.5623057267882607\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:15 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:17 INFO 140718936323904] Epoch[179] Batch[0] avg_epoch_loss=1.141231\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:17 INFO 140718936323904] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=1.1412309408187866\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:18 INFO 140718936323904] Epoch[179] Batch[5] avg_epoch_loss=1.516540\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:18 INFO 140718936323904] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=1.5165395935376484\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:18 INFO 140718936323904] Epoch[179] Batch [5]#011Speed: 208.79 samples/sec#011loss=1.516540\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] Epoch[179] Batch[10] avg_epoch_loss=1.388791\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=1.2354921281337738\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] Epoch[179] Batch [10]#011Speed: 184.76 samples/sec#011loss=1.235492\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519875.3811471, \"EndTime\": 1663519880.27509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4893.32389831543, \"count\": 1, \"min\": 4893.32389831543, \"max\": 4893.32389831543}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.4869418793324 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] #quality_metric: host=algo-1, epoch=179, train loss <loss>=1.3887907456267963\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:20 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:21 INFO 140718936323904] Epoch[180] Batch[0] avg_epoch_loss=1.079315\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:21 INFO 140718936323904] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=1.0793153047561646\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:23 INFO 140718936323904] Epoch[180] Batch[5] avg_epoch_loss=1.570748\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:23 INFO 140718936323904] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=1.5707483887672424\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:23 INFO 140718936323904] Epoch[180] Batch [5]#011Speed: 221.37 samples/sec#011loss=1.570748\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:24 INFO 140718936323904] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519880.275184, \"EndTime\": 1663519884.540284, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4264.580249786377, \"count\": 1, \"min\": 4264.580249786377, \"max\": 4264.580249786377}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:24 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=142.79797979910919 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:24 INFO 140718936323904] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:24 INFO 140718936323904] #quality_metric: host=algo-1, epoch=180, train loss <loss>=1.6991639256477356\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:24 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:26 INFO 140718936323904] Epoch[181] Batch[0] avg_epoch_loss=1.645963\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:26 INFO 140718936323904] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=1.6459628343582153\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:27 INFO 140718936323904] Epoch[181] Batch[5] avg_epoch_loss=1.448334\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:27 INFO 140718936323904] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=1.4483339687188466\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:27 INFO 140718936323904] Epoch[181] Batch [5]#011Speed: 206.10 samples/sec#011loss=1.448334\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:28 INFO 140718936323904] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519884.5404255, \"EndTime\": 1663519888.926212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4385.300636291504, \"count\": 1, \"min\": 4385.300636291504, \"max\": 4385.300636291504}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:28 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=145.9382078991398 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:28 INFO 140718936323904] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:28 INFO 140718936323904] #quality_metric: host=algo-1, epoch=181, train loss <loss>=1.4858711302280425\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:28 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:30 INFO 140718936323904] Epoch[182] Batch[0] avg_epoch_loss=1.216494\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:30 INFO 140718936323904] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=1.2164939641952515\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:32 INFO 140718936323904] Epoch[182] Batch[5] avg_epoch_loss=1.612058\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:32 INFO 140718936323904] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=1.6120582620302837\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:32 INFO 140718936323904] Epoch[182] Batch [5]#011Speed: 212.65 samples/sec#011loss=1.612058\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:33 INFO 140718936323904] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519888.9262912, \"EndTime\": 1663519893.2957816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4369.034051895142, \"count\": 1, \"min\": 4369.034051895142, \"max\": 4369.034051895142}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:33 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=135.03748886869732 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:33 INFO 140718936323904] #progress_metric: host=algo-1, completed 45.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:33 INFO 140718936323904] #quality_metric: host=algo-1, epoch=182, train loss <loss>=1.8610395312309265\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:33 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:34 INFO 140718936323904] Epoch[183] Batch[0] avg_epoch_loss=1.514016\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:34 INFO 140718936323904] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=1.5140163898468018\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:36 INFO 140718936323904] Epoch[183] Batch[5] avg_epoch_loss=1.701483\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:36 INFO 140718936323904] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=1.7014829516410828\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:36 INFO 140718936323904] Epoch[183] Batch [5]#011Speed: 216.79 samples/sec#011loss=1.701483\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] Epoch[183] Batch[10] avg_epoch_loss=1.584011\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=1.4430440664291382\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] Epoch[183] Batch [10]#011Speed: 182.10 samples/sec#011loss=1.443044\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519893.295865, \"EndTime\": 1663519898.1556761, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4859.274387359619, \"count\": 1, \"min\": 4859.274387359619, \"max\": 4859.274387359619}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=132.3211084340447 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] #quality_metric: host=algo-1, epoch=183, train loss <loss>=1.584010731090199\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:38 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:39 INFO 140718936323904] Epoch[184] Batch[0] avg_epoch_loss=1.246680\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:39 INFO 140718936323904] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=1.2466797828674316\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:41 INFO 140718936323904] Epoch[184] Batch[5] avg_epoch_loss=1.436210\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:41 INFO 140718936323904] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=1.4362095991770427\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:41 INFO 140718936323904] Epoch[184] Batch [5]#011Speed: 210.84 samples/sec#011loss=1.436210\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519898.1557527, \"EndTime\": 1663519902.6396222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4483.422756195068, \"count\": 1, \"min\": 4483.422756195068, \"max\": 4483.422756195068}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] #throughput_metric: host=algo-1, train throughput=131.59224940041025 records/second\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 46.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] #quality_metric: host=algo-1, epoch=184, train loss <loss>=1.678283166885376\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] Loading parameters from best epoch (144)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519902.6397092, \"EndTime\": 1663519902.6784208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 38.23566436767578, \"count\": 1, \"min\": 38.23566436767578, \"max\": 38.23566436767578}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] stopping training now\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] Final loss: 1.2922834808176213 (occurred at epoch 144)\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] #quality_metric: host=algo-1, train final_loss <loss>=1.2922834808176213\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 WARNING 140718936323904] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:42 INFO 140718936323904] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519902.678513, \"EndTime\": 1663519905.2934234, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 2614.1867637634277, \"count\": 1, \"min\": 2614.1867637634277, \"max\": 2614.1867637634277}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519905.293509, \"EndTime\": 1663519905.6299512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2950.76584815979, \"count\": 1, \"min\": 2950.76584815979, \"max\": 2950.76584815979}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519905.630008, \"EndTime\": 1663519905.6739995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 43.94888877868652, \"count\": 1, \"min\": 43.94888877868652, \"max\": 43.94888877868652}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] #memory_usage::<batchbuffer> = 291.4404296875 mb\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:45 INFO 140718936323904] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519905.6740792, \"EndTime\": 1663519905.6805823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.049591064453125, \"count\": 1, \"min\": 0.049591064453125, \"max\": 0.049591064453125}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519905.6806445, \"EndTime\": 1663519910.8330884, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 5152.5561809539795, \"count\": 1, \"min\": 5152.5561809539795, \"max\": 5152.5561809539795}}}\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, RMSE): 14487.25992621786\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, mean_absolute_QuantileLoss): 1737040.3728059\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, mean_wQuantileLoss): 0.8480365007545982\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.1]): 0.33466417345429117\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.2]): 0.5509799214047377\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.3]): 0.7180723039725486\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.4]): 0.8585237944035647\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.5]): 0.9790627550455677\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.6]): 1.0643676919689407\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.7]): 1.1111352367712393\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.8]): 1.0918111746342065\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #test_score (algo-1, wQuantileLoss[0.9]): 0.9237114551362889\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #quality_metric: host=algo-1, test RMSE <loss>=14487.25992621786\u001b[0m\n",
      "\u001b[34m[09/18/2022 16:51:50 INFO 140718936323904] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.8480365007545982\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663519910.833194, \"EndTime\": 1663519910.9486203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 11.522769927978516, \"count\": 1, \"min\": 11.522769927978516, \"max\": 11.522769927978516}, \"totaltime\": {\"sum\": 875914.5753383636, \"count\": 1, \"min\": 875914.5753383636, \"max\": 875914.5753383636}}}\u001b[0m\n",
      "\n",
      "2022-09-18 16:51:59 Uploading - Uploading generated training model\n",
      "2022-09-18 16:52:20 Completed - Training job completed\n",
      "ProfilerReport-1663518852: NoIssuesFound\n",
      "Training seconds: 995\n",
      "Billable seconds: 995\n",
      "CPU times: user 2.46 s, sys: 264 ms, total: 2.72 s\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2659a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe187dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "    \n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c52fc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8e077545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-30 02:00:00</th>\n",
       "      <td>-0.226100</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>0.175329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 04:00:00</th>\n",
       "      <td>-0.172893</td>\n",
       "      <td>-0.015433</td>\n",
       "      <td>0.109206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 06:00:00</th>\n",
       "      <td>-0.119177</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.089974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 08:00:00</th>\n",
       "      <td>-0.061439</td>\n",
       "      <td>0.020356</td>\n",
       "      <td>0.099211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 10:00:00</th>\n",
       "      <td>-0.050779</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.064644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0.1       0.5       0.9\n",
       "2018-12-30 02:00:00 -0.226100 -0.007297  0.175329\n",
       "2018-12-30 04:00:00 -0.172893 -0.015433  0.109206\n",
       "2018-12-30 06:00:00 -0.119177 -0.010195  0.089974\n",
       "2018-12-30 08:00:00 -0.061439  0.020356  0.099211\n",
       "2018-12-30 10:00:00 -0.050779  0.015012  0.064644"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[2], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c195188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-30 02:00:00</th>\n",
       "      <td>-0.226100</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>0.175329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 04:00:00</th>\n",
       "      <td>-0.172893</td>\n",
       "      <td>-0.015433</td>\n",
       "      <td>0.109206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 06:00:00</th>\n",
       "      <td>-0.119177</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.089974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 08:00:00</th>\n",
       "      <td>-0.061439</td>\n",
       "      <td>0.020356</td>\n",
       "      <td>0.099211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30 10:00:00</th>\n",
       "      <td>-0.050779</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.064644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05 16:00:00</th>\n",
       "      <td>-0.070191</td>\n",
       "      <td>-0.009156</td>\n",
       "      <td>0.073515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05 18:00:00</th>\n",
       "      <td>-0.068202</td>\n",
       "      <td>-0.006418</td>\n",
       "      <td>0.070176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05 20:00:00</th>\n",
       "      <td>-0.053795</td>\n",
       "      <td>-0.000758</td>\n",
       "      <td>0.066665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05 22:00:00</th>\n",
       "      <td>-0.220906</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>2.949383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06 00:00:00</th>\n",
       "      <td>-85.386063</td>\n",
       "      <td>2.078950</td>\n",
       "      <td>83.436096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.1       0.5        0.9\n",
       "2018-12-30 02:00:00  -0.226100 -0.007297   0.175329\n",
       "2018-12-30 04:00:00  -0.172893 -0.015433   0.109206\n",
       "2018-12-30 06:00:00  -0.119177 -0.010195   0.089974\n",
       "2018-12-30 08:00:00  -0.061439  0.020356   0.099211\n",
       "2018-12-30 10:00:00  -0.050779  0.015012   0.064644\n",
       "...                        ...       ...        ...\n",
       "2019-01-05 16:00:00  -0.070191 -0.009156   0.073515\n",
       "2019-01-05 18:00:00  -0.068202 -0.006418   0.070176\n",
       "2019-01-05 20:00:00  -0.053795 -0.000758   0.066665\n",
       "2019-01-05 22:00:00  -0.220906  0.105821   2.949383\n",
       "2019-01-06 00:00:00 -85.386063  2.078950  83.436096\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[2], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "600aee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Order Date\n",
       " 2015-01-02 00:00:00      67.625\n",
       " 2015-01-02 02:00:00       0.000\n",
       " 2015-01-02 04:00:00       0.000\n",
       " 2015-01-02 06:00:00       0.000\n",
       " 2015-01-02 08:00:00       0.000\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00       0.000\n",
       " 2018-12-29 18:00:00       0.000\n",
       " 2018-12-29 20:00:00       0.000\n",
       " 2018-12-29 22:00:00       0.000\n",
       " 2018-12-30 00:00:00    1382.125\n",
       " Freq: 2H, Name: Row ID, Length: 17497, dtype: float64,\n",
       " Order Date\n",
       " 2015-01-02 00:00:00     6787.75\n",
       " 2015-01-02 02:00:00        0.00\n",
       " 2015-01-02 04:00:00        0.00\n",
       " 2015-01-02 06:00:00        0.00\n",
       " 2015-01-02 08:00:00        0.00\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00        0.00\n",
       " 2018-12-29 18:00:00        0.00\n",
       " 2018-12-29 20:00:00        0.00\n",
       " 2018-12-29 22:00:00        0.00\n",
       " 2018-12-30 00:00:00    43354.00\n",
       " Freq: 2H, Name: Postal Code, Length: 17497, dtype: float64,\n",
       " Order Date\n",
       " 2015-01-02 00:00:00    58.61250\n",
       " 2015-01-02 02:00:00     0.00000\n",
       " 2015-01-02 04:00:00     0.00000\n",
       " 2015-01-02 06:00:00     0.00000\n",
       " 2015-01-02 08:00:00     0.00000\n",
       "                          ...   \n",
       " 2018-12-29 16:00:00     0.00000\n",
       " 2018-12-29 18:00:00     0.00000\n",
       " 2018-12-29 20:00:00     0.00000\n",
       " 2018-12-29 22:00:00     0.00000\n",
       " 2018-12-30 00:00:00    89.22375\n",
       " Freq: 2H, Name: Sales, Length: 17497, dtype: float64]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134fbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34c192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a7c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc55b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9e697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdee2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0b6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c26118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e20677",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sales.csv', index_col=0, parse_dates=True )\n",
    "num_timeseries = data.shape[1]\n",
    "data_sales = data.resample(\"D\").sum()\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_sales.iloc[:, i], trim=\"f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "394827b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>5.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>457.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>17.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>63.552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sales\n",
       "Order Date         \n",
       "2015-01-02  468.900\n",
       "2015-01-03    5.940\n",
       "2015-01-03  457.568\n",
       "2015-01-03   17.472\n",
       "2015-01-03   63.552"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2375f208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Order Date\n",
       " 2015-01-02     468.9000\n",
       " 2015-01-03    2203.1510\n",
       " 2015-01-04     119.8880\n",
       " 2015-01-05       0.0000\n",
       " 2015-01-06    5188.5200\n",
       "                 ...    \n",
       " 2018-12-26     814.5940\n",
       " 2018-12-27     177.6360\n",
       " 2018-12-28    1657.3508\n",
       " 2018-12-29    2915.5340\n",
       " 2018-12-30     713.7900\n",
       " Freq: D, Name: Sales, Length: 1459, dtype: float64]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb21141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c01fbb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\\naxx = axs.ravel()\\nfor i in range(0, 10):\\n    timeseries[i].loc[\"2015-01-02 \":\"2015-01-13\"].plot(ax=axx[i])\\n    axx[i].set_xlabel(\"date\")\\n    axx[i].set_ylabel(\"Sales\")\\n    axx[i].grid(which=\"minor\", axis=\"x\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 10):\n",
    "    timeseries[i].loc[\"2015-01-02 \":\"2015-01-13\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")\n",
    "    axx[i].set_ylabel(\"Sales\")\n",
    "    axx[i].grid(which=\"minor\", axis=\"x\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0113619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 day frequency for the time series\n",
    "freq = \"D\"\n",
    "\n",
    "# we predict for 150 days\n",
    "prediction_length = 150\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8949c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab81b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset =pd.Timestamp(\"2015-01-02\")\n",
    "end_training = pd.Timestamp(\"2018-03-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4844f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[\n",
    "            start_dataset : end_training - timedelta(days=1)\n",
    "        ].tolist(),  # We use -1, because pandas indexing includes the upper bound\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4645021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628c1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326afd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdc3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c06861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0342c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13de283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd2e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3445d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
