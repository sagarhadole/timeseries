{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9b6cfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "23329c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c8cb758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d525892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-salesdata-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "40dbefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7795d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8df39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a40ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f553b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272da3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd953d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2017-138688</td>\n",
       "      <td>12/06/2017</td>\n",
       "      <td>16/06/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036.0</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
       "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City       State  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "\n",
       "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
       "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
       "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  \n",
       "0                  Bush Somerset Collection Bookcase  261.9600  \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "89288872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Order Date', 'Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bff08133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Order Date'] = pd.to_datetime(df['Order Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc4925a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Order Date  9800 non-null   datetime64[ns]\n",
      " 1   Sales       9800 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 153.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "344204f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>2203.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>119.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>5188.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date     Sales\n",
       "0 2015-01-02   468.900\n",
       "1 2015-01-03  2203.151\n",
       "2 2015-01-04   119.888\n",
       "3 2015-01-05     0.000\n",
       "4 2015-01-06  5188.520"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.resample('D', label='right', closed = 'right', on='Order Date').sum()\n",
    "new_df=df.index.to_frame(index=False)\n",
    "df= df.merge(new_df,on='Order Date',how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97c9874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1459 entries, 0 to 1458\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Order Date  1459 non-null   datetime64[ns]\n",
      " 1   Sales       1459 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 34.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f9f08add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Order Date', ylabel='Sales'>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6AUlEQVR4nO2deZgdRdXwfyeTBLOwJCQgJEBYwhIQWUIEF1xQQVFBhZeAAgoKIioun37g+6Eo5hV8RRQRZInsS9hBIYGQAAkh22Tfk8k+yWQyyWSZZDKZ7Xx/dN+Zvvd239t36btMzu955pm+1VXVp6u769SpOlUlqophGIZh5JtuxRbAMAzD6JqYgjEMwzAiwRSMYRiGEQmmYAzDMIxIMAVjGIZhREL3YgtQaAYMGKBDhgwpthiGYRhlxaxZs7ao6sBM0uxzCmbIkCFUVlYWWwzDMIyyQkTWZprGusgMwzCMSDAFYxiGYUSCKRjDMAwjEkzBGIZhGJFgCsYwDMOIBFMwhmEYRiSYgjEMwzAiwRRMgZi1dhtLanYWWwzDMIyCsc9NtCwW37z/AwDW3HFhkSUxDMMoDGbBGIZhGJFgCsYwDMOIBFMwhmEYRiSYgjEMwzAiwRSMYRiGEQmmYAzDMIxIMAVjGIZhRIIpGMMwDCMSTMEYhmEYkWAKxjAMw4gEUzCGYRhGJJiCMQzDMCLBFIxhGIYRCaZgDMMwjEgwBWMYhmFEgikYwzAMIxJMwRiGYRiRYArGMAzDiITIFIyIHCEi74jIEhFZJCI3ueG3icgGEZnr/n3Zk+YWEakSkWUicr4n/EwRWeCeu0dExA3fT0TGuOHTRWRIVPdjGIZhZEaUFkwr8AtVPQk4G7hRRIa55+5W1dPcvzcA3HMjgZOBC4D7RKTCjX8/cB0w1P27wA2/FtimqscBdwN3Rng/hmEYRgZEpmBUtUZVZ7vHDcASYFCKJBcBz6rqXlVdDVQBI0TkMOAAVZ2qqgo8DlzsSfOYe/wCcF7MujEMwzCKS0HGYNyuq9OB6W7Qj0Rkvoj8S0T6uWGDgPWeZNVu2CD3ODE8Lo2qtgI7gIN9rn+diFSKSGVdXV1+bsowDMNISeQKRkT6Ai8CP1XVnTjdXccCpwE1wF2xqD7JNUV4qjTxAaoPqupwVR0+cODAzG7AMAzDyIpIFYyI9MBRLk+p6ksAqlqrqm2q2g48BIxwo1cDR3iSDwY2uuGDfcLj0ohId+BAoD6auzEMwzAyIUovMgFGA0tU9S+e8MM80b4OLHSPXwNGup5hR+MM5s9Q1RqgQUTOdvO8CnjVk+Zq9/gSYKI7TmMYhmEUme4R5v0J4EpggYjMdcN+DVwuIqfhdGWtAa4HUNVFIvIcsBjHA+1GVW1z090APAr0Asa6f+AosCdEpArHchkZ4f0YhmEYGRCZglHV9/EfI3kjRZpRwCif8ErgFJ/wJuDSHMQ0DMMwIsJm8huGYRiRYArGMAzDiARTMIZhGEYkmIIxDMMwIsEUjGEYhhEJpmAMwzCMSDAFYxiGYUSCKRjDMAwjEkzBGIZhGJFgCsYwDMOIBFMwhmEYRiSYgjEMwzAiwRSMYRiGEQmmYAzDMIxIMAVjGIZhRIIpGMMwDCMSTMEYhmEYkWAKxjAMw4gEUzCGYRhGJJiCMQzDMCLBFIxhGIYRCaZgDMMwjEgwBWMYhmFEgikYwzAMIxJMwRiGYRiRYArGMAzDiITIFIyIHCEi74jIEhFZJCI3ueH9RWS8iKxw//fzpLlFRKpEZJmInO8JP1NEFrjn7hERccP3E5Exbvh0ERkS1f0YhmEYmRGlBdMK/EJVTwLOBm4UkWHAzcAEVR0KTHB/454bCZwMXADcJyIVbl73A9cBQ92/C9zwa4FtqnoccDdwZ4T3YxiGYWRAZApGVWtUdbZ73AAsAQYBFwGPudEeAy52jy8CnlXVvaq6GqgCRojIYcABqjpVVRV4PCFNLK8XgPNi1o1hGIZRXAoyBuN2XZ0OTAcOVdUacJQQcIgbbRCw3pOs2g0b5B4nhselUdVWYAdwsM/1rxORShGprKury9NdGYZhGKmIXMGISF/gReCnqrozVVSfME0RnipNfIDqg6o6XFWHDxw4MJ3IhmEYRh6IVMGISA8c5fKUqr7kBte63V64/ze74dXAEZ7kg4GNbvhgn/C4NCLSHTgQqM//nRiGYRiZEqUXmQCjgSWq+hfPqdeAq93jq4FXPeEjXc+wo3EG82e43WgNInK2m+dVCWlieV0CTHTHaQzDMIwi0z3CvD8BXAksEJG5btivgTuA50TkWmAdcCmAqi4SkeeAxTgeaDeqapub7gbgUaAXMNb9A0eBPSEiVTiWy8gI78cwDMPIgMgUjKq+j/8YCcB5AWlGAaN8wiuBU3zCm3AVlGEYhlFa2Ex+wzAMIxJMwRiGYRiRYArGMAzDiARTMIZhGEYkmIIxjDJh6sqtvLNsc/qIhlEiROmmbBhGHrn8oWkArLnjwiJLYhjhMAvGMAzDiARTMIZhGEYkmIIxDMMwIsEUjGEYhhEJpmAMwzCMSDAFYxiGYUSCKRjDMAwjEkzBGIZhGJFgCsYwDMOIBFMwhmEYRiSYgjEMwzAiwRSMYRiGEQmmYAzDMIxIMAVjGIZhRIIpGMMwDCMSTMEYhmEYkWAKxjAMw4gEUzCGYRhGJJiCMQzDMCIhYwUjIt1E5IAohDEMwzC6DqEUjIg8LSIHiEgfYDGwTER+mSbNv0Rks4gs9ITdJiIbRGSu+/dlz7lbRKRKRJaJyPme8DNFZIF77h4RETd8PxEZ44ZPF5EhGd67YRiGESFhLZhhqroTuBh4AzgSuDJNmkeBC3zC71bV09y/NwBEZBgwEjjZTXOfiFS48e8HrgOGun+xPK8FtqnqccDdwJ0h78UwDKMsqd/dzNSVW4stRmjCKpgeItIDR8G8qqotgKZKoKqTgPqQ+V8EPKuqe1V1NVAFjBCRw4ADVHWqqirwuCtDLM1j7vELwHkx68YwDKMrcsVD07j8oWm0t6esfkuGsArmAWAN0AeYJCJHATuzvOaPRGS+24XWzw0bBKz3xKl2wwa5x4nhcWlUtRXYARzsd0ERuU5EKkWksq6uLkuxDcMwisvSTQ3FFiEjQikYVb1HVQep6pfVYS3w2Syudz9wLHAaUAPc5Yb7WR6aIjxVmuRA1QdVdbiqDh84cGBGAhuG0XUZcvPr/HHskmKL0WUJO8h/qIiMFpGx7u9hwNWZXkxVa1W1TVXbgYeAEe6pauAIT9TBwEY3fLBPeFwaEekOHEj4LjnDMAwAHnhvVbFFyJjy6CAL30X2KPAmcLj7eznw00wv5o6pxPg6EPMwew0Y6XqGHY0zmD9DVWuABhE52x1fuQp41ZMmpuQuASa64zSGYRhGCdA9ZLwBqvqciNwCzpiHiLSlSiAizwCfAQaISDXwW+AzInIajgJeA1zv5rdIRJ7DcYFuBW5U1Vj+N+AouF7AWPcPYDTwhIhU4VguI0Pei2EYRlnjtKVL36cprILZLSIH41pmInI2zqB6IKp6uU/w6BTxRwGjfMIrgVN8wpuAS1OLbdz6ykJa29v54zdOLbYohmHkiXLpqgmrYH6O0yV1rIhMAQbidEsZJc4T09YCmIIxDKPghFIwqjpbRD4NnIBjly1z58IYhmEYBaZcRptTKhgR+UbAqeNFBFV9KQKZDMMwjBRomXSSpbNgvprinAKmYAzDMAxfUioYVf1uoQQxDMMwwpHYRba3tY2eFd0otdWywg7yIyIX4ixG+aFYmKr+PgqhDMMwjHDs2NPCR3/3Fj//wvH85LyhxRYnjrAz+f8JXAb8GGeQ/1LgqAjlMgzDMEKwdddeAF6aXZ0mZuEJO5P/46p6Fc7y+L8DziF+aRfDMAyjQPh5kZVa9xiEVzB73P+NInI4zmz7o6MRyTAMw0iFnxdZKa6UFXYM5j8ichDwJ2CWG/ZwJBIZhmEYXYJ082DOAtar6u3u777AAmApzi6ShmEYRoHpKl1kDwDNACJyLnCHG7YDeDBa0QzDMIxyJl0XWYWqxvZYuQx4UFVfBF4UkbmRSmYYhmH4ogHHpUY6C6bC3cwL4Dxgoudc6Dk0hlFOvDynmsUbs90R3IiSsQtqqNmxJ33ELo7fgH7pdZClVxLPAO+JyBYcT7LJACJyHGmW6zeMcuVnY+YBsOaOC4ssieGlvV254anZDO7Xi/f/7+eKLY4RgnRLxYwSkQnAYcBbnh0ju+FMujQMwygIscpnw/b8WDCl6NYblrgushK+jbTdXKo6zSdseTTiGIZhpCZfFWopV8zpKBfZw060NAzDKCrlbHHsq5iCMTLmzNvHc9Ozc4otRlGoa9jLx/7nbZbXNhRblH2OfKuXslZXfsKX4Ci/KRgjY7bububVuRuLLUZReHtJLbU79zJ68upii2Lsw5TLhmOmYAwjA0qwkbjPkO8esq7T5Va692EKxjCMsqBcWu2FwHepmMKLkRZTMIaRBVbZlT/2BKPHFIxhZEAJrie4z9BlerTyQLnMgzEFY1C1eReXPTCVxubWYotiFBhVpb29hGuoCPGrmC97YCrjFtYUXpgMKZfxI1MwBn98YwnTV9fzQdXWYotSNpTJ952WByet4phfv8GOPS3FFiUj/vneykgU4/TV9fzgydl5zzdKStmqjkzBiMi/RGSziCz0hPUXkfEissL9389z7hYRqRKRZSJyvif8TBFZ4J67R9xND0RkPxEZ44ZPF5EhUd2LYcSQkhxKzZ5nZ64HYIu7r3sp41Xqd4xdyoSlm3PLL2EUplysArAuMoBHgQsSwm4GJqjqUGCC+xsRGQaMBE5209wnIhVumvuB64Ch7l8sz2uBbap6HM7mZ3dGdieGkUAJf9NZUY5qs7m1Pa/5lXJFnUhX2XAsa1R1ElCfEHwR8Jh7/BhwsSf8WVXdq6qrgSpghIgcBhygqlPdhTYfT0gTy+sF4DwpxRIuA8rouyo+XfQNK4d3IN+ee+WkUMqVQo/BHKqqNQDu/0Pc8EHAek+8ajdskHucGB6XRlVbcbYPODgyyfcBTD3ve5TTI89UITS3tnPl6Oks3BBuZ5Fy0jdeZVvKcpfKIL/fe64pwlOlSc5c5DoRqRSRyrq6uixF7PpYiy48XaWsusht+LK4ZieTV2zhv19eECp+OY3B+D24UmwsFFrB1LrdXrj/Y6N01cARnniDgY1u+GCf8Lg07q6bB5LcJQeAqj6oqsNVdfjAgQPzdCtdh1J8MUuVrlpW5XBfmVb/7TGFEdI0LyP1UjYUWsG8BlztHl8NvOoJH+l6hh2NM5g/w+1GaxCRs93xlasS0sTyugSYqGXVBCkdrNCMciDTz7tDv6Q5Xwy27W7mR0/PpqEpO/fwcvlmo3RTfgaYCpwgItUici1wB/AFEVkBfMH9jaouAp4DFgPjgBtVtc3N6gbgYZyB/5XAWDd8NHCwiFQBP8f1SDMyQ1WZ6Lp72hhMeMppqZjX5m2kqaUtfcQuh/OMwr7XhVQ4971bxX/m1/D09HWFu2gRSLujZbao6uUBp84LiD8KGOUTXgmc4hPeBFyai4wGTFiS21yCfY1yc1ScvmorP3lmDt/62JGM+vpHks6Xk9GfqaRpLZjEeTAFbDR06+ZIle1cUe9jK+VHGJmCMcqDXXs7l4cp5Rd1X0ZVs342DU3O8920oyllvHJQnJmWQYc3UAneWzeJKZjsHmy8F1lmllohKRUvMqNIlOJLWRYUUBk/OX0dx/z6jZRx6nc353SNcrJkwhK7pW4B73jiLReyCGIy5aPcS/nRmYIxOjBlk55iFNFLs6tTnp+0vI4zbh/Pu8sy7+4sxdZ9IBlWpDHroBSX9+m0YLJLXy5dZKZg9nG6lVMFY/hSuXYbAHPXb086l67uKSfLJdMxkrcW1QLBDadi3nlMsbdlqWHi1iKjhBVpsQUwiospmOwopWo5piT8nmXsXLrHXFaWTEj+NWU1UJpeZBWuUPlU8KXo2WgKpgsyfdVWVtXtChW3C9YrkVKK5dXeoWBSxSpBwdPQ1NLG5BWdK29kWxcHtewTK/eCepG5IoU1YN5bXseQm1/v+O2VvZSNUFMwXZDLHpzG5+56L1Tc8qt2jETa3EWFs7FCSrhu4paXFnDl6Bms2bIbyF7WUrRgYm7K975Txe696Tf6+8fEqrjfvqspl+DXbApmH8f78V37WCUz1/iutmMkUEpjFzFZKlKbMCn5+8QVzHLHckqFGauddzFXq7E0x2A6j3Pd7K2EXsUkTMHs4yS2en/76qIiSVIelG8XWWpemr2Bb97/QZ4kCmbX3tY4l+r63c2Bqwxsa3TixRRntko9aJwxyU05q9yzoyLNi7SnuS2uXFJ135Xi2EsMUzD7OJkM8u/Y0xLXD2yUBrEusnJw2PjUnRM54/bxHb/PuH0837jPX7HtdTcUiymCqKvRQlql6azNk34zjlNve6vjd6qxmgzX9CwopmD2cTJ5J5dtagg819rWzkOTVrG3dd9Y86qQbcZ09V57Ci+yUmNbY3J30OKanb5xYy68udb7gWNTRWr4797bysOTV6eN19wWvGNn3DyYfAgVEaZg9nG65ekNeGbmeka9sYR/vrsqPxmWKFEMpC7dtJNFG8NtiuWHpugiK+XKJyyxLqDsvcjCXqcw3DF2KZt2pl66J5FUHm/pLK+mlraiLXZqCmYfJ18VZqPrCbNrb24DlsWmGIP3F/x1Mhfe837H7wlLapm0PPzGeG0xBZOi26UMjJtAOrvIsns2wQZMQqVdoEefbon+dp/+sDCiLQ3oYRj2m3Fx3ZKFxBTMvk6eKp58V2CvzNmQ1dIn+URVGb+41ne2dZSV0bWPVXLVv2Z0/A4q2znrtnHirWOpa9gLlEcXWTbkWtShy6UEzL1FG3ekXXcOMusia1dobDYLxigCmVRKia372hBmfs2OPQy5+XWmr9qakVw/HTOX7zwyM6M0+cB7i28uquX7j1fywKSVHWGlVIc/NHkVTS3tTF3plG02CqaUXVxjaMAof9jbLbUNx1LNV5pStcU3PJXHWyk/Q1Mw+zi5uLb6Lbj30OTVcbOvp7mK5ekZ5bGxkvdbrWtwFOiGbXuS4r02byPbG3NbwThfpFs1uNwJqj/DVqzeCn3D9j186+Fp7PTppioFd9+ge0otWefZJTU7qdoc7IxTaEzB7OPkMgYT1BC7cnRn9066TZ/KgaD7nFed/cB8PiknL7JsyNVN2Vss97y9gilVW3l9fk1SfqVgCYQVIWipmC/9bTKf/8uk/AqVA6ZgishNz87htteKO7ExsU7K5BsLU511+uiXR+VXSjP0Y6R3U3b+l0kRZ0H+vcgyLaobnpzFq3M3ZCdAPkjyIusk1kUaH7003mNTMEXk1bkbefSDNUWVIadKyZM27Wq9OVymkPh9lv+ZX1NwOcIQq0NSWTAlUs/kRO7zYDx5pXDvTXWZsQs3cdOzc3MTJCZPinPZdJHdNX55UtjUDMc8o8IUzD5OYhdZqpZP4plSXFwvCrY3tjC/enuxxQgk9shyWYusVPj+45W8OCt+gzXt+J+lm7LnPU01670Qrf5vPzydcYs2BZ5P3EJ5Ra0znpLp7puxVRCKjSkYIzTZfH/l1oAOusdd7jyfKLv6rn+iktN+/1b6iAl07NyYQrRyUT3jF9fyi+fnxYV1jMGkeJkmLq1l/OJa33PxFowbhiSPwWQmala8X7UlyWV4xup637kvAKs7VpLOXdpsNzfLBVMw+zhRe85okUf5dzS28M7S8PNpCu1J9P9eWdBx/OaiWrb7LKWSTqe1pRzkT+d/VJpNAO/M88RWfQzv7V7zaCXff7zSN17P7j7VXAlp3J+OmcsDk5wVMIKsqHwYV+8tL/y8MlMw+zo5vLhhKidvi7EY/PDpWXz30Zls2bU347SFqHqfnJa9+3asgu10U44v4z3NbUyp2hoXt1zwjnsFeZFt3tnEV/4+mU07Us/H+lD3iqS8Eo/9fuebVF1wKwM2CIxZzPmQtbWt8I2J7gW/olFS5PTKhUlcZA+nVXVOF0NzyD7pdB9uOdXT//3yAl6aU0TPpxzo7hlP6lyLLP7hPDtzPUs3NfDU9LUp8/JaMJ371ycTtTUXZkXkKJVcMWxVs2AypLm1vWPyYFcglxfamzSdhVJOFXOpEehZlDS7Oz6gKuS22aWI12Eh6P5jrft0Ywv7ebvIPG7zhe4eDONEkOxI4x+eLidVZfKKuqJvrWwKJkNGvb6YkQ9OY+km/yXGy4W6hr3s3tua00f2sf+ZwA6fMQMvqfJ/ePKqjJeQyZR8K7Z8WmLrtjbmLzMfsnFIWBKwdH6h6e7jEZdYQVa4tVdbmpoz3oJx8C2ZiCvglBaMe/GN25NXjQAfl2r35w+emOUb/+np67hy9Az+Hedib4P8JU9s74qde9Lvo13KnDXqbb527/vpIwLbdjfzfOV6X2WxZuvuUHn41XV/eH0Jlz04LVT6XAk/Q9r/OArO/d938ppfUgWchTL80t8mdxwXc5n3MBZMbFfIdI2c7hUeBeP1uEvI12+vmnwSpjH37Mz1GeUZ5PK8vt5RVN5ljvYZC0ZE1ojIAhGZKyKVblh/ERkvIivc//088W8RkSoRWSYi53vCz3TzqRKReyTi6eLbG5tZ47Y6o5pzULNjT8E+6pV1u0O9dD9+Zg6/fGE+a7Ykt7hbA5plL8+p5oH3Vub8UrcmbLq0dNPOUF0NmxuailY5Fouk7hXP5+DtwtzZ1MJDk1al8Fhywj9y25uc9JtxeZczDN0rksdgEondX1ClvP9+7hCzz3361RQ/fmZ2hlJmRj4r+HTKqsMBxDuxNH+XD00xLZjPquppqjrc/X0zMEFVhwIT3N+IyDBgJHAycAFwn4jE3ELuB64Dhrp/F0Qp8Gf+/G7H0uh+Jnw+OOePE/nhU9G+6F5i8ztSEVs1ucVnh71EBRDjZ2Pm8cexS3P2Ivv7xKqO41lrt3HBXyeH2g1wxKgJcUvehyWwMnPlL+XJpYkKI+gVve21RYx6YwlH3+K/LHyszdDSpkVbCaCim9fqiP8fI903OKhfr6QwDTgGqEnjjZYrKcsycJzJP23Y51JIi9yPUuoiuwh4zD1+DLjYE/6squ5V1dVAFTBCRA4DDlDVqep8WY970kSCd45ClLOmJ2YwbyNXMlFmfi3e1nZFVRlT6d+KzHW/8GqPiV+9zbGg5oWcVT9jdb3HzTPc1xX/QXpbf0pTS1tJzRtJ6/EWUOje7l2/PNKVVfW2Rs6/exKbG5IrZFXNy4S+CvFaME6+762I34Qt7DcYRprP/fldGprSN7amVG1Ju2FYsBzZl0umaf2efdB8oigploJR4C0RmSUi17lhh6pqDYD7/xA3fBDgrb2q3bBB7nFieBIicp2IVIpIZV1d+J0CU5HJwwpfuZVO5RWWlrZ2nq+spmpzao+lbBWMN123gDkBhWDnnlZOvHUc93osqkIRer+shHLJtg2UWLwfrNzCZQ9M7bBWH52yhmW1Dbw6Z2NS2u8/PotjEzbM2t7Y7Ls8fmoZvN5PynOV67n1lYVxcdIpGD/LJ3b873k1Hb0RAKu2BI8ler/Lbz08nZ88Myed+CnlCcKvhyBbCyZWMkErBBSKYimYT6jqGcCXgBtF5NwUcYOmJ4eetqyqD6rqcFUdPnDgwMyl9SGKZReK/C6kxa9V1NauLKsN3n8iny3+WIUSZUssKOfYRM2gbWmjJPF2g7o1E8s62+X7E8v3p8/OZfrqerbs8t//5rEP1jD8D86WvG8vSV6u5bTfj+e038UvgTPk5tf53zeXppCh81jx35Mn3f11zJ/xGYeYuHQz33vMf+Z/Uj4J5b+8NrgxNaVqC0Nufp21Ps4v6d7bVGOGyW7KqfOK9TCm6hIsBEVRMKq60f2/GXgZGAHUut1euP9j/UTVwBGe5IOBjW74YJ/wghCNgikdDfPYB2tYvDHeZdXPwmpJMzu4M0nuXYqxBmsx1lQqnSdDx7LxSVstJLnxBu7lmDL/oN0TOweO4/nta4sClU8Mv0f2j3dWJgfG4nsSqKqvGZdOf/pbMJ0/NoXYkRUy+y5Hv++MD/o1RNINwbSnmAucae9GbKww6N4LRcEVjIj0EZH9Y8fAF4GFwGvA1W60q4FX3ePXgJEisp+IHI0zmD/D7UZrEJGzXe+xqzxpIifIe8qPbAbkvDQ2t/Jf/5zK8hSWQr757WuL+PI9k9PGa21vT3l/iZVTLsQsqGz0S/hnEBCxhJR/EIkiZutUGdQd45fdnHXbsrqG/3U7L+yt1FX9mydpu8gS/mctV+LvFO/Czj1OV+CBvXok55NGgWzZHX45o/Tjb2484sux0BTDgjkUeF9E5gEzgNdVdRxwB/AFEVkBfMH9jaouAp4DFgPjgBtVNWZL3gA8jDPwvxIYW6ibKIQFU7mmnu88MoMpVVuZsaaeP76xJO/XzAS/O25r11DdYLGq4NTb3uS7j2Tu3QWdXSJtqZp6ORJ0J37h2Q725krYPUO81W8muiZo5V4/D7rtezrLINdJmt778n5fir/8FT6Bk1ekHmPN5qsNsugAFm7YwZCbX2d5bQOqSuVaR+H6rfuV7ju5anTwd5Gp3B1jMHFdjfuABaOqq1T1o+7fyao6yg3fqqrnqepQ93+9J80oVT1WVU9Q1bGe8EpVPcU99yMtoA2YrYLxDiwmkij9j5+Zw7vL6nxN+b9PWMHc9duzkiEdmRSjamYto51NrbyzLDtHi9h8uXeW1bF26242NzSxY0+4Sj6KN+NHT4cb7K3avIvZeWzpx0i2NBLHYPKVb6rInYfeSZpZXddzHFcxBlzfz0K7cvSMDmeEWHnEpc/G+k1I5M0vtijnn99cxux12zvC/Qbs072DG3xm8Xco9YS0zW3tgVMEnIQB3gEFppTclMuKbBXMH15fHHgu/eSpzg/qrvHLufgfU7KSIR2Z3JufzHHffR5fcO/9L964kxGjJvDJOycmy+RzzTD96DubWuLKNF+Sf/4v7/GN+z7IU27B1kiivGEG+f2KJbGsOruaoq2svM/tr28vjwt/bV7y8GpFQO2V2DUWP8if+T2kWvMtdvzW4lpWejwp/RRMLmOsiV5u37jvg5QKPfbkvXN79pUusi5BJmMwif3JwfFykSh/ZHRvPo2oOP0SC8vDGIy3SyQmY+LchQv+OomfeLa2DRqY9uOdpZs7Vl9OJB8fZ9gVndMRKEtCeLcsTZjE7GNyjxg1Ic6DLZ/rZjQ0tbDbsxGXd5Bcwfe5BCnQhqZWxyPLZxAmm+cY1qL71YvzO44nLt3M5+56N84zLKQhGE+KMl6RYmpA7NE/79kdtBj1iy3XnyWZtPLDRg3bwonatz2jOT4kWwwi0vEVdgwQIwkru2Z+D94KJaj8l25q8PXgSXdP6+sbU+65HiTv+vpGjujfO2XeMWp3NoWOmw3JbspZ5pNwq3s8leSf31zWMbiezxbxR257q3NplzTypOOM28czcP/96OvmF9dDlo8ussAfncSWr1lX38jbS2r56OCDGHpo38wvniXe7roY+4QXWVehNYOB5rAVdiovk/hrx+fX0tbOPRNW0NicnwU4gywY3+6UNMrOu7igd5/wbDzrvBWmXxdEGDmCCNputyN9QPjsddvY29rGra8sZGuaTc0+9ad3Up4PS2AXWaIFk6WJsbe1La5/31t2e1ujW9+tIeT8nhipbq+uYa9nDCa3ijXbZVoAvnj3JP40bhnfenh6/vpcs6QYlzcFkyWZGBGhK9OQr8CijTvifr84q5q/jF/O3yasSLhudq9UWwY737W2J0vt20VGvOLKpj9aQlgwyWmc/5//y6SU8fxyW7Bhh09oPN27dWP84lqemLaWP7we7+U3fdVWKtfUx4W9uyz3ZYDCepF5FUwmumbEqAlceE/nStupu3air7YenbLG/9ppLt3RQ5aFiN97bGZSPsk5Z1ZppyzHQtT+NgZTPiRW3vPWb+fvCRV8jLgxmBR5hlVaX08YMI718+5pjm9dZtuTlskYTLq9OLyELYcgvJVZSwE6lF+a3bkbZNBtVnQT9nO35H3Zs3vkzDX1XPbgNC7559S4+Okspbwi3sP0y9978a7OEB8/XlMVomJ8K6DM0jVS/E6HVYhvL+lsCATtxeJ3LhXFnki9T7gpdxUSX5aL/jGFu8Yv940bthIOfAEzchvujJutp5s3nde91k++trbkiZZBreX2HC0Yb5K9CctqPF+5nk/ckexRFkRjcysTl9aGXtI/SNoeFUKfnp17vk9ftZXWtnYuTVAsMZ6avi7j55Lojh51F1kqvFkWaxvsGGGLMdcxmMTrZJtfqrh+XnJOmvwpBfMiKyMymesXdmzF+wL8zxtLOpZhCfshSUIemYwTefGmu3Ns53pRfi9oq89ES+/yMTHZRSTU3IZUeNN4rbV/vFPFL1+Y7zuPIIgrHprONY9WcuKt41iTYqHDzmsHC+z11trZ1JrWApy+Ovwunpl4niU+B78Nx34+Zi4Tclitu7m1PU4pB3vdJZdBvp1T0lW+sfIY/f7qjmeclQQJiWKNo//M38jD7tIwWWQTLk0eiyyqeXOpMAWTJUEt8MSXvr1dmbA02cT3+zi8YQ9OWtWxwGJoJwHiX+KW1nAyxp9Ltnxi7pB+llg62WKVSkU3iYubKt37K7b45+VJ43Vp/d83lwXmFbR/i/djW7F5V/rKKuB0S1t73KxtVU3rgJDJRmiZdGuEsWBe8nTjZcMLs6p5avo6ACrXbGNUwOoSfuWVSXdqGNLpK+/lrnHHVLKxCIImWoadaBsjGwWbT5387Mz1gd9WVJiCyZL3q7YwbmHydqWJlfPTM9bx8+fmdfyOveD/fG9VUtqgl6nDCgiQJWjNqZYACybdN5a4TEenHMkJnf1gUuSlQQomOM23R0/3Dfcm2ZMnjzmA3/17Udo4zQFKo7lN4yw+xX+ZEC/XPBpuFV/wt5TTDfK/s3QzK+t2hVqLLBOrL5GquuB5GH7vyvcfD3/fYchkDCam9DMZX/TLx/lduL6mfF9rw/bkXWmjxBRMlrw6dyM/eHJWUnhiK63aZ5lxgOdnxW/QVbV5F2f/cYJv3Lh9xNPw2rzOFuruALfPdB9m0BiBXwus3ceLzO9a3UTiKkuvDGfePp5vP+yvVDZu9+4p7m/B5ErQM/ISZCW1tLYnKZQgxZ4NqZ5VUsm7cb/76EzOu+u9OLfufC5KGSPVUiV+r9C7WS4RlMk1gugmwrbdzVmNSyamCMphyMGp5zhlM+7Yrs64Xr6IeFf5JEzB5JlUy0o4v/35xfPzAs5k0EWmMG1lp1vsb171b5mnym3j9j2B66X5fZvpLJg57oSvbhK8osHW3c28X+Vvuk/1fFxBYzD54N53sttIrLmtPa5VrJp+CwOAaSErDb9upaA64r3l8WXolWtjBNsBp7IGYs/6g4Dnmg/Sd2t2nl+7tZHTbx/P5Cy6iJK+vzQWZBB7Muga9eZ62YPTskjnTxSOH6mwmfwhCWuqZuu5NS/FAFy6LL3vjHfr4oUB8zhSKayGva1c4bUmPFH97u2vb6/g8hFHBuYXc8tN7CILKs+qzeE2L9udxy4yiN8OOxNa2trjusjGLaxh1tr6FCkcRoasNPwcRIIe39tLalm3tbML5OUcx1vSEUbJXxFgmeaDtF1kebjGu8s2MyVBSQbl25LGIWPMTP9txVORb2/8CHd698UUTEhiq6amY1ltA2cc2a8zIA8vyB1jg3f+g+AKx68Snl+9nQ8f8KGs5AgahwiDiMTJ6d0XPsa6rY0pJ0R6e54a94ZrDWY64z9Tmlvb4xTvK3Pzu+ddqoHx6auTFVmmWxPnwtbdwZuMFWLOR7rGXD5E+M4jM5PCdu1t5d8+bsXp5mZl1T2XdwVjXWQlSbo952P88MnZqSMoPDhpJZt3ht9cyMuYmesCzyW+O00tyZXr1+6dErr1DPFWw/3v+u9AGKaPuEIk7gNbXJNsXdWlWWrF+62FtWC8y9MMufl1akPuYhiWljYN1SWWLX4Vdew5+1ldxdjt049CiJHuGmF3rMyGHz+T7EG2LYXCBXh86tqMr5NvRV3ouUtmwYQk7GPe1hj/kiWmW7G5gdcXhLOG/Pi/Ly4IPJfqXVxSs5MnpzkveOLS37kSlF/vnhU0erpRvB/LD3wU8aaAcQJVdefRdKZvDDkGkzghc+GGHRyapQXnR0tbe6QboPk5VqR6zqnGRfK1Vl0Y2lUjXbcMCuvNFYZsPNTS4afIciHdLqD5xhRMWEK+zHsT+mETP4LsBvoc/LpEZq3dFqpVcsVD09iWxThDLt+wVwkomrbFeePT/tZfa7vSoyK+iy1sZZn4PPLdwm9pa2dXyO66TBn9/mpu/0/w/kF+pLq/L+e4IVgmTFmxhTcXJbvx55PmCC3HrooN8pco+XqVN27P3mxP3PsE4Jv3h9vIyi9tGFLtOZEJ7Zp9i/P6J2ZxzSeO7tjvHMKPwSS2Kq97YhZr7rgwKzn8aG5rZ0dj6q6RbPnTOP+xt227m3kuYMA41eoNa7YWbg7EDU+l6SrOAxtCuJcb8RR6dR9TMHngF8/5uxivr29Mat1G2UeeakA4W/M97JbEadH0FkwQE5duZmLC8iaNOViC+exaaW5tpynPLtMxghwUgta8A/I+xlTKbEkzZmckk+/VFNJhCiYkqZ7Li7OrfcPD7v/xwiz/9Jny9PRgB4Bio+RXueaSVz51fEtbe+jxoEzJRs6fjQmeT2VkxqNTVhdbhLxTaCcQ8yILSdB89Xwoh/+TYpJlV0G1+MuVxwjrERiGllbN+6RPozS47d+ZjX+VA4VWMGbBhCSobtwXlEM+UFLP+C8klz+Uv5nRYyrXM7hfr7zlZxhRYhZMiZLpY8n30uTlTnsJWTD1aeYrZEqYtcwMoxQo9DdoCiYicpn1Xg68/fNzM4qvWvgBRsMw4olirk4qTMGEJJO6sb09/Z4g5c5xh+yfUXxVLbmJcYaxr3FU/z4FvZ4pmJBksvHTrubWSJcPKUfmVW+3MjFy5uPHHpwU9uh3z+KEQzNr8BSaM448KNL8LzlzcKh4nxw6IFI5Eil7BSMiF4jIMhGpEpGbo7rOF046NHTc3Xtbu7wFkynTVtVntAZaV+LMo/qlj9SFGXF0/7zlddTByS3wM47qVzLje14O7NWj4/jqjw/h+EP7JsXpu19+/KxuvXBYXvLJN2WtYESkAvgH8CVgGHC5iERS0icedkDouH8dv4LVeV7vyyhfDj1gv2KLkJJvnD4o0vx/+vmhecurwqfGEkpzfM+77JeI+G7ffcEpH/ZNO/lXn83oWhUVTt4f6pFcQCtGfSmjvPJJWSsYYARQpaqrVLUZeBa4KIoL9epRETrumMr1aVvrhVp07vyTgy2vF2/4eMfx09/7WNy5X55/gm93hJd5v/libsLtA/zjijM4ZdCBKeM8dNXwAkmTzNdPH8Rd//VR+vXubG179/dJ9d736dl57sj+ybs5Du7Xi369eyStB5cLhx2Y7BIuIvTr3TN0Hl7LIiwXn3Y4PbtnVl328VgnPboJffZLLstRXz/FN+0RCeV5VJrdMivcNcZOHXxQ0rkeFd149/98JmPHnHxQ7vNgBgHeRZmqgY8lRhKR64DrAI48MnhzrFRUdBP+95JTWbhhB0f07834xbVUrt2GqtKrRwUv/fATrN6ymx8/M7tjrOHwAz/Exh1NfOaEgVx82iCW1OykV88KPn38QE4+/EDuHLeU91ds4dzjB7B1VzMr63ZxcN/9+MwJAzmif2927mnhotMG8caCGiavqOPcoQN5bd5Gjuzfmx17Whi3aBM3nTeUz590KC/MquaAXj1oammjtU15Ze4G+uxXwU/OG8pZQ/rz8pwN7N7bylc/ejgbtzfRo0I4+fADeOQ7Z/HSnA2cc+zBjL3pU3zpb5P5+LEHc8WII7nh08dyx7ilzFhdz5Zde/n8SYfSZ78KhrjdFAf27sElZw7mhVnV/P3y0znxw/vzwuxqBh3Ui949u/OrF+YxZEAfvv+pY7jlJWcV6KGH9GVQv15s3dXMgg07OHpAH1Zv2c3pRx7UsfvlZ08YyEePOIi1Wxv5yKADOepgp7xrdjSxYMMOvv+pY+jXuwebdjbxfGU1Qw/tyz+/fSbjF9cyZ912zjjqIGq2N7GstoF1Wxs5on9vBuzfk749u3PX+OWMPOsIltc2cET/3qzd2si6+ka+cuphPD19HQf17sld//VRHvtgDRXdhCP792Z5bQPXfPJoFlbvoLahic+deAjV2/ZQv7uZuoa99KjoxrnHD2DNlkbqdzezfU8zDU2tHNirBxec8mGaW9vZ0djCWUP609qu/ODJWZx5VD9+/eWTmLG6ns+fdAgPXTWc/3ljCWcfczAfO7o/H6zcQnNrO1NWbqW5tZ0vf+QwvnHGICav2MLkFXWsqN3FgL49ufUrw2hqaeeQA/ZjRe0u1tU3csaRB1G7s4kde1oYdvgB7GluZ371dkSE2eu2cdU5R/Gjp+fwg08fy81fOhGAx64ZwYQlm2lqaePnXzyer556GPe+U8W9V5zB0pqdjFu0ifGLa/ntV4dx/skf5o9jl/KpoQPYvbeVHhXdGHb4AYx6fQmNzW18+viBDBnQh08fPxBwtla+/tPHcP25xzJ+8Sa27GpmQN+enH/yh7n/3ZUcPaAPQwb04bmZ63l13kaevPZjVHQTRhzdn/rdzTw0eRXfPGMwby3exLWfPJqdTS2ccOj+TFu1lX59etKnZwW//eowxsxcz8mHH8i2Rif/xuY2tje20NzWToUImxuaOOrgPlxy5mCW1OxkfX0jG7Y3cf25x/DApFW0q7KkZic3nTeUl+ds4KbzhvLm4lqemb6O2752Mj/63HG8tbgWVRi4/35U1zdS39jMk9PW8cvzT+C4Q/oyaXkd1dv2oMDvv3YyvXtWMPr91Xxh2KGcesRBPDtjHa3tyvc/dQz9+zhK8dUbP8G86u0cPaAPU1du7ehSfOQ7Z/HIB2vAzavvh7rz0KRVDBnQh3X1jbS3K1877XCmr6qnV88Knrv+HE748P68OKuaxTU76dOzgvNdC2nIgMIO7seQcvbsEZFLgfNV9Xvu7yuBEar646A0w4cP18rKykKJaBiG0SUQkVmqmpG5Xe5dZNXAEZ7fg4H8biloGIZhZEW5K5iZwFAROVpEegIjgdeKLJNhGIZBmY/BqGqriPwIeBOoAP6lqouKLJZhGIZBmSsYAFV9A3ij2HIYhmEY8ZR7F5lhGIZRopiCMQzDMCLBFIxhGIYRCaZgDMMwjEgo64mW2SAidcDaLJMPALbkUZx8YXJlRqnKBaUpWynKFKNUZeuKch2lqgMzSbDPKZhcEJHKTGeyFgKTKzNKVS4oTdlKUaYYpSqbyeVgXWSGYRhGJJiCMQzDMCLBFExmPFhsAQIwuTKjVOWC0pStFGWKUaqymVzYGIxhGIYREWbBGIZhGJFgCsYwDMOIBlXtsn84e8W8AywBFgE3ueH9gfHACvd/Pzf8YDf+LuDehLzeBZYBc92/QwKueSawAKgC7qGzG/JcYDbQClxfQnLd7Um7CmjJk1w9cfp7lwNLgW+WSHnlKlfeywvY35PnXJx5Cn8tdnnlSS5veS0HduTxWV7uXnM+MA4YkEOZLSwRmaIsr8tcuRYBf0pRb4Ypr0tC1cGZVtrl9AccBpzh+ViWA8OAPwE3u+E3A3e6x32ATwI/8Hk47wLDQ1xzBnAOIMBY4Etu+BDgVOBx4HulIldCnP8GXs2TXL8D/uAed0vxoRW6vHKSK6rySsh3FnBuKZRXrnIlxPkx8Ew+ZMNZCX5z7Pm56W/LssyeB35ZCjJFWF4HA+uAge7vx4DzcnjHQimYLt1Fpqo1qjrbPW7AaQUMAi7CKWDc/xe7cXar6vtAUzbXE5HDgANUdao6T+RxT95rVHU+0A5sLxW5ErgQuDdPcl0D/NGN166qSbOHi1ReOcmVQD7LK3btocAhwORM5Ir6/cpWrgQux9mzKR+yifvXR0QEOACf3WxDltluYHUpyBRheR0DLFfVOvf328A3syyvdh9ZfenSCsaLiAwBTgemA4eqag04SgjnwwnDIyIyV0RudV+gRAbhbOMco9oNK3m5ROQo4GhgYq5yichB7uHtIjJbRJ4XkUOzkcsn75KQK5/llcDlwBj3485YrkRKRa7E8spVNlVtAW7A6crZiNOqH52NbAlyloRM+S4vnO6uE0VkiIh0x1EaR/jEy/gdS8U+oWBEpC/wIvBTVd2ZZTbfUtWPAJ9y/670u5RPmN8HWYpyjQReUNW2PMjVHRgMTFHVM4CpwJ+zlKszcmnJlc/ySsz3mYBzxXi/8iVXR3nlQzYR6YFTmZ8OHI4ztnBLlrLF8iwlmfJaXqq6zZVtDI4VugZnLCUb2ULT5RWM+9BfBJ5S1Zfc4FrXFIyZhJvT5aOqG9z/DcDTwAgRqXAth7ki8nscbT/Yk2wwPiZyico1EngmT3JtBRqBl93fzwNnlEB55VOufJZX7B4/CnRX1Vnu72KXVz7l6lBQeZLtNABVXelaVc8BH8+hzCpKTKZ8lxeq+m9V/ZiqnoPjGLQil3csDF1awbjdRaOBJar6F8+p14Cr3eOrgVfT5NNdRAa4xz2ArwALVbVNVU9z/37jmqoNInK2e+2rUuRdMnKJyAlAP5wWfc5yuR/Xv4HPuEHnAYuLXV75kivf5eXhcjxWQrHLK19yecsrX98ksAEYJiKx1X2/4OaZbZndUCoyRVReiMgh7v9+wA+Bh3N8x9KjITwByvUPx5tCcUzVue7fl3E8KibguPhNAPp70qwB6nHc/Kpx+lH74HjQxFz8/gZUBFxzOI7L40qcAeCYi99Zbn67cVwPS0Iu99xtwB35Ki83/ChgkpvXBODIYpdXPuSKqrzcc6uAE9O80wUtr1zl8pZXPr9JN/wHOIPe83EaDgdnWWZ7XJmaii1TxOX1DLDY/RuZ4zu2FViUrg62pWIMwzCMSOjSXWSGYRhG8TAFYxiGYUSCKRjDMAwjEkzBGIZhGJFgCsYwDMOIBFMwhpEGERksIq+KyAoRWSkifxORniHTvisiw3O49m0issGdCLdCRF4SkWEh0n1HRA7P9rqGkQ9MwRhGCtzJZi8Br6jqUOB4oC8wyidu9zxcr8In+G51JsINxVnqY6JnMl8Q38FZrsQwioYpGMNIzeeAJlV9BJxZ7MDPgGtEpLdrKTwvIv8G3hKRXiLyrIjMF5ExQK9YRiLyRRGZKp0LbfZ1w9eIyG9E5H3g0lTCqOoY4C3gCjftb0RkpogsFJEHxeESnMlyT7mWTy8ROVNE3hORWSLyZmyZEcOIElMwhpGak3FWS+hAncUG1wHHuUHnAFer6udwlhxpVNVTcaycMwHcJX3+H/B5dRbarAR+7sm2SVU/qarPhpBpNnCie3yvqp6lqqfgKLOvqOoLbv7fUtXTcBY1/DvOHh5nAv/CxwIzjHyTs0lvGF0cwX81WW/4eFWtd4/PxdkFEFWdLyLz3fCzcZb3meL0utETZy2zGGMylCnGZ0XkV0BvnF0OF+EsUeLlBOAUYLx77QqgJoPrGUZWmIIxjNQsImFjJhE5AGcvjZU4FsruhDRBCmm8ql4ecJ3EPFJxOlApIh8C7sPZ0XS9iNwGfCjg2ovUWUXXMAqGdZEZRmomAL1F5CroGIS/C3hUVRt94k8CvuXGPQVni1mAacAnROQ491xvETk+U2FE5JvAF3EWLowpky3ueM4lnqgNOFvsgrM0+0AROcfNo4eInJzptQ0jU0zBGEYK1FkN9uvApSKyAmdP9Cbg1wFJ7gf6ul1jv8LZ3xx1tqr9Ds4eMvNxFM6JAXkk8rOYmzLwbeBzqlqnqtuBh3B2UHwFmOlJ8yjwTxGZi9Mldglwp4jMw1mR9+Mhr20YWWOrKRuGYRiRYBaMYRiGEQmmYAzDMIxIMAVjGIZhRIIpGMMwDCMSTMEYhmEYkWAKxjAMw4gEUzCGYRhGJPx/skdKx8IXOA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=df['Order Date'], y=df['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9a03b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date is:  2015-01-02 00:00:00\n",
      "End date is:  2018-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Start date is: \", df['Order Date'].min())\n",
    "print(\"End date is: \", df['Order Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eddb4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-10-19 00:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Order Date'][1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50c4991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-10-20 00:00:00')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Order Date'][1022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f35e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.loc[df['Order Date'] <= '2017-10-19 00:00:00'], df.loc[df['Order Date'] > '2017-10-19 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a293812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1022 entries, 0 to 1021\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Order Date  1022 non-null   datetime64[ns]\n",
      " 1   Sales       1022 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 24.0 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb989790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 437 entries, 1022 to 1458\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Order Date  437 non-null    datetime64[ns]\n",
      " 1   Sales       437 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 10.2 KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe34fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.set_index('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49a09c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>2203.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>119.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>5188.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sales\n",
       "Order Date          \n",
       "2015-01-02   468.900\n",
       "2015-01-03  2203.151\n",
       "2015-01-04   119.888\n",
       "2015-01-05     0.000\n",
       "2015-01-06  5188.520"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330ae23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.set_index('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a0744ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-20</th>\n",
       "      <td>946.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-21</th>\n",
       "      <td>7877.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-22</th>\n",
       "      <td>188.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>467.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>1365.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sales\n",
       "Order Date          \n",
       "2017-10-20   946.174\n",
       "2017-10-21  7877.272\n",
       "2017-10-22   188.280\n",
       "2017-10-23   467.422\n",
       "2017-10-24  1365.498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bfa05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.values\n",
    "test=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed66c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 468.9  ],\n",
       "       [2203.151],\n",
       "       [ 119.888],\n",
       "       ...,\n",
       "       [2953.346],\n",
       "       [ 307.92 ],\n",
       "       [   0.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715d9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.flatten()\n",
    "test = test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285efb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34001d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a5e65efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c6a9af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Row ID'],df['Order ID'],df['Ship Date'],df['Ship Mode'],df['Customer ID'],df['Customer Name'],df['Segment'],df['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c0f4a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['City'],df['State'],df['Postal Code'],df['Region'],df['Product ID'],df['Category'],df['Sub-Category'],df['Product Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "320073a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>5.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>457.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6548</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>17.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7949</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>63.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>52.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>209.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>90.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>323.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>20.720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Order Date    Sales\n",
       "540  2015-01-02  468.900\n",
       "5713 2015-01-03    5.940\n",
       "157  2015-01-03  457.568\n",
       "6548 2015-01-03   17.472\n",
       "7949 2015-01-03   63.552\n",
       "...         ...      ...\n",
       "908  2018-12-30   52.776\n",
       "645  2018-12-30  209.300\n",
       "907  2018-12-30   90.930\n",
       "906  2018-12-30  323.136\n",
       "1297 2018-12-30   20.720\n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "df = df.sort_values(by=['Order Date'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5ab52d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>5.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>457.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>17.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>63.552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sales\n",
       "Order Date         \n",
       "2015-01-02  468.900\n",
       "2015-01-03    5.940\n",
       "2015-01-03  457.568\n",
       "2015-01-03   17.472\n",
       "2015-01-03   63.552"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.set_index('Order Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "36272bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>468.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>2203.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>119.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>5188.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sales\n",
       "Order Date          \n",
       "2015-01-02   468.900\n",
       "2015-01-03  2203.151\n",
       "2015-01-04   119.888\n",
       "2015-01-05     0.000\n",
       "2015-01-06  5188.520"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.resample('D').sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d6dd526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'size = int(len(df)*0.8)'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"size = int(len(df)*0.8)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "810c8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_df =  df.iloc[:size]\\ntest_df  =  df.iloc[size:]'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_df =  df.iloc[:size]\n",
    "test_df  =  df.iloc[size:]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "99e784b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_df.head()'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_df.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2624e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_df.tail()'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_df.tail()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "87c4f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31527/4106656742.py:1: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  start_dataset =pd.Timestamp(\"2015-01-02\",freq='D')\n",
      "/tmp/ipykernel_31527/4106656742.py:2: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  end_training = pd.Timestamp(\"2018-03-13\",freq='D')\n"
     ]
    }
   ],
   "source": [
    "start_dataset =pd.Timestamp(\"2015-01-02\",freq='D')\n",
    "end_training = pd.Timestamp(\"2018-03-13\",freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3886bb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-02 00:00:00', freq='D')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0954b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_list=df[start_dataset:end_training -pd.Timedelta(1)].values.round(2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6fd4586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_array = np. array(traget_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "17225494",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_array_flatten=traget_array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4d872fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target=traget_array_flatten.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fdb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dc6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b423341",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [{\n",
    "    'start':str(start_dataset),\n",
    "    'target':training_target\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f4e2a1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#practice cell\\ntraining_data = [{\\n    'start':str(start_dataset),\\n    'target':df[start_dataset:end_training -pd.Timedelta(1)].values.round(2).flatten()\\n}]\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#practice cell\n",
    "training_data = [{\n",
    "    'start':str(start_dataset),\n",
    "    'target':df[start_dataset:end_training -pd.Timedelta(1)].values.round(2).flatten()\n",
    "}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8d60412c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '2015-01-02 00:00:00',\n",
       "  'target': [468.9,\n",
       "   2203.15,\n",
       "   119.89,\n",
       "   0.0,\n",
       "   5188.52,\n",
       "   601.02,\n",
       "   228.33,\n",
       "   469.44,\n",
       "   4.71,\n",
       "   4637.82,\n",
       "   5624.39,\n",
       "   3553.8,\n",
       "   61.96,\n",
       "   149.95,\n",
       "   299.96,\n",
       "   0.0,\n",
       "   64.86,\n",
       "   378.59,\n",
       "   2673.87,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   40.08,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1097.25,\n",
       "   426.67,\n",
       "   3.93,\n",
       "   0.0,\n",
       "   240.5,\n",
       "   290.67,\n",
       "   0.0,\n",
       "   211.65,\n",
       "   47.76,\n",
       "   1458.56,\n",
       "   506.12,\n",
       "   738.6,\n",
       "   79.56,\n",
       "   886.28,\n",
       "   3254.15,\n",
       "   598.14,\n",
       "   2126.37,\n",
       "   1771.81,\n",
       "   0.0,\n",
       "   576.73,\n",
       "   21.36,\n",
       "   9.04,\n",
       "   54.21,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   95.59,\n",
       "   8.85,\n",
       "   19.44,\n",
       "   11.36,\n",
       "   55.67,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   19.46,\n",
       "   0.0,\n",
       "   16.45,\n",
       "   97.11,\n",
       "   1345.89,\n",
       "   0.0,\n",
       "   72.63,\n",
       "   531.16,\n",
       "   0.0,\n",
       "   477.99,\n",
       "   22.08,\n",
       "   862.68,\n",
       "   2784.16,\n",
       "   505.88,\n",
       "   0.0,\n",
       "   2108.55,\n",
       "   370.78,\n",
       "   471.92,\n",
       "   3960.36,\n",
       "   28106.72,\n",
       "   590.76,\n",
       "   0.0,\n",
       "   4109.82,\n",
       "   464.09,\n",
       "   945.06,\n",
       "   65.38,\n",
       "   459.15,\n",
       "   145.13,\n",
       "   0.0,\n",
       "   1493.22,\n",
       "   890.84,\n",
       "   1170.32,\n",
       "   1959.55,\n",
       "   288.06,\n",
       "   134.38,\n",
       "   370.45,\n",
       "   475.84,\n",
       "   187.43,\n",
       "   456.27,\n",
       "   247.41,\n",
       "   2743.15,\n",
       "   0.0,\n",
       "   728.75,\n",
       "   6693.75,\n",
       "   129.98,\n",
       "   638.58,\n",
       "   0.0,\n",
       "   294.72,\n",
       "   39.07,\n",
       "   0.0,\n",
       "   1020.53,\n",
       "   205.47,\n",
       "   1250.45,\n",
       "   845.36,\n",
       "   257.75,\n",
       "   643.98,\n",
       "   0.0,\n",
       "   2379.99,\n",
       "   282.57,\n",
       "   0.0,\n",
       "   2578.48,\n",
       "   768.84,\n",
       "   1048.74,\n",
       "   19.54,\n",
       "   0.0,\n",
       "   705.56,\n",
       "   5179.74,\n",
       "   260.98,\n",
       "   0.0,\n",
       "   1444.1,\n",
       "   2408.72,\n",
       "   296.3,\n",
       "   191.9,\n",
       "   1694.56,\n",
       "   2963.64,\n",
       "   898.38,\n",
       "   310.88,\n",
       "   0.0,\n",
       "   289.28,\n",
       "   91.68,\n",
       "   944.96,\n",
       "   91.62,\n",
       "   320.1,\n",
       "   4264.33,\n",
       "   180.93,\n",
       "   323.76,\n",
       "   116.28,\n",
       "   119.54,\n",
       "   1325.72,\n",
       "   2157.39,\n",
       "   221.97,\n",
       "   0.0,\n",
       "   1924.91,\n",
       "   773.7,\n",
       "   4407.1,\n",
       "   330.51,\n",
       "   0.0,\n",
       "   1650.69,\n",
       "   254.46,\n",
       "   1374.0,\n",
       "   1038.47,\n",
       "   262.89,\n",
       "   100.36,\n",
       "   494.71,\n",
       "   0.0,\n",
       "   1970.98,\n",
       "   14.52,\n",
       "   212.94,\n",
       "   942.97,\n",
       "   763.79,\n",
       "   3356.49,\n",
       "   139.8,\n",
       "   0.0,\n",
       "   1828.95,\n",
       "   4107.44,\n",
       "   1975.5,\n",
       "   109.5,\n",
       "   4.27,\n",
       "   792.76,\n",
       "   0.0,\n",
       "   616.14,\n",
       "   1645.79,\n",
       "   46.68,\n",
       "   739.62,\n",
       "   87.16,\n",
       "   180.32,\n",
       "   1561.06,\n",
       "   1021.17,\n",
       "   1091.35,\n",
       "   281.4,\n",
       "   241.19,\n",
       "   0.0,\n",
       "   3523.88,\n",
       "   237.36,\n",
       "   1302.45,\n",
       "   548.4,\n",
       "   351.22,\n",
       "   292.59,\n",
       "   9.51,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   273.04,\n",
       "   461.33,\n",
       "   2285.79,\n",
       "   1961.48,\n",
       "   1348.34,\n",
       "   2613.18,\n",
       "   0.0,\n",
       "   8341.29,\n",
       "   5039.99,\n",
       "   580.06,\n",
       "   193.15,\n",
       "   0.0,\n",
       "   1367.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   14.56,\n",
       "   0.0,\n",
       "   1958.75,\n",
       "   1799.97,\n",
       "   2501.26,\n",
       "   763.07,\n",
       "   3955.63,\n",
       "   14228.43,\n",
       "   147.39,\n",
       "   333.58,\n",
       "   1095.12,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   337.5,\n",
       "   853.09,\n",
       "   500.34,\n",
       "   0.0,\n",
       "   1439.11,\n",
       "   1838.72,\n",
       "   0.0,\n",
       "   23.1,\n",
       "   565.22,\n",
       "   59.74,\n",
       "   1451.65,\n",
       "   366.81,\n",
       "   2070.13,\n",
       "   0.0,\n",
       "   832.32,\n",
       "   121.24,\n",
       "   92.52,\n",
       "   40.54,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   394.48,\n",
       "   5463.01,\n",
       "   265.52,\n",
       "   3605.68,\n",
       "   4043.59,\n",
       "   249.36,\n",
       "   1574.09,\n",
       "   1522.65,\n",
       "   1275.41,\n",
       "   5023.72,\n",
       "   133.44,\n",
       "   33.55,\n",
       "   820.52,\n",
       "   0.0,\n",
       "   9338.64,\n",
       "   5335.97,\n",
       "   2679.65,\n",
       "   1240.27,\n",
       "   10662.34,\n",
       "   211.96,\n",
       "   354.89,\n",
       "   1326.79,\n",
       "   773.44,\n",
       "   490.29,\n",
       "   8109.07,\n",
       "   987.53,\n",
       "   54.83,\n",
       "   0.0,\n",
       "   741.96,\n",
       "   0.0,\n",
       "   2343.8,\n",
       "   491.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1855.23,\n",
       "   2049.39,\n",
       "   2002.56,\n",
       "   573.34,\n",
       "   2212.18,\n",
       "   4525.48,\n",
       "   206.72,\n",
       "   194.32,\n",
       "   136.07,\n",
       "   3134.92,\n",
       "   641.1,\n",
       "   1501.1,\n",
       "   1356.6,\n",
       "   223.81,\n",
       "   0.0,\n",
       "   56.49,\n",
       "   103.88,\n",
       "   447.88,\n",
       "   22.32,\n",
       "   1510.58,\n",
       "   2813.81,\n",
       "   0.0,\n",
       "   5362.03,\n",
       "   9.94,\n",
       "   2043.4,\n",
       "   918.31,\n",
       "   1915.24,\n",
       "   1642.8,\n",
       "   0.0,\n",
       "   2022.36,\n",
       "   451.62,\n",
       "   127.95,\n",
       "   1381.16,\n",
       "   7836.98,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1034.73,\n",
       "   1780.82,\n",
       "   1355.93,\n",
       "   11544.27,\n",
       "   3406.86,\n",
       "   7226.09,\n",
       "   907.56,\n",
       "   1249.62,\n",
       "   1103.3,\n",
       "   1363.28,\n",
       "   5572.86,\n",
       "   4415.7,\n",
       "   1705.99,\n",
       "   203.66,\n",
       "   2280.83,\n",
       "   649.04,\n",
       "   219.15,\n",
       "   0.0,\n",
       "   129.57,\n",
       "   0.0,\n",
       "   1872.44,\n",
       "   734.85,\n",
       "   0.0,\n",
       "   1960.01,\n",
       "   1545.73,\n",
       "   4904.66,\n",
       "   58.53,\n",
       "   901.65,\n",
       "   4938.4,\n",
       "   293.15,\n",
       "   7641.66,\n",
       "   1264.47,\n",
       "   4065.6,\n",
       "   45.53,\n",
       "   0.0,\n",
       "   3436.76,\n",
       "   6962.44,\n",
       "   1933.44,\n",
       "   798.97,\n",
       "   942.11,\n",
       "   1950.2,\n",
       "   0.0,\n",
       "   2041.41,\n",
       "   2966.39,\n",
       "   2682.33,\n",
       "   2072.1,\n",
       "   2172.65,\n",
       "   5253.27,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1642.17,\n",
       "   0.0,\n",
       "   450.94,\n",
       "   834.2,\n",
       "   0.0,\n",
       "   1011.7,\n",
       "   2367.56,\n",
       "   1158.41,\n",
       "   3106.11,\n",
       "   7130.59,\n",
       "   622.28,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   350.38,\n",
       "   0.0,\n",
       "   301.74,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   43.66,\n",
       "   13.12,\n",
       "   0.0,\n",
       "   182.72,\n",
       "   3573.25,\n",
       "   4297.64,\n",
       "   0.0,\n",
       "   2161.64,\n",
       "   99.26,\n",
       "   1932.1,\n",
       "   0.0,\n",
       "   899.57,\n",
       "   299.22,\n",
       "   167.23,\n",
       "   0.0,\n",
       "   1631.9,\n",
       "   1290.48,\n",
       "   0.0,\n",
       "   1269.27,\n",
       "   5528.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1079.96,\n",
       "   1602.38,\n",
       "   160.43,\n",
       "   0.0,\n",
       "   105.84,\n",
       "   0.0,\n",
       "   316.78,\n",
       "   2591.1,\n",
       "   79.36,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   25.87,\n",
       "   0.0,\n",
       "   1159.27,\n",
       "   551.26,\n",
       "   0.0,\n",
       "   1768.22,\n",
       "   492.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2198.76,\n",
       "   0.0,\n",
       "   823.98,\n",
       "   0.0,\n",
       "   1874.9,\n",
       "   275.22,\n",
       "   2800.85,\n",
       "   1972.64,\n",
       "   1384.86,\n",
       "   707.9,\n",
       "   16.78,\n",
       "   7230.21,\n",
       "   48.84,\n",
       "   0.0,\n",
       "   5203.89,\n",
       "   102.52,\n",
       "   1270.38,\n",
       "   2481.42,\n",
       "   633.3,\n",
       "   412.95,\n",
       "   0.0,\n",
       "   131.16,\n",
       "   83.7,\n",
       "   243.34,\n",
       "   1559.08,\n",
       "   571.22,\n",
       "   200.72,\n",
       "   264.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1484.6,\n",
       "   1121.36,\n",
       "   1192.31,\n",
       "   1138.29,\n",
       "   0.0,\n",
       "   1071.28,\n",
       "   505.58,\n",
       "   0.0,\n",
       "   5185.81,\n",
       "   3192.68,\n",
       "   35.21,\n",
       "   0.0,\n",
       "   4161.97,\n",
       "   2745.11,\n",
       "   2006.96,\n",
       "   329.96,\n",
       "   1194.73,\n",
       "   893.09,\n",
       "   152.92,\n",
       "   0.0,\n",
       "   25.99,\n",
       "   920.95,\n",
       "   2090.11,\n",
       "   2719.59,\n",
       "   4044.36,\n",
       "   7.97,\n",
       "   1571.7,\n",
       "   295.27,\n",
       "   0.0,\n",
       "   1969.27,\n",
       "   2974.06,\n",
       "   0.0,\n",
       "   1533.2,\n",
       "   121.65,\n",
       "   72.65,\n",
       "   934.1,\n",
       "   851.09,\n",
       "   834.87,\n",
       "   897.32,\n",
       "   238.38,\n",
       "   3141.2,\n",
       "   69.91,\n",
       "   255.97,\n",
       "   47.37,\n",
       "   10.86,\n",
       "   0.0,\n",
       "   169.54,\n",
       "   72.33,\n",
       "   3862.1,\n",
       "   1304.96,\n",
       "   610.31,\n",
       "   1739.42,\n",
       "   982.56,\n",
       "   0.0,\n",
       "   794.15,\n",
       "   1119.79,\n",
       "   681.1,\n",
       "   5178.14,\n",
       "   31.54,\n",
       "   2432.54,\n",
       "   1555.24,\n",
       "   140.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1379.81,\n",
       "   494.87,\n",
       "   919.67,\n",
       "   0.0,\n",
       "   11.48,\n",
       "   6061.5,\n",
       "   1128.9,\n",
       "   51.07,\n",
       "   734.14,\n",
       "   3443.9,\n",
       "   0.0,\n",
       "   1883.09,\n",
       "   446.35,\n",
       "   829.54,\n",
       "   107.98,\n",
       "   2018.95,\n",
       "   836.96,\n",
       "   0.0,\n",
       "   857.08,\n",
       "   1592.2,\n",
       "   0.0,\n",
       "   1126.16,\n",
       "   432.93,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   324.11,\n",
       "   807.54,\n",
       "   907.44,\n",
       "   305.62,\n",
       "   59.77,\n",
       "   0.0,\n",
       "   968.21,\n",
       "   2922.67,\n",
       "   0.0,\n",
       "   3073.07,\n",
       "   1338.3,\n",
       "   2033.26,\n",
       "   860.92,\n",
       "   0.0,\n",
       "   3029.39,\n",
       "   448.08,\n",
       "   525.72,\n",
       "   2.02,\n",
       "   2693.17,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   208.27,\n",
       "   525.95,\n",
       "   2395.02,\n",
       "   958.86,\n",
       "   29.97,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   443.8,\n",
       "   3712.16,\n",
       "   0.0,\n",
       "   129.09,\n",
       "   2563.3,\n",
       "   0.0,\n",
       "   3981.97,\n",
       "   145.5,\n",
       "   21.12,\n",
       "   760.11,\n",
       "   89.64,\n",
       "   222.12,\n",
       "   12197.0,\n",
       "   2672.73,\n",
       "   586.9,\n",
       "   0.0,\n",
       "   497.59,\n",
       "   3378.93,\n",
       "   381.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2827.52,\n",
       "   66.63,\n",
       "   1202.28,\n",
       "   7194.78,\n",
       "   178.53,\n",
       "   0.0,\n",
       "   639.97,\n",
       "   6491.29,\n",
       "   246.5,\n",
       "   0.0,\n",
       "   2601.8,\n",
       "   364.07,\n",
       "   785.55,\n",
       "   478.01,\n",
       "   757.99,\n",
       "   48.81,\n",
       "   797.12,\n",
       "   1587.11,\n",
       "   4878.39,\n",
       "   0.0,\n",
       "   2133.54,\n",
       "   4620.29,\n",
       "   34.02,\n",
       "   2109.42,\n",
       "   3262.42,\n",
       "   2535.85,\n",
       "   31.12,\n",
       "   11525.01,\n",
       "   3924.32,\n",
       "   500.38,\n",
       "   1836.05,\n",
       "   6982.06,\n",
       "   1783.5,\n",
       "   0.0,\n",
       "   4180.61,\n",
       "   3033.76,\n",
       "   3722.27,\n",
       "   2382.96,\n",
       "   1058.36,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1018.1,\n",
       "   77.24,\n",
       "   90.93,\n",
       "   169.54,\n",
       "   154.44,\n",
       "   0.0,\n",
       "   43.29,\n",
       "   851.47,\n",
       "   1415.73,\n",
       "   1855.01,\n",
       "   3536.9,\n",
       "   3080.38,\n",
       "   371.66,\n",
       "   0.0,\n",
       "   3219.55,\n",
       "   824.97,\n",
       "   77.88,\n",
       "   378.49,\n",
       "   2135.09,\n",
       "   720.09,\n",
       "   0.0,\n",
       "   15.13,\n",
       "   2262.3,\n",
       "   831.14,\n",
       "   1425.41,\n",
       "   1395.33,\n",
       "   0.0,\n",
       "   104.51,\n",
       "   244.24,\n",
       "   2122.04,\n",
       "   3211.01,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1328.35,\n",
       "   191.97,\n",
       "   3373.56,\n",
       "   1941.95,\n",
       "   196.11,\n",
       "   1962.13,\n",
       "   1011.9,\n",
       "   542.18,\n",
       "   1590.89,\n",
       "   2867.91,\n",
       "   2512.71,\n",
       "   702.17,\n",
       "   4072.33,\n",
       "   3885.99,\n",
       "   0.0,\n",
       "   1040.78,\n",
       "   2861.15,\n",
       "   3613.57,\n",
       "   614.5,\n",
       "   3450.19,\n",
       "   1218.76,\n",
       "   13.12,\n",
       "   1484.08,\n",
       "   3109.78,\n",
       "   1864.48,\n",
       "   2760.17,\n",
       "   1798.4,\n",
       "   747.5,\n",
       "   0.0,\n",
       "   51.02,\n",
       "   0.0,\n",
       "   1060.35,\n",
       "   174.48,\n",
       "   2052.57,\n",
       "   0.0,\n",
       "   707.82,\n",
       "   2390.76,\n",
       "   1127.99,\n",
       "   1448.85,\n",
       "   1023.39,\n",
       "   488.74,\n",
       "   5494.8,\n",
       "   4.98,\n",
       "   1918.49,\n",
       "   935.29,\n",
       "   6120.66,\n",
       "   1107.46,\n",
       "   3257.76,\n",
       "   473.31,\n",
       "   194.32,\n",
       "   7484.57,\n",
       "   4204.97,\n",
       "   561.49,\n",
       "   5148.45,\n",
       "   79.2,\n",
       "   0.0,\n",
       "   3251.34,\n",
       "   1381.34,\n",
       "   0.0,\n",
       "   161.97,\n",
       "   6285.81,\n",
       "   2161.48,\n",
       "   517.87,\n",
       "   0.0,\n",
       "   1724.38,\n",
       "   1124.65,\n",
       "   5766.15,\n",
       "   781.41,\n",
       "   453.37,\n",
       "   7365.46,\n",
       "   0.0,\n",
       "   405.34,\n",
       "   701.92,\n",
       "   102.22,\n",
       "   1040.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1167.06,\n",
       "   917.63,\n",
       "   1732.3,\n",
       "   31.36,\n",
       "   858.71,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   39.68,\n",
       "   0.0,\n",
       "   3107.32,\n",
       "   1601.55,\n",
       "   405.92,\n",
       "   8996.78,\n",
       "   0.0,\n",
       "   1454.9,\n",
       "   767.32,\n",
       "   157.84,\n",
       "   772.79,\n",
       "   482.58,\n",
       "   6792.19,\n",
       "   18452.97,\n",
       "   0.0,\n",
       "   4684.62,\n",
       "   146.82,\n",
       "   837.92,\n",
       "   407.07,\n",
       "   1135.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   392.66,\n",
       "   16.5,\n",
       "   1524.13,\n",
       "   983.17,\n",
       "   57.58,\n",
       "   0.0,\n",
       "   46.72,\n",
       "   0.0,\n",
       "   243.89,\n",
       "   147.58,\n",
       "   2095.83,\n",
       "   866.4,\n",
       "   7264.42,\n",
       "   441.2,\n",
       "   1099.01,\n",
       "   0.0,\n",
       "   847.7,\n",
       "   248.82,\n",
       "   2365.6,\n",
       "   1680.7,\n",
       "   3802.8,\n",
       "   5269.82,\n",
       "   8866.88,\n",
       "   669.37,\n",
       "   1805.99,\n",
       "   0.0,\n",
       "   2360.33,\n",
       "   1061.68,\n",
       "   138.68,\n",
       "   928.31,\n",
       "   645.71,\n",
       "   226.47,\n",
       "   0.0,\n",
       "   745.77,\n",
       "   1346.97,\n",
       "   682.25,\n",
       "   92.7,\n",
       "   125.3,\n",
       "   2659.34,\n",
       "   11.34,\n",
       "   640.76,\n",
       "   1069.22,\n",
       "   197.87,\n",
       "   315.45,\n",
       "   970.19,\n",
       "   0.0,\n",
       "   2316.52,\n",
       "   1651.3,\n",
       "   497.91,\n",
       "   1707.81,\n",
       "   5912.78,\n",
       "   3888.82,\n",
       "   944.29,\n",
       "   6.12,\n",
       "   1466.57,\n",
       "   2201.86,\n",
       "   9335.09,\n",
       "   421.08,\n",
       "   2514.54,\n",
       "   1414.13,\n",
       "   0.0,\n",
       "   271.62,\n",
       "   1765.46,\n",
       "   720.37,\n",
       "   1607.1,\n",
       "   108.42,\n",
       "   434.65,\n",
       "   0.0,\n",
       "   486.67,\n",
       "   0.0,\n",
       "   142.21,\n",
       "   255.9,\n",
       "   2994.48,\n",
       "   162.34,\n",
       "   1046.33,\n",
       "   2701.28,\n",
       "   659.82,\n",
       "   0.0,\n",
       "   197.05,\n",
       "   5226.35,\n",
       "   0.0,\n",
       "   4266.9,\n",
       "   5138.93,\n",
       "   0.0,\n",
       "   455.93,\n",
       "   648.91,\n",
       "   880.46,\n",
       "   346.92,\n",
       "   122.22,\n",
       "   1582.01,\n",
       "   2471.91,\n",
       "   2718.21,\n",
       "   1799.2,\n",
       "   10560.98,\n",
       "   277.76,\n",
       "   1311.97,\n",
       "   1975.36,\n",
       "   5305.08,\n",
       "   2137.32,\n",
       "   1063.18,\n",
       "   8090.58,\n",
       "   438.86,\n",
       "   0.0,\n",
       "   132.22,\n",
       "   811.3,\n",
       "   1294.75,\n",
       "   236.96,\n",
       "   5477.35,\n",
       "   0.0,\n",
       "   320.39,\n",
       "   1313.54,\n",
       "   830.4,\n",
       "   773.26,\n",
       "   2859.97,\n",
       "   430.44,\n",
       "   3457.1,\n",
       "   173.49,\n",
       "   147.27,\n",
       "   3000.92,\n",
       "   541.94,\n",
       "   17.12,\n",
       "   1229.55,\n",
       "   577.11,\n",
       "   0.0,\n",
       "   2475.09,\n",
       "   5310.95,\n",
       "   893.27,\n",
       "   1919.6,\n",
       "   778.71,\n",
       "   877.16,\n",
       "   191.88,\n",
       "   1352.95,\n",
       "   83.58,\n",
       "   762.14,\n",
       "   21.07,\n",
       "   1900.3,\n",
       "   4835.98,\n",
       "   51.56,\n",
       "   3882.23,\n",
       "   179.97,\n",
       "   0.0,\n",
       "   973.71,\n",
       "   3810.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2149.96,\n",
       "   380.2,\n",
       "   910.17,\n",
       "   2184.88,\n",
       "   2891.86,\n",
       "   1536.45,\n",
       "   89.95,\n",
       "   991.09,\n",
       "   1368.05,\n",
       "   654.59,\n",
       "   326.96,\n",
       "   4979.31,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2033.37,\n",
       "   819.15,\n",
       "   1770.78,\n",
       "   2174.16,\n",
       "   1859.16,\n",
       "   1369.48,\n",
       "   2190.81,\n",
       "   2906.4,\n",
       "   1340.24,\n",
       "   0.0,\n",
       "   2616.51,\n",
       "   1115.2,\n",
       "   4493.98,\n",
       "   561.54,\n",
       "   993.9,\n",
       "   4709.1,\n",
       "   731.52,\n",
       "   923.69,\n",
       "   1680.27,\n",
       "   204.54,\n",
       "   15.71,\n",
       "   496.11,\n",
       "   448.79,\n",
       "   14.78,\n",
       "   4225.12,\n",
       "   405.35,\n",
       "   4187.35,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   4874.3,\n",
       "   767.91,\n",
       "   322.02,\n",
       "   632.19,\n",
       "   1656.87,\n",
       "   23.97,\n",
       "   743.05,\n",
       "   121.83,\n",
       "   635.6,\n",
       "   1115.2,\n",
       "   1283.51,\n",
       "   1688.47,\n",
       "   573.29,\n",
       "   661.35,\n",
       "   84.99,\n",
       "   546.92,\n",
       "   479.97,\n",
       "   1855.48,\n",
       "   2374.73,\n",
       "   1137.34,\n",
       "   2101.18,\n",
       "   425.89,\n",
       "   2043.13,\n",
       "   4751.08,\n",
       "   3839.31,\n",
       "   1232.0,\n",
       "   0.0,\n",
       "   418.06,\n",
       "   1322.14,\n",
       "   2286.05,\n",
       "   2384.0,\n",
       "   2698.17,\n",
       "   1992.99,\n",
       "   ...]}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09671f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b5d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cc070f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_list=df[start_dataset:end_training + pd.Timedelta(prediction_length)].values.round(2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f1558878",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_array = np. array(traget_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f1907223",
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_array_flatten=traget_array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f36c8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_target=traget_array_flatten.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f37bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6d5a297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b3ea95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [{\n",
    "    'start':str(start_dataset),\n",
    "    'target':testing_target\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "502a3e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#practice cell\\ntest_data = [{\\n    'start':str(start_dataset),\\n    'target':df[start_dataset:end_training + pd.Timedelta(prediction_length)].values.round(2).flatten()\\n    \\n}]\\n\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#practice cell\n",
    "test_data = [{\n",
    "    'start':str(start_dataset),\n",
    "    'target':df[start_dataset:end_training + pd.Timedelta(prediction_length)].values.round(2).flatten()\n",
    "    \n",
    "}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b759e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8ff1c6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b= test_data.ravel'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''b= test_data.ravel'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ec568430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '2015-01-02 00:00:00',\n",
       "  'target': [468.9,\n",
       "   2203.15,\n",
       "   119.89,\n",
       "   0.0,\n",
       "   5188.52,\n",
       "   601.02,\n",
       "   228.33,\n",
       "   469.44,\n",
       "   4.71,\n",
       "   4637.82,\n",
       "   5624.39,\n",
       "   3553.8,\n",
       "   61.96,\n",
       "   149.95,\n",
       "   299.96,\n",
       "   0.0,\n",
       "   64.86,\n",
       "   378.59,\n",
       "   2673.87,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   40.08,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1097.25,\n",
       "   426.67,\n",
       "   3.93,\n",
       "   0.0,\n",
       "   240.5,\n",
       "   290.67,\n",
       "   0.0,\n",
       "   211.65,\n",
       "   47.76,\n",
       "   1458.56,\n",
       "   506.12,\n",
       "   738.6,\n",
       "   79.56,\n",
       "   886.28,\n",
       "   3254.15,\n",
       "   598.14,\n",
       "   2126.37,\n",
       "   1771.81,\n",
       "   0.0,\n",
       "   576.73,\n",
       "   21.36,\n",
       "   9.04,\n",
       "   54.21,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   95.59,\n",
       "   8.85,\n",
       "   19.44,\n",
       "   11.36,\n",
       "   55.67,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   19.46,\n",
       "   0.0,\n",
       "   16.45,\n",
       "   97.11,\n",
       "   1345.89,\n",
       "   0.0,\n",
       "   72.63,\n",
       "   531.16,\n",
       "   0.0,\n",
       "   477.99,\n",
       "   22.08,\n",
       "   862.68,\n",
       "   2784.16,\n",
       "   505.88,\n",
       "   0.0,\n",
       "   2108.55,\n",
       "   370.78,\n",
       "   471.92,\n",
       "   3960.36,\n",
       "   28106.72,\n",
       "   590.76,\n",
       "   0.0,\n",
       "   4109.82,\n",
       "   464.09,\n",
       "   945.06,\n",
       "   65.38,\n",
       "   459.15,\n",
       "   145.13,\n",
       "   0.0,\n",
       "   1493.22,\n",
       "   890.84,\n",
       "   1170.32,\n",
       "   1959.55,\n",
       "   288.06,\n",
       "   134.38,\n",
       "   370.45,\n",
       "   475.84,\n",
       "   187.43,\n",
       "   456.27,\n",
       "   247.41,\n",
       "   2743.15,\n",
       "   0.0,\n",
       "   728.75,\n",
       "   6693.75,\n",
       "   129.98,\n",
       "   638.58,\n",
       "   0.0,\n",
       "   294.72,\n",
       "   39.07,\n",
       "   0.0,\n",
       "   1020.53,\n",
       "   205.47,\n",
       "   1250.45,\n",
       "   845.36,\n",
       "   257.75,\n",
       "   643.98,\n",
       "   0.0,\n",
       "   2379.99,\n",
       "   282.57,\n",
       "   0.0,\n",
       "   2578.48,\n",
       "   768.84,\n",
       "   1048.74,\n",
       "   19.54,\n",
       "   0.0,\n",
       "   705.56,\n",
       "   5179.74,\n",
       "   260.98,\n",
       "   0.0,\n",
       "   1444.1,\n",
       "   2408.72,\n",
       "   296.3,\n",
       "   191.9,\n",
       "   1694.56,\n",
       "   2963.64,\n",
       "   898.38,\n",
       "   310.88,\n",
       "   0.0,\n",
       "   289.28,\n",
       "   91.68,\n",
       "   944.96,\n",
       "   91.62,\n",
       "   320.1,\n",
       "   4264.33,\n",
       "   180.93,\n",
       "   323.76,\n",
       "   116.28,\n",
       "   119.54,\n",
       "   1325.72,\n",
       "   2157.39,\n",
       "   221.97,\n",
       "   0.0,\n",
       "   1924.91,\n",
       "   773.7,\n",
       "   4407.1,\n",
       "   330.51,\n",
       "   0.0,\n",
       "   1650.69,\n",
       "   254.46,\n",
       "   1374.0,\n",
       "   1038.47,\n",
       "   262.89,\n",
       "   100.36,\n",
       "   494.71,\n",
       "   0.0,\n",
       "   1970.98,\n",
       "   14.52,\n",
       "   212.94,\n",
       "   942.97,\n",
       "   763.79,\n",
       "   3356.49,\n",
       "   139.8,\n",
       "   0.0,\n",
       "   1828.95,\n",
       "   4107.44,\n",
       "   1975.5,\n",
       "   109.5,\n",
       "   4.27,\n",
       "   792.76,\n",
       "   0.0,\n",
       "   616.14,\n",
       "   1645.79,\n",
       "   46.68,\n",
       "   739.62,\n",
       "   87.16,\n",
       "   180.32,\n",
       "   1561.06,\n",
       "   1021.17,\n",
       "   1091.35,\n",
       "   281.4,\n",
       "   241.19,\n",
       "   0.0,\n",
       "   3523.88,\n",
       "   237.36,\n",
       "   1302.45,\n",
       "   548.4,\n",
       "   351.22,\n",
       "   292.59,\n",
       "   9.51,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   273.04,\n",
       "   461.33,\n",
       "   2285.79,\n",
       "   1961.48,\n",
       "   1348.34,\n",
       "   2613.18,\n",
       "   0.0,\n",
       "   8341.29,\n",
       "   5039.99,\n",
       "   580.06,\n",
       "   193.15,\n",
       "   0.0,\n",
       "   1367.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   14.56,\n",
       "   0.0,\n",
       "   1958.75,\n",
       "   1799.97,\n",
       "   2501.26,\n",
       "   763.07,\n",
       "   3955.63,\n",
       "   14228.43,\n",
       "   147.39,\n",
       "   333.58,\n",
       "   1095.12,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   337.5,\n",
       "   853.09,\n",
       "   500.34,\n",
       "   0.0,\n",
       "   1439.11,\n",
       "   1838.72,\n",
       "   0.0,\n",
       "   23.1,\n",
       "   565.22,\n",
       "   59.74,\n",
       "   1451.65,\n",
       "   366.81,\n",
       "   2070.13,\n",
       "   0.0,\n",
       "   832.32,\n",
       "   121.24,\n",
       "   92.52,\n",
       "   40.54,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   394.48,\n",
       "   5463.01,\n",
       "   265.52,\n",
       "   3605.68,\n",
       "   4043.59,\n",
       "   249.36,\n",
       "   1574.09,\n",
       "   1522.65,\n",
       "   1275.41,\n",
       "   5023.72,\n",
       "   133.44,\n",
       "   33.55,\n",
       "   820.52,\n",
       "   0.0,\n",
       "   9338.64,\n",
       "   5335.97,\n",
       "   2679.65,\n",
       "   1240.27,\n",
       "   10662.34,\n",
       "   211.96,\n",
       "   354.89,\n",
       "   1326.79,\n",
       "   773.44,\n",
       "   490.29,\n",
       "   8109.07,\n",
       "   987.53,\n",
       "   54.83,\n",
       "   0.0,\n",
       "   741.96,\n",
       "   0.0,\n",
       "   2343.8,\n",
       "   491.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1855.23,\n",
       "   2049.39,\n",
       "   2002.56,\n",
       "   573.34,\n",
       "   2212.18,\n",
       "   4525.48,\n",
       "   206.72,\n",
       "   194.32,\n",
       "   136.07,\n",
       "   3134.92,\n",
       "   641.1,\n",
       "   1501.1,\n",
       "   1356.6,\n",
       "   223.81,\n",
       "   0.0,\n",
       "   56.49,\n",
       "   103.88,\n",
       "   447.88,\n",
       "   22.32,\n",
       "   1510.58,\n",
       "   2813.81,\n",
       "   0.0,\n",
       "   5362.03,\n",
       "   9.94,\n",
       "   2043.4,\n",
       "   918.31,\n",
       "   1915.24,\n",
       "   1642.8,\n",
       "   0.0,\n",
       "   2022.36,\n",
       "   451.62,\n",
       "   127.95,\n",
       "   1381.16,\n",
       "   7836.98,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1034.73,\n",
       "   1780.82,\n",
       "   1355.93,\n",
       "   11544.27,\n",
       "   3406.86,\n",
       "   7226.09,\n",
       "   907.56,\n",
       "   1249.62,\n",
       "   1103.3,\n",
       "   1363.28,\n",
       "   5572.86,\n",
       "   4415.7,\n",
       "   1705.99,\n",
       "   203.66,\n",
       "   2280.83,\n",
       "   649.04,\n",
       "   219.15,\n",
       "   0.0,\n",
       "   129.57,\n",
       "   0.0,\n",
       "   1872.44,\n",
       "   734.85,\n",
       "   0.0,\n",
       "   1960.01,\n",
       "   1545.73,\n",
       "   4904.66,\n",
       "   58.53,\n",
       "   901.65,\n",
       "   4938.4,\n",
       "   293.15,\n",
       "   7641.66,\n",
       "   1264.47,\n",
       "   4065.6,\n",
       "   45.53,\n",
       "   0.0,\n",
       "   3436.76,\n",
       "   6962.44,\n",
       "   1933.44,\n",
       "   798.97,\n",
       "   942.11,\n",
       "   1950.2,\n",
       "   0.0,\n",
       "   2041.41,\n",
       "   2966.39,\n",
       "   2682.33,\n",
       "   2072.1,\n",
       "   2172.65,\n",
       "   5253.27,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1642.17,\n",
       "   0.0,\n",
       "   450.94,\n",
       "   834.2,\n",
       "   0.0,\n",
       "   1011.7,\n",
       "   2367.56,\n",
       "   1158.41,\n",
       "   3106.11,\n",
       "   7130.59,\n",
       "   622.28,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   350.38,\n",
       "   0.0,\n",
       "   301.74,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   43.66,\n",
       "   13.12,\n",
       "   0.0,\n",
       "   182.72,\n",
       "   3573.25,\n",
       "   4297.64,\n",
       "   0.0,\n",
       "   2161.64,\n",
       "   99.26,\n",
       "   1932.1,\n",
       "   0.0,\n",
       "   899.57,\n",
       "   299.22,\n",
       "   167.23,\n",
       "   0.0,\n",
       "   1631.9,\n",
       "   1290.48,\n",
       "   0.0,\n",
       "   1269.27,\n",
       "   5528.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1079.96,\n",
       "   1602.38,\n",
       "   160.43,\n",
       "   0.0,\n",
       "   105.84,\n",
       "   0.0,\n",
       "   316.78,\n",
       "   2591.1,\n",
       "   79.36,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   25.87,\n",
       "   0.0,\n",
       "   1159.27,\n",
       "   551.26,\n",
       "   0.0,\n",
       "   1768.22,\n",
       "   492.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2198.76,\n",
       "   0.0,\n",
       "   823.98,\n",
       "   0.0,\n",
       "   1874.9,\n",
       "   275.22,\n",
       "   2800.85,\n",
       "   1972.64,\n",
       "   1384.86,\n",
       "   707.9,\n",
       "   16.78,\n",
       "   7230.21,\n",
       "   48.84,\n",
       "   0.0,\n",
       "   5203.89,\n",
       "   102.52,\n",
       "   1270.38,\n",
       "   2481.42,\n",
       "   633.3,\n",
       "   412.95,\n",
       "   0.0,\n",
       "   131.16,\n",
       "   83.7,\n",
       "   243.34,\n",
       "   1559.08,\n",
       "   571.22,\n",
       "   200.72,\n",
       "   264.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1484.6,\n",
       "   1121.36,\n",
       "   1192.31,\n",
       "   1138.29,\n",
       "   0.0,\n",
       "   1071.28,\n",
       "   505.58,\n",
       "   0.0,\n",
       "   5185.81,\n",
       "   3192.68,\n",
       "   35.21,\n",
       "   0.0,\n",
       "   4161.97,\n",
       "   2745.11,\n",
       "   2006.96,\n",
       "   329.96,\n",
       "   1194.73,\n",
       "   893.09,\n",
       "   152.92,\n",
       "   0.0,\n",
       "   25.99,\n",
       "   920.95,\n",
       "   2090.11,\n",
       "   2719.59,\n",
       "   4044.36,\n",
       "   7.97,\n",
       "   1571.7,\n",
       "   295.27,\n",
       "   0.0,\n",
       "   1969.27,\n",
       "   2974.06,\n",
       "   0.0,\n",
       "   1533.2,\n",
       "   121.65,\n",
       "   72.65,\n",
       "   934.1,\n",
       "   851.09,\n",
       "   834.87,\n",
       "   897.32,\n",
       "   238.38,\n",
       "   3141.2,\n",
       "   69.91,\n",
       "   255.97,\n",
       "   47.37,\n",
       "   10.86,\n",
       "   0.0,\n",
       "   169.54,\n",
       "   72.33,\n",
       "   3862.1,\n",
       "   1304.96,\n",
       "   610.31,\n",
       "   1739.42,\n",
       "   982.56,\n",
       "   0.0,\n",
       "   794.15,\n",
       "   1119.79,\n",
       "   681.1,\n",
       "   5178.14,\n",
       "   31.54,\n",
       "   2432.54,\n",
       "   1555.24,\n",
       "   140.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1379.81,\n",
       "   494.87,\n",
       "   919.67,\n",
       "   0.0,\n",
       "   11.48,\n",
       "   6061.5,\n",
       "   1128.9,\n",
       "   51.07,\n",
       "   734.14,\n",
       "   3443.9,\n",
       "   0.0,\n",
       "   1883.09,\n",
       "   446.35,\n",
       "   829.54,\n",
       "   107.98,\n",
       "   2018.95,\n",
       "   836.96,\n",
       "   0.0,\n",
       "   857.08,\n",
       "   1592.2,\n",
       "   0.0,\n",
       "   1126.16,\n",
       "   432.93,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   324.11,\n",
       "   807.54,\n",
       "   907.44,\n",
       "   305.62,\n",
       "   59.77,\n",
       "   0.0,\n",
       "   968.21,\n",
       "   2922.67,\n",
       "   0.0,\n",
       "   3073.07,\n",
       "   1338.3,\n",
       "   2033.26,\n",
       "   860.92,\n",
       "   0.0,\n",
       "   3029.39,\n",
       "   448.08,\n",
       "   525.72,\n",
       "   2.02,\n",
       "   2693.17,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   208.27,\n",
       "   525.95,\n",
       "   2395.02,\n",
       "   958.86,\n",
       "   29.97,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   443.8,\n",
       "   3712.16,\n",
       "   0.0,\n",
       "   129.09,\n",
       "   2563.3,\n",
       "   0.0,\n",
       "   3981.97,\n",
       "   145.5,\n",
       "   21.12,\n",
       "   760.11,\n",
       "   89.64,\n",
       "   222.12,\n",
       "   12197.0,\n",
       "   2672.73,\n",
       "   586.9,\n",
       "   0.0,\n",
       "   497.59,\n",
       "   3378.93,\n",
       "   381.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2827.52,\n",
       "   66.63,\n",
       "   1202.28,\n",
       "   7194.78,\n",
       "   178.53,\n",
       "   0.0,\n",
       "   639.97,\n",
       "   6491.29,\n",
       "   246.5,\n",
       "   0.0,\n",
       "   2601.8,\n",
       "   364.07,\n",
       "   785.55,\n",
       "   478.01,\n",
       "   757.99,\n",
       "   48.81,\n",
       "   797.12,\n",
       "   1587.11,\n",
       "   4878.39,\n",
       "   0.0,\n",
       "   2133.54,\n",
       "   4620.29,\n",
       "   34.02,\n",
       "   2109.42,\n",
       "   3262.42,\n",
       "   2535.85,\n",
       "   31.12,\n",
       "   11525.01,\n",
       "   3924.32,\n",
       "   500.38,\n",
       "   1836.05,\n",
       "   6982.06,\n",
       "   1783.5,\n",
       "   0.0,\n",
       "   4180.61,\n",
       "   3033.76,\n",
       "   3722.27,\n",
       "   2382.96,\n",
       "   1058.36,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1018.1,\n",
       "   77.24,\n",
       "   90.93,\n",
       "   169.54,\n",
       "   154.44,\n",
       "   0.0,\n",
       "   43.29,\n",
       "   851.47,\n",
       "   1415.73,\n",
       "   1855.01,\n",
       "   3536.9,\n",
       "   3080.38,\n",
       "   371.66,\n",
       "   0.0,\n",
       "   3219.55,\n",
       "   824.97,\n",
       "   77.88,\n",
       "   378.49,\n",
       "   2135.09,\n",
       "   720.09,\n",
       "   0.0,\n",
       "   15.13,\n",
       "   2262.3,\n",
       "   831.14,\n",
       "   1425.41,\n",
       "   1395.33,\n",
       "   0.0,\n",
       "   104.51,\n",
       "   244.24,\n",
       "   2122.04,\n",
       "   3211.01,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1328.35,\n",
       "   191.97,\n",
       "   3373.56,\n",
       "   1941.95,\n",
       "   196.11,\n",
       "   1962.13,\n",
       "   1011.9,\n",
       "   542.18,\n",
       "   1590.89,\n",
       "   2867.91,\n",
       "   2512.71,\n",
       "   702.17,\n",
       "   4072.33,\n",
       "   3885.99,\n",
       "   0.0,\n",
       "   1040.78,\n",
       "   2861.15,\n",
       "   3613.57,\n",
       "   614.5,\n",
       "   3450.19,\n",
       "   1218.76,\n",
       "   13.12,\n",
       "   1484.08,\n",
       "   3109.78,\n",
       "   1864.48,\n",
       "   2760.17,\n",
       "   1798.4,\n",
       "   747.5,\n",
       "   0.0,\n",
       "   51.02,\n",
       "   0.0,\n",
       "   1060.35,\n",
       "   174.48,\n",
       "   2052.57,\n",
       "   0.0,\n",
       "   707.82,\n",
       "   2390.76,\n",
       "   1127.99,\n",
       "   1448.85,\n",
       "   1023.39,\n",
       "   488.74,\n",
       "   5494.8,\n",
       "   4.98,\n",
       "   1918.49,\n",
       "   935.29,\n",
       "   6120.66,\n",
       "   1107.46,\n",
       "   3257.76,\n",
       "   473.31,\n",
       "   194.32,\n",
       "   7484.57,\n",
       "   4204.97,\n",
       "   561.49,\n",
       "   5148.45,\n",
       "   79.2,\n",
       "   0.0,\n",
       "   3251.34,\n",
       "   1381.34,\n",
       "   0.0,\n",
       "   161.97,\n",
       "   6285.81,\n",
       "   2161.48,\n",
       "   517.87,\n",
       "   0.0,\n",
       "   1724.38,\n",
       "   1124.65,\n",
       "   5766.15,\n",
       "   781.41,\n",
       "   453.37,\n",
       "   7365.46,\n",
       "   0.0,\n",
       "   405.34,\n",
       "   701.92,\n",
       "   102.22,\n",
       "   1040.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1167.06,\n",
       "   917.63,\n",
       "   1732.3,\n",
       "   31.36,\n",
       "   858.71,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   39.68,\n",
       "   0.0,\n",
       "   3107.32,\n",
       "   1601.55,\n",
       "   405.92,\n",
       "   8996.78,\n",
       "   0.0,\n",
       "   1454.9,\n",
       "   767.32,\n",
       "   157.84,\n",
       "   772.79,\n",
       "   482.58,\n",
       "   6792.19,\n",
       "   18452.97,\n",
       "   0.0,\n",
       "   4684.62,\n",
       "   146.82,\n",
       "   837.92,\n",
       "   407.07,\n",
       "   1135.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   392.66,\n",
       "   16.5,\n",
       "   1524.13,\n",
       "   983.17,\n",
       "   57.58,\n",
       "   0.0,\n",
       "   46.72,\n",
       "   0.0,\n",
       "   243.89,\n",
       "   147.58,\n",
       "   2095.83,\n",
       "   866.4,\n",
       "   7264.42,\n",
       "   441.2,\n",
       "   1099.01,\n",
       "   0.0,\n",
       "   847.7,\n",
       "   248.82,\n",
       "   2365.6,\n",
       "   1680.7,\n",
       "   3802.8,\n",
       "   5269.82,\n",
       "   8866.88,\n",
       "   669.37,\n",
       "   1805.99,\n",
       "   0.0,\n",
       "   2360.33,\n",
       "   1061.68,\n",
       "   138.68,\n",
       "   928.31,\n",
       "   645.71,\n",
       "   226.47,\n",
       "   0.0,\n",
       "   745.77,\n",
       "   1346.97,\n",
       "   682.25,\n",
       "   92.7,\n",
       "   125.3,\n",
       "   2659.34,\n",
       "   11.34,\n",
       "   640.76,\n",
       "   1069.22,\n",
       "   197.87,\n",
       "   315.45,\n",
       "   970.19,\n",
       "   0.0,\n",
       "   2316.52,\n",
       "   1651.3,\n",
       "   497.91,\n",
       "   1707.81,\n",
       "   5912.78,\n",
       "   3888.82,\n",
       "   944.29,\n",
       "   6.12,\n",
       "   1466.57,\n",
       "   2201.86,\n",
       "   9335.09,\n",
       "   421.08,\n",
       "   2514.54,\n",
       "   1414.13,\n",
       "   0.0,\n",
       "   271.62,\n",
       "   1765.46,\n",
       "   720.37,\n",
       "   1607.1,\n",
       "   108.42,\n",
       "   434.65,\n",
       "   0.0,\n",
       "   486.67,\n",
       "   0.0,\n",
       "   142.21,\n",
       "   255.9,\n",
       "   2994.48,\n",
       "   162.34,\n",
       "   1046.33,\n",
       "   2701.28,\n",
       "   659.82,\n",
       "   0.0,\n",
       "   197.05,\n",
       "   5226.35,\n",
       "   0.0,\n",
       "   4266.9,\n",
       "   5138.93,\n",
       "   0.0,\n",
       "   455.93,\n",
       "   648.91,\n",
       "   880.46,\n",
       "   346.92,\n",
       "   122.22,\n",
       "   1582.01,\n",
       "   2471.91,\n",
       "   2718.21,\n",
       "   1799.2,\n",
       "   10560.98,\n",
       "   277.76,\n",
       "   1311.97,\n",
       "   1975.36,\n",
       "   5305.08,\n",
       "   2137.32,\n",
       "   1063.18,\n",
       "   8090.58,\n",
       "   438.86,\n",
       "   0.0,\n",
       "   132.22,\n",
       "   811.3,\n",
       "   1294.75,\n",
       "   236.96,\n",
       "   5477.35,\n",
       "   0.0,\n",
       "   320.39,\n",
       "   1313.54,\n",
       "   830.4,\n",
       "   773.26,\n",
       "   2859.97,\n",
       "   430.44,\n",
       "   3457.1,\n",
       "   173.49,\n",
       "   147.27,\n",
       "   3000.92,\n",
       "   541.94,\n",
       "   17.12,\n",
       "   1229.55,\n",
       "   577.11,\n",
       "   0.0,\n",
       "   2475.09,\n",
       "   5310.95,\n",
       "   893.27,\n",
       "   1919.6,\n",
       "   778.71,\n",
       "   877.16,\n",
       "   191.88,\n",
       "   1352.95,\n",
       "   83.58,\n",
       "   762.14,\n",
       "   21.07,\n",
       "   1900.3,\n",
       "   4835.98,\n",
       "   51.56,\n",
       "   3882.23,\n",
       "   179.97,\n",
       "   0.0,\n",
       "   973.71,\n",
       "   3810.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2149.96,\n",
       "   380.2,\n",
       "   910.17,\n",
       "   2184.88,\n",
       "   2891.86,\n",
       "   1536.45,\n",
       "   89.95,\n",
       "   991.09,\n",
       "   1368.05,\n",
       "   654.59,\n",
       "   326.96,\n",
       "   4979.31,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2033.37,\n",
       "   819.15,\n",
       "   1770.78,\n",
       "   2174.16,\n",
       "   1859.16,\n",
       "   1369.48,\n",
       "   2190.81,\n",
       "   2906.4,\n",
       "   1340.24,\n",
       "   0.0,\n",
       "   2616.51,\n",
       "   1115.2,\n",
       "   4493.98,\n",
       "   561.54,\n",
       "   993.9,\n",
       "   4709.1,\n",
       "   731.52,\n",
       "   923.69,\n",
       "   1680.27,\n",
       "   204.54,\n",
       "   15.71,\n",
       "   496.11,\n",
       "   448.79,\n",
       "   14.78,\n",
       "   4225.12,\n",
       "   405.35,\n",
       "   4187.35,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   4874.3,\n",
       "   767.91,\n",
       "   322.02,\n",
       "   632.19,\n",
       "   1656.87,\n",
       "   23.97,\n",
       "   743.05,\n",
       "   121.83,\n",
       "   635.6,\n",
       "   1115.2,\n",
       "   1283.51,\n",
       "   1688.47,\n",
       "   573.29,\n",
       "   661.35,\n",
       "   84.99,\n",
       "   546.92,\n",
       "   479.97,\n",
       "   1855.48,\n",
       "   2374.73,\n",
       "   1137.34,\n",
       "   2101.18,\n",
       "   425.89,\n",
       "   2043.13,\n",
       "   4751.08,\n",
       "   3839.31,\n",
       "   1232.0,\n",
       "   0.0,\n",
       "   418.06,\n",
       "   1322.14,\n",
       "   2286.05,\n",
       "   2384.0,\n",
       "   2698.17,\n",
       "   1992.99,\n",
       "   ...]}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "64504a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '2015-01-02 00:00:00',\n",
       "  'target': [468.9,\n",
       "   2203.15,\n",
       "   119.89,\n",
       "   0.0,\n",
       "   5188.52,\n",
       "   601.02,\n",
       "   228.33,\n",
       "   469.44,\n",
       "   4.71,\n",
       "   4637.82,\n",
       "   5624.39,\n",
       "   3553.8,\n",
       "   61.96,\n",
       "   149.95,\n",
       "   299.96,\n",
       "   0.0,\n",
       "   64.86,\n",
       "   378.59,\n",
       "   2673.87,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   40.08,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1097.25,\n",
       "   426.67,\n",
       "   3.93,\n",
       "   0.0,\n",
       "   240.5,\n",
       "   290.67,\n",
       "   0.0,\n",
       "   211.65,\n",
       "   47.76,\n",
       "   1458.56,\n",
       "   506.12,\n",
       "   738.6,\n",
       "   79.56,\n",
       "   886.28,\n",
       "   3254.15,\n",
       "   598.14,\n",
       "   2126.37,\n",
       "   1771.81,\n",
       "   0.0,\n",
       "   576.73,\n",
       "   21.36,\n",
       "   9.04,\n",
       "   54.21,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   95.59,\n",
       "   8.85,\n",
       "   19.44,\n",
       "   11.36,\n",
       "   55.67,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   19.46,\n",
       "   0.0,\n",
       "   16.45,\n",
       "   97.11,\n",
       "   1345.89,\n",
       "   0.0,\n",
       "   72.63,\n",
       "   531.16,\n",
       "   0.0,\n",
       "   477.99,\n",
       "   22.08,\n",
       "   862.68,\n",
       "   2784.16,\n",
       "   505.88,\n",
       "   0.0,\n",
       "   2108.55,\n",
       "   370.78,\n",
       "   471.92,\n",
       "   3960.36,\n",
       "   28106.72,\n",
       "   590.76,\n",
       "   0.0,\n",
       "   4109.82,\n",
       "   464.09,\n",
       "   945.06,\n",
       "   65.38,\n",
       "   459.15,\n",
       "   145.13,\n",
       "   0.0,\n",
       "   1493.22,\n",
       "   890.84,\n",
       "   1170.32,\n",
       "   1959.55,\n",
       "   288.06,\n",
       "   134.38,\n",
       "   370.45,\n",
       "   475.84,\n",
       "   187.43,\n",
       "   456.27,\n",
       "   247.41,\n",
       "   2743.15,\n",
       "   0.0,\n",
       "   728.75,\n",
       "   6693.75,\n",
       "   129.98,\n",
       "   638.58,\n",
       "   0.0,\n",
       "   294.72,\n",
       "   39.07,\n",
       "   0.0,\n",
       "   1020.53,\n",
       "   205.47,\n",
       "   1250.45,\n",
       "   845.36,\n",
       "   257.75,\n",
       "   643.98,\n",
       "   0.0,\n",
       "   2379.99,\n",
       "   282.57,\n",
       "   0.0,\n",
       "   2578.48,\n",
       "   768.84,\n",
       "   1048.74,\n",
       "   19.54,\n",
       "   0.0,\n",
       "   705.56,\n",
       "   5179.74,\n",
       "   260.98,\n",
       "   0.0,\n",
       "   1444.1,\n",
       "   2408.72,\n",
       "   296.3,\n",
       "   191.9,\n",
       "   1694.56,\n",
       "   2963.64,\n",
       "   898.38,\n",
       "   310.88,\n",
       "   0.0,\n",
       "   289.28,\n",
       "   91.68,\n",
       "   944.96,\n",
       "   91.62,\n",
       "   320.1,\n",
       "   4264.33,\n",
       "   180.93,\n",
       "   323.76,\n",
       "   116.28,\n",
       "   119.54,\n",
       "   1325.72,\n",
       "   2157.39,\n",
       "   221.97,\n",
       "   0.0,\n",
       "   1924.91,\n",
       "   773.7,\n",
       "   4407.1,\n",
       "   330.51,\n",
       "   0.0,\n",
       "   1650.69,\n",
       "   254.46,\n",
       "   1374.0,\n",
       "   1038.47,\n",
       "   262.89,\n",
       "   100.36,\n",
       "   494.71,\n",
       "   0.0,\n",
       "   1970.98,\n",
       "   14.52,\n",
       "   212.94,\n",
       "   942.97,\n",
       "   763.79,\n",
       "   3356.49,\n",
       "   139.8,\n",
       "   0.0,\n",
       "   1828.95,\n",
       "   4107.44,\n",
       "   1975.5,\n",
       "   109.5,\n",
       "   4.27,\n",
       "   792.76,\n",
       "   0.0,\n",
       "   616.14,\n",
       "   1645.79,\n",
       "   46.68,\n",
       "   739.62,\n",
       "   87.16,\n",
       "   180.32,\n",
       "   1561.06,\n",
       "   1021.17,\n",
       "   1091.35,\n",
       "   281.4,\n",
       "   241.19,\n",
       "   0.0,\n",
       "   3523.88,\n",
       "   237.36,\n",
       "   1302.45,\n",
       "   548.4,\n",
       "   351.22,\n",
       "   292.59,\n",
       "   9.51,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   273.04,\n",
       "   461.33,\n",
       "   2285.79,\n",
       "   1961.48,\n",
       "   1348.34,\n",
       "   2613.18,\n",
       "   0.0,\n",
       "   8341.29,\n",
       "   5039.99,\n",
       "   580.06,\n",
       "   193.15,\n",
       "   0.0,\n",
       "   1367.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   14.56,\n",
       "   0.0,\n",
       "   1958.75,\n",
       "   1799.97,\n",
       "   2501.26,\n",
       "   763.07,\n",
       "   3955.63,\n",
       "   14228.43,\n",
       "   147.39,\n",
       "   333.58,\n",
       "   1095.12,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   337.5,\n",
       "   853.09,\n",
       "   500.34,\n",
       "   0.0,\n",
       "   1439.11,\n",
       "   1838.72,\n",
       "   0.0,\n",
       "   23.1,\n",
       "   565.22,\n",
       "   59.74,\n",
       "   1451.65,\n",
       "   366.81,\n",
       "   2070.13,\n",
       "   0.0,\n",
       "   832.32,\n",
       "   121.24,\n",
       "   92.52,\n",
       "   40.54,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   394.48,\n",
       "   5463.01,\n",
       "   265.52,\n",
       "   3605.68,\n",
       "   4043.59,\n",
       "   249.36,\n",
       "   1574.09,\n",
       "   1522.65,\n",
       "   1275.41,\n",
       "   5023.72,\n",
       "   133.44,\n",
       "   33.55,\n",
       "   820.52,\n",
       "   0.0,\n",
       "   9338.64,\n",
       "   5335.97,\n",
       "   2679.65,\n",
       "   1240.27,\n",
       "   10662.34,\n",
       "   211.96,\n",
       "   354.89,\n",
       "   1326.79,\n",
       "   773.44,\n",
       "   490.29,\n",
       "   8109.07,\n",
       "   987.53,\n",
       "   54.83,\n",
       "   0.0,\n",
       "   741.96,\n",
       "   0.0,\n",
       "   2343.8,\n",
       "   491.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1855.23,\n",
       "   2049.39,\n",
       "   2002.56,\n",
       "   573.34,\n",
       "   2212.18,\n",
       "   4525.48,\n",
       "   206.72,\n",
       "   194.32,\n",
       "   136.07,\n",
       "   3134.92,\n",
       "   641.1,\n",
       "   1501.1,\n",
       "   1356.6,\n",
       "   223.81,\n",
       "   0.0,\n",
       "   56.49,\n",
       "   103.88,\n",
       "   447.88,\n",
       "   22.32,\n",
       "   1510.58,\n",
       "   2813.81,\n",
       "   0.0,\n",
       "   5362.03,\n",
       "   9.94,\n",
       "   2043.4,\n",
       "   918.31,\n",
       "   1915.24,\n",
       "   1642.8,\n",
       "   0.0,\n",
       "   2022.36,\n",
       "   451.62,\n",
       "   127.95,\n",
       "   1381.16,\n",
       "   7836.98,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1034.73,\n",
       "   1780.82,\n",
       "   1355.93,\n",
       "   11544.27,\n",
       "   3406.86,\n",
       "   7226.09,\n",
       "   907.56,\n",
       "   1249.62,\n",
       "   1103.3,\n",
       "   1363.28,\n",
       "   5572.86,\n",
       "   4415.7,\n",
       "   1705.99,\n",
       "   203.66,\n",
       "   2280.83,\n",
       "   649.04,\n",
       "   219.15,\n",
       "   0.0,\n",
       "   129.57,\n",
       "   0.0,\n",
       "   1872.44,\n",
       "   734.85,\n",
       "   0.0,\n",
       "   1960.01,\n",
       "   1545.73,\n",
       "   4904.66,\n",
       "   58.53,\n",
       "   901.65,\n",
       "   4938.4,\n",
       "   293.15,\n",
       "   7641.66,\n",
       "   1264.47,\n",
       "   4065.6,\n",
       "   45.53,\n",
       "   0.0,\n",
       "   3436.76,\n",
       "   6962.44,\n",
       "   1933.44,\n",
       "   798.97,\n",
       "   942.11,\n",
       "   1950.2,\n",
       "   0.0,\n",
       "   2041.41,\n",
       "   2966.39,\n",
       "   2682.33,\n",
       "   2072.1,\n",
       "   2172.65,\n",
       "   5253.27,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1642.17,\n",
       "   0.0,\n",
       "   450.94,\n",
       "   834.2,\n",
       "   0.0,\n",
       "   1011.7,\n",
       "   2367.56,\n",
       "   1158.41,\n",
       "   3106.11,\n",
       "   7130.59,\n",
       "   622.28,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   350.38,\n",
       "   0.0,\n",
       "   301.74,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   43.66,\n",
       "   13.12,\n",
       "   0.0,\n",
       "   182.72,\n",
       "   3573.25,\n",
       "   4297.64,\n",
       "   0.0,\n",
       "   2161.64,\n",
       "   99.26,\n",
       "   1932.1,\n",
       "   0.0,\n",
       "   899.57,\n",
       "   299.22,\n",
       "   167.23,\n",
       "   0.0,\n",
       "   1631.9,\n",
       "   1290.48,\n",
       "   0.0,\n",
       "   1269.27,\n",
       "   5528.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1079.96,\n",
       "   1602.38,\n",
       "   160.43,\n",
       "   0.0,\n",
       "   105.84,\n",
       "   0.0,\n",
       "   316.78,\n",
       "   2591.1,\n",
       "   79.36,\n",
       "   37.78,\n",
       "   0.0,\n",
       "   25.87,\n",
       "   0.0,\n",
       "   1159.27,\n",
       "   551.26,\n",
       "   0.0,\n",
       "   1768.22,\n",
       "   492.84,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2198.76,\n",
       "   0.0,\n",
       "   823.98,\n",
       "   0.0,\n",
       "   1874.9,\n",
       "   275.22,\n",
       "   2800.85,\n",
       "   1972.64,\n",
       "   1384.86,\n",
       "   707.9,\n",
       "   16.78,\n",
       "   7230.21,\n",
       "   48.84,\n",
       "   0.0,\n",
       "   5203.89,\n",
       "   102.52,\n",
       "   1270.38,\n",
       "   2481.42,\n",
       "   633.3,\n",
       "   412.95,\n",
       "   0.0,\n",
       "   131.16,\n",
       "   83.7,\n",
       "   243.34,\n",
       "   1559.08,\n",
       "   571.22,\n",
       "   200.72,\n",
       "   264.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1484.6,\n",
       "   1121.36,\n",
       "   1192.31,\n",
       "   1138.29,\n",
       "   0.0,\n",
       "   1071.28,\n",
       "   505.58,\n",
       "   0.0,\n",
       "   5185.81,\n",
       "   3192.68,\n",
       "   35.21,\n",
       "   0.0,\n",
       "   4161.97,\n",
       "   2745.11,\n",
       "   2006.96,\n",
       "   329.96,\n",
       "   1194.73,\n",
       "   893.09,\n",
       "   152.92,\n",
       "   0.0,\n",
       "   25.99,\n",
       "   920.95,\n",
       "   2090.11,\n",
       "   2719.59,\n",
       "   4044.36,\n",
       "   7.97,\n",
       "   1571.7,\n",
       "   295.27,\n",
       "   0.0,\n",
       "   1969.27,\n",
       "   2974.06,\n",
       "   0.0,\n",
       "   1533.2,\n",
       "   121.65,\n",
       "   72.65,\n",
       "   934.1,\n",
       "   851.09,\n",
       "   834.87,\n",
       "   897.32,\n",
       "   238.38,\n",
       "   3141.2,\n",
       "   69.91,\n",
       "   255.97,\n",
       "   47.37,\n",
       "   10.86,\n",
       "   0.0,\n",
       "   169.54,\n",
       "   72.33,\n",
       "   3862.1,\n",
       "   1304.96,\n",
       "   610.31,\n",
       "   1739.42,\n",
       "   982.56,\n",
       "   0.0,\n",
       "   794.15,\n",
       "   1119.79,\n",
       "   681.1,\n",
       "   5178.14,\n",
       "   31.54,\n",
       "   2432.54,\n",
       "   1555.24,\n",
       "   140.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1379.81,\n",
       "   494.87,\n",
       "   919.67,\n",
       "   0.0,\n",
       "   11.48,\n",
       "   6061.5,\n",
       "   1128.9,\n",
       "   51.07,\n",
       "   734.14,\n",
       "   3443.9,\n",
       "   0.0,\n",
       "   1883.09,\n",
       "   446.35,\n",
       "   829.54,\n",
       "   107.98,\n",
       "   2018.95,\n",
       "   836.96,\n",
       "   0.0,\n",
       "   857.08,\n",
       "   1592.2,\n",
       "   0.0,\n",
       "   1126.16,\n",
       "   432.93,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   324.11,\n",
       "   807.54,\n",
       "   907.44,\n",
       "   305.62,\n",
       "   59.77,\n",
       "   0.0,\n",
       "   968.21,\n",
       "   2922.67,\n",
       "   0.0,\n",
       "   3073.07,\n",
       "   1338.3,\n",
       "   2033.26,\n",
       "   860.92,\n",
       "   0.0,\n",
       "   3029.39,\n",
       "   448.08,\n",
       "   525.72,\n",
       "   2.02,\n",
       "   2693.17,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   208.27,\n",
       "   525.95,\n",
       "   2395.02,\n",
       "   958.86,\n",
       "   29.97,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   443.8,\n",
       "   3712.16,\n",
       "   0.0,\n",
       "   129.09,\n",
       "   2563.3,\n",
       "   0.0,\n",
       "   3981.97,\n",
       "   145.5,\n",
       "   21.12,\n",
       "   760.11,\n",
       "   89.64,\n",
       "   222.12,\n",
       "   12197.0,\n",
       "   2672.73,\n",
       "   586.9,\n",
       "   0.0,\n",
       "   497.59,\n",
       "   3378.93,\n",
       "   381.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2827.52,\n",
       "   66.63,\n",
       "   1202.28,\n",
       "   7194.78,\n",
       "   178.53,\n",
       "   0.0,\n",
       "   639.97,\n",
       "   6491.29,\n",
       "   246.5,\n",
       "   0.0,\n",
       "   2601.8,\n",
       "   364.07,\n",
       "   785.55,\n",
       "   478.01,\n",
       "   757.99,\n",
       "   48.81,\n",
       "   797.12,\n",
       "   1587.11,\n",
       "   4878.39,\n",
       "   0.0,\n",
       "   2133.54,\n",
       "   4620.29,\n",
       "   34.02,\n",
       "   2109.42,\n",
       "   3262.42,\n",
       "   2535.85,\n",
       "   31.12,\n",
       "   11525.01,\n",
       "   3924.32,\n",
       "   500.38,\n",
       "   1836.05,\n",
       "   6982.06,\n",
       "   1783.5,\n",
       "   0.0,\n",
       "   4180.61,\n",
       "   3033.76,\n",
       "   3722.27,\n",
       "   2382.96,\n",
       "   1058.36,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1018.1,\n",
       "   77.24,\n",
       "   90.93,\n",
       "   169.54,\n",
       "   154.44,\n",
       "   0.0,\n",
       "   43.29,\n",
       "   851.47,\n",
       "   1415.73,\n",
       "   1855.01,\n",
       "   3536.9,\n",
       "   3080.38,\n",
       "   371.66,\n",
       "   0.0,\n",
       "   3219.55,\n",
       "   824.97,\n",
       "   77.88,\n",
       "   378.49,\n",
       "   2135.09,\n",
       "   720.09,\n",
       "   0.0,\n",
       "   15.13,\n",
       "   2262.3,\n",
       "   831.14,\n",
       "   1425.41,\n",
       "   1395.33,\n",
       "   0.0,\n",
       "   104.51,\n",
       "   244.24,\n",
       "   2122.04,\n",
       "   3211.01,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1328.35,\n",
       "   191.97,\n",
       "   3373.56,\n",
       "   1941.95,\n",
       "   196.11,\n",
       "   1962.13,\n",
       "   1011.9,\n",
       "   542.18,\n",
       "   1590.89,\n",
       "   2867.91,\n",
       "   2512.71,\n",
       "   702.17,\n",
       "   4072.33,\n",
       "   3885.99,\n",
       "   0.0,\n",
       "   1040.78,\n",
       "   2861.15,\n",
       "   3613.57,\n",
       "   614.5,\n",
       "   3450.19,\n",
       "   1218.76,\n",
       "   13.12,\n",
       "   1484.08,\n",
       "   3109.78,\n",
       "   1864.48,\n",
       "   2760.17,\n",
       "   1798.4,\n",
       "   747.5,\n",
       "   0.0,\n",
       "   51.02,\n",
       "   0.0,\n",
       "   1060.35,\n",
       "   174.48,\n",
       "   2052.57,\n",
       "   0.0,\n",
       "   707.82,\n",
       "   2390.76,\n",
       "   1127.99,\n",
       "   1448.85,\n",
       "   1023.39,\n",
       "   488.74,\n",
       "   5494.8,\n",
       "   4.98,\n",
       "   1918.49,\n",
       "   935.29,\n",
       "   6120.66,\n",
       "   1107.46,\n",
       "   3257.76,\n",
       "   473.31,\n",
       "   194.32,\n",
       "   7484.57,\n",
       "   4204.97,\n",
       "   561.49,\n",
       "   5148.45,\n",
       "   79.2,\n",
       "   0.0,\n",
       "   3251.34,\n",
       "   1381.34,\n",
       "   0.0,\n",
       "   161.97,\n",
       "   6285.81,\n",
       "   2161.48,\n",
       "   517.87,\n",
       "   0.0,\n",
       "   1724.38,\n",
       "   1124.65,\n",
       "   5766.15,\n",
       "   781.41,\n",
       "   453.37,\n",
       "   7365.46,\n",
       "   0.0,\n",
       "   405.34,\n",
       "   701.92,\n",
       "   102.22,\n",
       "   1040.55,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1167.06,\n",
       "   917.63,\n",
       "   1732.3,\n",
       "   31.36,\n",
       "   858.71,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   39.68,\n",
       "   0.0,\n",
       "   3107.32,\n",
       "   1601.55,\n",
       "   405.92,\n",
       "   8996.78,\n",
       "   0.0,\n",
       "   1454.9,\n",
       "   767.32,\n",
       "   157.84,\n",
       "   772.79,\n",
       "   482.58,\n",
       "   6792.19,\n",
       "   18452.97,\n",
       "   0.0,\n",
       "   4684.62,\n",
       "   146.82,\n",
       "   837.92,\n",
       "   407.07,\n",
       "   1135.64,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   392.66,\n",
       "   16.5,\n",
       "   1524.13,\n",
       "   983.17,\n",
       "   57.58,\n",
       "   0.0,\n",
       "   46.72,\n",
       "   0.0,\n",
       "   243.89,\n",
       "   147.58,\n",
       "   2095.83,\n",
       "   866.4,\n",
       "   7264.42,\n",
       "   441.2,\n",
       "   1099.01,\n",
       "   0.0,\n",
       "   847.7,\n",
       "   248.82,\n",
       "   2365.6,\n",
       "   1680.7,\n",
       "   3802.8,\n",
       "   5269.82,\n",
       "   8866.88,\n",
       "   669.37,\n",
       "   1805.99,\n",
       "   0.0,\n",
       "   2360.33,\n",
       "   1061.68,\n",
       "   138.68,\n",
       "   928.31,\n",
       "   645.71,\n",
       "   226.47,\n",
       "   0.0,\n",
       "   745.77,\n",
       "   1346.97,\n",
       "   682.25,\n",
       "   92.7,\n",
       "   125.3,\n",
       "   2659.34,\n",
       "   11.34,\n",
       "   640.76,\n",
       "   1069.22,\n",
       "   197.87,\n",
       "   315.45,\n",
       "   970.19,\n",
       "   0.0,\n",
       "   2316.52,\n",
       "   1651.3,\n",
       "   497.91,\n",
       "   1707.81,\n",
       "   5912.78,\n",
       "   3888.82,\n",
       "   944.29,\n",
       "   6.12,\n",
       "   1466.57,\n",
       "   2201.86,\n",
       "   9335.09,\n",
       "   421.08,\n",
       "   2514.54,\n",
       "   1414.13,\n",
       "   0.0,\n",
       "   271.62,\n",
       "   1765.46,\n",
       "   720.37,\n",
       "   1607.1,\n",
       "   108.42,\n",
       "   434.65,\n",
       "   0.0,\n",
       "   486.67,\n",
       "   0.0,\n",
       "   142.21,\n",
       "   255.9,\n",
       "   2994.48,\n",
       "   162.34,\n",
       "   1046.33,\n",
       "   2701.28,\n",
       "   659.82,\n",
       "   0.0,\n",
       "   197.05,\n",
       "   5226.35,\n",
       "   0.0,\n",
       "   4266.9,\n",
       "   5138.93,\n",
       "   0.0,\n",
       "   455.93,\n",
       "   648.91,\n",
       "   880.46,\n",
       "   346.92,\n",
       "   122.22,\n",
       "   1582.01,\n",
       "   2471.91,\n",
       "   2718.21,\n",
       "   1799.2,\n",
       "   10560.98,\n",
       "   277.76,\n",
       "   1311.97,\n",
       "   1975.36,\n",
       "   5305.08,\n",
       "   2137.32,\n",
       "   1063.18,\n",
       "   8090.58,\n",
       "   438.86,\n",
       "   0.0,\n",
       "   132.22,\n",
       "   811.3,\n",
       "   1294.75,\n",
       "   236.96,\n",
       "   5477.35,\n",
       "   0.0,\n",
       "   320.39,\n",
       "   1313.54,\n",
       "   830.4,\n",
       "   773.26,\n",
       "   2859.97,\n",
       "   430.44,\n",
       "   3457.1,\n",
       "   173.49,\n",
       "   147.27,\n",
       "   3000.92,\n",
       "   541.94,\n",
       "   17.12,\n",
       "   1229.55,\n",
       "   577.11,\n",
       "   0.0,\n",
       "   2475.09,\n",
       "   5310.95,\n",
       "   893.27,\n",
       "   1919.6,\n",
       "   778.71,\n",
       "   877.16,\n",
       "   191.88,\n",
       "   1352.95,\n",
       "   83.58,\n",
       "   762.14,\n",
       "   21.07,\n",
       "   1900.3,\n",
       "   4835.98,\n",
       "   51.56,\n",
       "   3882.23,\n",
       "   179.97,\n",
       "   0.0,\n",
       "   973.71,\n",
       "   3810.46,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2149.96,\n",
       "   380.2,\n",
       "   910.17,\n",
       "   2184.88,\n",
       "   2891.86,\n",
       "   1536.45,\n",
       "   89.95,\n",
       "   991.09,\n",
       "   1368.05,\n",
       "   654.59,\n",
       "   326.96,\n",
       "   4979.31,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2033.37,\n",
       "   819.15,\n",
       "   1770.78,\n",
       "   2174.16,\n",
       "   1859.16,\n",
       "   1369.48,\n",
       "   2190.81,\n",
       "   2906.4,\n",
       "   1340.24,\n",
       "   0.0,\n",
       "   2616.51,\n",
       "   1115.2,\n",
       "   4493.98,\n",
       "   561.54,\n",
       "   993.9,\n",
       "   4709.1,\n",
       "   731.52,\n",
       "   923.69,\n",
       "   1680.27,\n",
       "   204.54,\n",
       "   15.71,\n",
       "   496.11,\n",
       "   448.79,\n",
       "   14.78,\n",
       "   4225.12,\n",
       "   405.35,\n",
       "   4187.35,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   4874.3,\n",
       "   767.91,\n",
       "   322.02,\n",
       "   632.19,\n",
       "   1656.87,\n",
       "   23.97,\n",
       "   743.05,\n",
       "   121.83,\n",
       "   635.6,\n",
       "   1115.2,\n",
       "   1283.51,\n",
       "   1688.47,\n",
       "   573.29,\n",
       "   661.35,\n",
       "   84.99,\n",
       "   546.92,\n",
       "   479.97,\n",
       "   1855.48,\n",
       "   2374.73,\n",
       "   1137.34,\n",
       "   2101.18,\n",
       "   425.89,\n",
       "   2043.13,\n",
       "   4751.08,\n",
       "   3839.31,\n",
       "   1232.0,\n",
       "   0.0,\n",
       "   418.06,\n",
       "   1322.14,\n",
       "   2286.05,\n",
       "   2384.0,\n",
       "   2698.17,\n",
       "   1992.99,\n",
       "   ...]}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a08f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "889fb0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 53 µs, total: 31.3 ms\n",
      "Wall time: 75.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2ed7fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b0c18eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_path = \"https://s3-{}.amazonaws.com/{}\".format(region, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c65cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "299e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path,df):\n",
    "    with open(path,'wb') as fp :\n",
    "        for d in df:\n",
    "            fp.write(json.dumps(d).encode('utf-8'))\n",
    "            fp.write('\\n'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e758f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 ms, sys: 0 ns, total: 2.19 ms\n",
      "Wall time: 1.73 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file('train_daily.json',training_data)\n",
    "write_dicts_to_file('test_daily.json',test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09682fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "42050a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e8103a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-1-101718508260/deepar-salesdata-demo-notebook/data/train/train_daily.json\n",
      "Uploading file to s3://sagemaker-us-east-1-101718508260/deepar-salesdata-demo-notebook/data/test/test_daily.json\n",
      "CPU times: user 33.4 ms, sys: 3.14 ms, total: 36.5 ms\n",
      "Wall time: 241 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train_daily.json\", s3_data_path + \"/train/train_daily.json\")\n",
    "copy_to_s3(\"test_daily.json\", s3_data_path + \"/test/test_daily.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100b2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e74d340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2015-01-02 00:00:00\", \"target\": [468.9, 2203.15, 119.89, 0.0, 5188.52, 601.02, 228.33, 46...\n"
     ]
    }
   ],
   "source": [
    "s3_sample = s3.Object(s3_bucket, s3_prefix + \"/data/train/train_daily.json\").get()[\"Body\"].read()\n",
    "StringVariable = s3_sample.decode(\"UTF-8\", \"ignore\")\n",
    "lines = StringVariable.split(\"\\n\")\n",
    "print(lines[0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35feb82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "eec4a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "41311f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.2xlarge\",\n",
    "    base_job_name=\"deepar-sales-demo\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3b4eefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": 100,\n",
    "    \"prediction_length\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "62a021fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5c3c568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-14 16:17:09 Starting - Starting the training job...\n",
      "2022-09-14 16:17:36 Starting - Preparing the instances for trainingProfilerReport-1663172229: InProgress\n",
      "......\n",
      "2022-09-14 16:18:36 Downloading - Downloading input data...\n",
      "2022-09-14 16:18:56 Training - Downloading the training image........\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:18 INFO 139830709036864] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:18 INFO 139830709036864] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '100', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '100', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:18 INFO 139830709036864] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '100', 'epochs': '400', 'prediction_length': '100', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:18 INFO 139830709036864] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_daily.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train_daily.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Real time series\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] number of time series: 1\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] number of observations: 1166\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] mean target length: 1166.0\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] min/mean/max target: 0.0/1436.5897298456262/28106.720703125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] mean abs(target): 1436.5897298456262\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Test set statistics:\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Real time series\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] number of time series: 1\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] number of observations: 1167\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] mean target length: 1167.0\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] min/mean/max target: 0.0/1439.464438731791/28106.720703125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] mean abs(target): 1439.464438731791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] #memory_usage::<batchbuffer> = 25.01220703125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172419.1190228, \"EndTime\": 1663172419.826944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 705.345869064331, \"count\": 1, \"min\": 705.345869064331, \"max\": 705.345869064331}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:19 INFO 139830709036864] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:20 INFO 139830709036864] #memory_usage::<model> = 64 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172419.8270452, \"EndTime\": 1663172420.7723784, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1653.2166004180908, \"count\": 1, \"min\": 1653.2166004180908, \"max\": 1653.2166004180908}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:21 INFO 139830709036864] Epoch[0] Batch[0] avg_epoch_loss=8.970686\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=8.970685958862305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:23 INFO 139830709036864] Epoch[0] Batch[5] avg_epoch_loss=8.719386\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=8.719386100769043\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:23 INFO 139830709036864] Epoch[0] Batch [5]#011Speed: 212.83 samples/sec#011loss=8.719386\u001b[0m\n",
      "\n",
      "2022-09-14 16:20:35 Training - Training image download completed. Training in progress.\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] Epoch[0] Batch[10] avg_epoch_loss=8.665156\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=8.600080108642578\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] Epoch[0] Batch [10]#011Speed: 206.91 samples/sec#011loss=8.600080\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172420.7724473, \"EndTime\": 1663172424.7038925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 3931.321382522583, \"count\": 1, \"min\": 3931.321382522583, \"max\": 3931.321382522583}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=171.69076906759193 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.665156104347922\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:24 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_58abf0df-a60a-4ab8-a039-186a13f86a71-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172424.7039866, \"EndTime\": 1663172424.8123934, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 107.79166221618652, \"count\": 1, \"min\": 107.79166221618652, \"max\": 107.79166221618652}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:25 INFO 139830709036864] Epoch[1] Batch[0] avg_epoch_loss=8.714516\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=8.714515686035156\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:27 INFO 139830709036864] Epoch[1] Batch[5] avg_epoch_loss=8.563689\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=8.5636887550354\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:27 INFO 139830709036864] Epoch[1] Batch [5]#011Speed: 203.45 samples/sec#011loss=8.563689\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172424.8124697, \"EndTime\": 1663172428.2985911, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3486.056327819824, \"count\": 1, \"min\": 3486.056327819824, \"max\": 3486.056327819824}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.29526691529503 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=1, train loss <loss>=8.586491870880128\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:28 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_e83529be-6ff3-4728-a4da-6dd7c885166d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172428.2986763, \"EndTime\": 1663172428.4021919, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.04737091064453, \"count\": 1, \"min\": 103.04737091064453, \"max\": 103.04737091064453}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:29 INFO 139830709036864] Epoch[2] Batch[0] avg_epoch_loss=8.537210\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=8.537210464477539\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:30 INFO 139830709036864] Epoch[2] Batch[5] avg_epoch_loss=8.473653\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=8.473653475443522\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:30 INFO 139830709036864] Epoch[2] Batch [5]#011Speed: 218.43 samples/sec#011loss=8.473653\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172428.4022794, \"EndTime\": 1663172431.7344267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3332.080602645874, \"count\": 1, \"min\": 3332.080602645874, \"max\": 3332.080602645874}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.9628557975197 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=2, train loss <loss>=8.525542068481446\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:31 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_cb1ca7a5-c5a1-46e2-b64e-51c0c7d92e4c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172431.7345197, \"EndTime\": 1663172431.8042035, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 69.02956962585449, \"count\": 1, \"min\": 69.02956962585449, \"max\": 69.02956962585449}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:32 INFO 139830709036864] Epoch[3] Batch[0] avg_epoch_loss=8.485950\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=8.485949516296387\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:33 INFO 139830709036864] Epoch[3] Batch[5] avg_epoch_loss=8.515172\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=8.515171686808268\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:33 INFO 139830709036864] Epoch[3] Batch [5]#011Speed: 218.68 samples/sec#011loss=8.515172\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] Epoch[3] Batch[10] avg_epoch_loss=8.540319\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=8.57049503326416\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] Epoch[3] Batch [10]#011Speed: 211.18 samples/sec#011loss=8.570495\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172431.8042736, \"EndTime\": 1663172435.4316056, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3627.2783279418945, \"count\": 1, \"min\": 3627.2783279418945, \"max\": 3627.2783279418945}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.94900927911866 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=3, train loss <loss>=8.540318662470037\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:36 INFO 139830709036864] Epoch[4] Batch[0] avg_epoch_loss=8.502934\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=8.502933502197266\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:37 INFO 139830709036864] Epoch[4] Batch[5] avg_epoch_loss=8.465843\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=8.465842564900717\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:37 INFO 139830709036864] Epoch[4] Batch [5]#011Speed: 212.29 samples/sec#011loss=8.465843\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172435.4316828, \"EndTime\": 1663172438.8461428, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3414.0031337738037, \"count\": 1, \"min\": 3414.0031337738037, \"max\": 3414.0031337738037}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.40617984185764 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=4, train loss <loss>=8.497468757629395\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:38 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_4fba52fd-0646-46e6-bc7b-f067269e7490-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172438.8462234, \"EndTime\": 1663172438.9105208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.809871673583984, \"count\": 1, \"min\": 63.809871673583984, \"max\": 63.809871673583984}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:39 INFO 139830709036864] Epoch[5] Batch[0] avg_epoch_loss=8.403014\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=8.403014183044434\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:41 INFO 139830709036864] Epoch[5] Batch[5] avg_epoch_loss=8.466676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=8.466676076253256\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:41 INFO 139830709036864] Epoch[5] Batch [5]#011Speed: 218.76 samples/sec#011loss=8.466676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172438.9105756, \"EndTime\": 1663172442.23387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3323.237657546997, \"count\": 1, \"min\": 3323.237657546997, \"max\": 3323.237657546997}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.26594719484402 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=5, train loss <loss>=8.488778591156006\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_1041e814-93bf-42aa-bf2b-d16387c801bf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172442.233961, \"EndTime\": 1663172442.313353, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.79948616027832, \"count\": 1, \"min\": 78.79948616027832, \"max\": 78.79948616027832}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] Epoch[6] Batch[0] avg_epoch_loss=8.514224\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=8.5142240524292\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:44 INFO 139830709036864] Epoch[6] Batch[5] avg_epoch_loss=8.432606\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=8.432605743408203\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:44 INFO 139830709036864] Epoch[6] Batch [5]#011Speed: 217.60 samples/sec#011loss=8.432606\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] Epoch[6] Batch[10] avg_epoch_loss=8.302888\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=8.147226524353027\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] Epoch[6] Batch [10]#011Speed: 208.63 samples/sec#011loss=8.147227\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172442.3134472, \"EndTime\": 1663172445.9627876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3649.2695808410645, \"count\": 1, \"min\": 3649.2695808410645, \"max\": 3649.2695808410645}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.51096510010805 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=6, train loss <loss>=8.302887916564941\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:45 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:46 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_2d41474e-3d12-432b-ba06-2e088f4e92cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172445.9628615, \"EndTime\": 1663172446.0674033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 104.02774810791016, \"count\": 1, \"min\": 104.02774810791016, \"max\": 104.02774810791016}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:46 INFO 139830709036864] Epoch[7] Batch[0] avg_epoch_loss=8.531143\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=8.531143188476562\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:48 INFO 139830709036864] Epoch[7] Batch[5] avg_epoch_loss=8.446486\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=8.44648551940918\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:48 INFO 139830709036864] Epoch[7] Batch [5]#011Speed: 227.71 samples/sec#011loss=8.446486\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] Epoch[7] Batch[10] avg_epoch_loss=8.447107\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=8.447851753234863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] Epoch[7] Batch [10]#011Speed: 213.82 samples/sec#011loss=8.447852\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172446.0674918, \"EndTime\": 1663172449.5880501, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3520.4906463623047, \"count\": 1, \"min\": 3520.4906463623047, \"max\": 3520.4906463623047}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.2720952920524 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=7, train loss <loss>=8.44710653478449\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:50 INFO 139830709036864] Epoch[8] Batch[0] avg_epoch_loss=8.254811\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=8.25481128692627\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:51 INFO 139830709036864] Epoch[8] Batch[5] avg_epoch_loss=8.392080\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=8.392079830169678\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:51 INFO 139830709036864] Epoch[8] Batch [5]#011Speed: 233.19 samples/sec#011loss=8.392080\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:52 INFO 139830709036864] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172449.5881417, \"EndTime\": 1663172452.7873197, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3198.6634731292725, \"count\": 1, \"min\": 3198.6634731292725, \"max\": 3198.6634731292725}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:52 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.69940260884408 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:52 INFO 139830709036864] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=8, train loss <loss>=8.431179618835449\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:52 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:53 INFO 139830709036864] Epoch[9] Batch[0] avg_epoch_loss=8.459938\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=8.459938049316406\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:54 INFO 139830709036864] Epoch[9] Batch[5] avg_epoch_loss=8.400832\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=8.400831699371338\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:54 INFO 139830709036864] Epoch[9] Batch [5]#011Speed: 226.07 samples/sec#011loss=8.400832\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] Epoch[9] Batch[10] avg_epoch_loss=8.409850\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=8.420672988891601\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] Epoch[9] Batch [10]#011Speed: 222.28 samples/sec#011loss=8.420673\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172452.7873998, \"EndTime\": 1663172456.2694323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3481.5218448638916, \"count\": 1, \"min\": 3481.5218448638916, \"max\": 3481.5218448638916}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.8311953537043 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=9, train loss <loss>=8.409850467335094\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] Epoch[10] Batch[0] avg_epoch_loss=8.355738\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=8.355737686157227\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:58 INFO 139830709036864] Epoch[10] Batch[5] avg_epoch_loss=8.397433\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=8.397432804107666\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:58 INFO 139830709036864] Epoch[10] Batch [5]#011Speed: 223.39 samples/sec#011loss=8.397433\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:59 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172456.2695243, \"EndTime\": 1663172459.4985585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3228.4152507781982, \"count\": 1, \"min\": 3228.4152507781982, \"max\": 3228.4152507781982}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:59 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.302911035293 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:59 INFO 139830709036864] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=10, train loss <loss>=8.398393154144287\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:20:59 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:00 INFO 139830709036864] Epoch[11] Batch[0] avg_epoch_loss=8.426277\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=8.426277160644531\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:01 INFO 139830709036864] Epoch[11] Batch[5] avg_epoch_loss=8.420163\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=8.420162836710611\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:01 INFO 139830709036864] Epoch[11] Batch [5]#011Speed: 223.18 samples/sec#011loss=8.420163\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:02 INFO 139830709036864] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172459.4986403, \"EndTime\": 1663172462.7086456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3209.5632553100586, \"count\": 1, \"min\": 3209.5632553100586, \"max\": 3209.5632553100586}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:02 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.657591496798 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:02 INFO 139830709036864] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=11, train loss <loss>=8.382410049438477\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:02 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:03 INFO 139830709036864] Epoch[12] Batch[0] avg_epoch_loss=8.521866\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=8.521865844726562\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:04 INFO 139830709036864] Epoch[12] Batch[5] avg_epoch_loss=8.411464\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=8.411463896433512\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:04 INFO 139830709036864] Epoch[12] Batch [5]#011Speed: 211.94 samples/sec#011loss=8.411464\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] Epoch[12] Batch[10] avg_epoch_loss=8.392902\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=8.370627975463867\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] Epoch[12] Batch [10]#011Speed: 204.64 samples/sec#011loss=8.370628\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172462.7087321, \"EndTime\": 1663172466.3982718, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3688.957452774048, \"count\": 1, \"min\": 3688.957452774048, \"max\": 3688.957452774048}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.07512019724027 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=12, train loss <loss>=8.392902114174582\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:06 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:07 INFO 139830709036864] Epoch[13] Batch[0] avg_epoch_loss=8.340893\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=8.340892791748047\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:08 INFO 139830709036864] Epoch[13] Batch[5] avg_epoch_loss=8.371239\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=8.37123934427897\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:08 INFO 139830709036864] Epoch[13] Batch [5]#011Speed: 239.77 samples/sec#011loss=8.371239\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:09 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172466.398349, \"EndTime\": 1663172469.5545626, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3155.6413173675537, \"count\": 1, \"min\": 3155.6413173675537, \"max\": 3155.6413173675537}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:09 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=201.8528111879532 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:09 INFO 139830709036864] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=13, train loss <loss>=8.355427551269532\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:09 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:10 INFO 139830709036864] Epoch[14] Batch[0] avg_epoch_loss=8.299916\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=8.29991626739502\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:11 INFO 139830709036864] Epoch[14] Batch[5] avg_epoch_loss=8.301120\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=8.301120281219482\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:11 INFO 139830709036864] Epoch[14] Batch [5]#011Speed: 227.25 samples/sec#011loss=8.301120\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] Epoch[14] Batch[10] avg_epoch_loss=8.327171\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=8.35843105316162\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] Epoch[14] Batch [10]#011Speed: 227.81 samples/sec#011loss=8.358431\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172469.5546448, \"EndTime\": 1663172472.9585853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3403.2835960388184, \"count\": 1, \"min\": 3403.2835960388184, \"max\": 3403.2835960388184}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.5675213042942 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=14, train loss <loss>=8.327170632102273\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:12 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:13 INFO 139830709036864] Epoch[15] Batch[0] avg_epoch_loss=8.219319\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=8.219319343566895\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:14 INFO 139830709036864] Epoch[15] Batch[5] avg_epoch_loss=8.326193\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=8.326193491617838\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:14 INFO 139830709036864] Epoch[15] Batch [5]#011Speed: 226.66 samples/sec#011loss=8.326193\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] Epoch[15] Batch[10] avg_epoch_loss=8.282189\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=8.229383277893067\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] Epoch[15] Batch [10]#011Speed: 208.83 samples/sec#011loss=8.229383\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172472.9586687, \"EndTime\": 1663172476.5222769, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3562.9515647888184, \"count\": 1, \"min\": 3562.9515647888184, \"max\": 3562.9515647888184}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.10923928851724 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=15, train loss <loss>=8.28218884901567\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:16 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_0ceba40e-2de1-4262-80af-0e246136aa61-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172476.5223856, \"EndTime\": 1663172476.6049745, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 82.08465576171875, \"count\": 1, \"min\": 82.08465576171875, \"max\": 82.08465576171875}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:17 INFO 139830709036864] Epoch[16] Batch[0] avg_epoch_loss=8.205662\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=8.20566177368164\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:18 INFO 139830709036864] Epoch[16] Batch[5] avg_epoch_loss=8.284754\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=8.284753799438477\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:18 INFO 139830709036864] Epoch[16] Batch [5]#011Speed: 212.37 samples/sec#011loss=8.284754\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:19 INFO 139830709036864] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172476.6050632, \"EndTime\": 1663172479.9596953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3354.5584678649902, \"count\": 1, \"min\": 3354.5584678649902, \"max\": 3354.5584678649902}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:19 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.3929352794822 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:19 INFO 139830709036864] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=16, train loss <loss>=8.252886295318604\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:19 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:20 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_9104ff3a-44cf-4b1c-acf0-67445ba36021-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172479.9597878, \"EndTime\": 1663172480.0638726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.58309745788574, \"count\": 1, \"min\": 103.58309745788574, \"max\": 103.58309745788574}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:20 INFO 139830709036864] Epoch[17] Batch[0] avg_epoch_loss=8.139242\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=8.139242172241211\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:22 INFO 139830709036864] Epoch[17] Batch[5] avg_epoch_loss=8.260944\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=8.26094357172648\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:22 INFO 139830709036864] Epoch[17] Batch [5]#011Speed: 212.38 samples/sec#011loss=8.260944\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] Epoch[17] Batch[10] avg_epoch_loss=8.284873\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=8.313589096069336\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] Epoch[17] Batch [10]#011Speed: 216.66 samples/sec#011loss=8.313589\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172480.063957, \"EndTime\": 1663172483.695872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3631.852626800537, \"count\": 1, \"min\": 3631.852626800537, \"max\": 3631.852626800537}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=177.86478032732518 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=17, train loss <loss>=8.284873355518688\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:23 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:24 INFO 139830709036864] Epoch[18] Batch[0] avg_epoch_loss=8.207362\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=8.207362174987793\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:25 INFO 139830709036864] Epoch[18] Batch[5] avg_epoch_loss=8.200748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=8.200748284657797\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:25 INFO 139830709036864] Epoch[18] Batch [5]#011Speed: 219.82 samples/sec#011loss=8.200748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172483.695956, \"EndTime\": 1663172487.0140355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3317.6310062408447, \"count\": 1, \"min\": 3317.6310062408447, \"max\": 3317.6310062408447}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.58588136953958 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=18, train loss <loss>=8.243192958831788\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_35c5db30-a62a-4819-8804-b8904ac415c9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172487.0141199, \"EndTime\": 1663172487.0811558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 66.45393371582031, \"count\": 1, \"min\": 66.45393371582031, \"max\": 66.45393371582031}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] Epoch[19] Batch[0] avg_epoch_loss=8.294518\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=8.29451847076416\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:29 INFO 139830709036864] Epoch[19] Batch[5] avg_epoch_loss=8.326754\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=8.326754093170166\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:29 INFO 139830709036864] Epoch[19] Batch [5]#011Speed: 218.92 samples/sec#011loss=8.326754\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] Epoch[19] Batch[10] avg_epoch_loss=8.262871\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=8.186211967468262\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] Epoch[19] Batch [10]#011Speed: 222.34 samples/sec#011loss=8.186212\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172487.081235, \"EndTime\": 1663172490.5665443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3485.2418899536133, \"count\": 1, \"min\": 3485.2418899536133, \"max\": 3485.2418899536133}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.20793958609366 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=19, train loss <loss>=8.262871308760209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:30 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:31 INFO 139830709036864] Epoch[20] Batch[0] avg_epoch_loss=7.980151\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=7.980151176452637\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:32 INFO 139830709036864] Epoch[20] Batch[5] avg_epoch_loss=8.111152\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=8.111152331034342\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:32 INFO 139830709036864] Epoch[20] Batch [5]#011Speed: 227.24 samples/sec#011loss=8.111152\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] Epoch[20] Batch[10] avg_epoch_loss=8.152907\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=8.203012275695801\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] Epoch[20] Batch [10]#011Speed: 219.04 samples/sec#011loss=8.203012\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172490.5666184, \"EndTime\": 1663172494.06169, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3494.645833969116, \"count\": 1, \"min\": 3494.645833969116, \"max\": 3494.645833969116}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.27503053931622 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=20, train loss <loss>=8.152906851335006\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_3d883ae3-7225-427a-8a14-c071dc23390a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172494.0617821, \"EndTime\": 1663172494.1346056, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 72.23820686340332, \"count\": 1, \"min\": 72.23820686340332, \"max\": 72.23820686340332}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] Epoch[21] Batch[0] avg_epoch_loss=7.929790\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=7.929790496826172\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:36 INFO 139830709036864] Epoch[21] Batch[5] avg_epoch_loss=8.105008\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=8.105007568995157\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:36 INFO 139830709036864] Epoch[21] Batch [5]#011Speed: 219.23 samples/sec#011loss=8.105008\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172494.1346822, \"EndTime\": 1663172497.0807235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2945.9750652313232, \"count\": 1, \"min\": 2945.9750652313232, \"max\": 2945.9750652313232}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.1532407268784 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=21, train loss <loss>=8.146741920047337\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_7ee4a139-f43c-4c3a-89ea-9b3538b77976-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172497.0808322, \"EndTime\": 1663172497.1516752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 70.21689414978027, \"count\": 1, \"min\": 70.21689414978027, \"max\": 70.21689414978027}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] Epoch[22] Batch[0] avg_epoch_loss=8.174608\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=8.17460823059082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:39 INFO 139830709036864] Epoch[22] Batch[5] avg_epoch_loss=8.167213\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=8.167213280995687\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:39 INFO 139830709036864] Epoch[22] Batch [5]#011Speed: 228.73 samples/sec#011loss=8.167213\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] Epoch[22] Batch[10] avg_epoch_loss=8.188764\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=8.214624214172364\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] Epoch[22] Batch [10]#011Speed: 217.64 samples/sec#011loss=8.214624\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172497.1517618, \"EndTime\": 1663172500.6114373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3459.606647491455, \"count\": 1, \"min\": 3459.606647491455, \"max\": 3459.606647491455}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.43026253659588 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=22, train loss <loss>=8.188763705166904\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:40 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:41 INFO 139830709036864] Epoch[23] Batch[0] avg_epoch_loss=8.021630\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=8.02163028717041\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:42 INFO 139830709036864] Epoch[23] Batch[5] avg_epoch_loss=8.036138\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=8.036138216654459\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:42 INFO 139830709036864] Epoch[23] Batch [5]#011Speed: 212.51 samples/sec#011loss=8.036138\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:43 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172500.6115258, \"EndTime\": 1663172503.9538603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3341.7751789093018, \"count\": 1, \"min\": 3341.7751789093018, \"max\": 3341.7751789093018}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:43 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.21645250187416 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:43 INFO 139830709036864] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=23, train loss <loss>=8.06635766029358\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:43 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:44 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_92bbab76-1e40-40fa-9c21-cf83f6acb4b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172503.9539423, \"EndTime\": 1663172504.0576463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.20258140563965, \"count\": 1, \"min\": 103.20258140563965, \"max\": 103.20258140563965}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:44 INFO 139830709036864] Epoch[24] Batch[0] avg_epoch_loss=8.134199\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=8.134199142456055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:46 INFO 139830709036864] Epoch[24] Batch[5] avg_epoch_loss=8.112091\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=8.112091382344564\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:46 INFO 139830709036864] Epoch[24] Batch [5]#011Speed: 218.51 samples/sec#011loss=8.112091\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] Epoch[24] Batch[10] avg_epoch_loss=8.137204\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=8.167339706420899\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] Epoch[24] Batch [10]#011Speed: 208.91 samples/sec#011loss=8.167340\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172504.057733, \"EndTime\": 1663172507.6926153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3634.8159313201904, \"count\": 1, \"min\": 3634.8159313201904, \"max\": 3634.8159313201904}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.3702161688632 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=24, train loss <loss>=8.137204256924717\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:47 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:48 INFO 139830709036864] Epoch[25] Batch[0] avg_epoch_loss=8.133135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=8.133134841918945\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:49 INFO 139830709036864] Epoch[25] Batch[5] avg_epoch_loss=8.039728\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=8.039727687835693\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:49 INFO 139830709036864] Epoch[25] Batch [5]#011Speed: 217.23 samples/sec#011loss=8.039728\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:50 INFO 139830709036864] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172507.6927023, \"EndTime\": 1663172510.99091, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3297.7683544158936, \"count\": 1, \"min\": 3297.7683544158936, \"max\": 3297.7683544158936}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:50 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.6045856342088 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:50 INFO 139830709036864] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=25, train loss <loss>=8.064386749267578\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:50 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:51 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_4f8bea0a-1aeb-492b-b711-d9410d391447-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172510.9910045, \"EndTime\": 1663172511.0671542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 75.46615600585938, \"count\": 1, \"min\": 75.46615600585938, \"max\": 75.46615600585938}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:51 INFO 139830709036864] Epoch[26] Batch[0] avg_epoch_loss=8.044311\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=8.044310569763184\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:53 INFO 139830709036864] Epoch[26] Batch[5] avg_epoch_loss=8.023076\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=8.023076057434082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:53 INFO 139830709036864] Epoch[26] Batch [5]#011Speed: 220.15 samples/sec#011loss=8.023076\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172511.0672717, \"EndTime\": 1663172514.333343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3265.9943103790283, \"count\": 1, \"min\": 3265.9943103790283, \"max\": 3265.9943103790283}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.03137360905086 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=26, train loss <loss>=8.047534704208374\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_ebc6d851-8d92-43a5-a1f9-e4d13f363789-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172514.3334465, \"EndTime\": 1663172514.4174514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 83.41217041015625, \"count\": 1, \"min\": 83.41217041015625, \"max\": 83.41217041015625}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] Epoch[27] Batch[0] avg_epoch_loss=8.058712\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=8.058712005615234\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:56 INFO 139830709036864] Epoch[27] Batch[5] avg_epoch_loss=8.002358\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=8.002357959747314\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:56 INFO 139830709036864] Epoch[27] Batch [5]#011Speed: 230.43 samples/sec#011loss=8.002358\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172514.4175303, \"EndTime\": 1663172517.4814327, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3063.8346672058105, \"count\": 1, \"min\": 3063.8346672058105, \"max\": 3063.8346672058105}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=198.76185518348694 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=27, train loss <loss>=7.988334083557129\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:57 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_ad300597-c91f-44a5-a8fb-e6e31355e557-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172517.4815264, \"EndTime\": 1663172517.5857475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.73473167419434, \"count\": 1, \"min\": 103.73473167419434, \"max\": 103.73473167419434}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:58 INFO 139830709036864] Epoch[28] Batch[0] avg_epoch_loss=8.150441\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=8.15044116973877\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:59 INFO 139830709036864] Epoch[28] Batch[5] avg_epoch_loss=8.101339\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=8.101338704427084\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:21:59 INFO 139830709036864] Epoch[28] Batch [5]#011Speed: 215.27 samples/sec#011loss=8.101339\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:00 INFO 139830709036864] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172517.5858352, \"EndTime\": 1663172520.8845592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3298.651933670044, \"count\": 1, \"min\": 3298.651933670044, \"max\": 3298.651933670044}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:00 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.01030807427273 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:00 INFO 139830709036864] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=28, train loss <loss>=8.077472591400147\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:00 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:01 INFO 139830709036864] Epoch[29] Batch[0] avg_epoch_loss=7.949364\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=7.949363708496094\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:02 INFO 139830709036864] Epoch[29] Batch[5] avg_epoch_loss=7.969577\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=7.969576994578044\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:02 INFO 139830709036864] Epoch[29] Batch [5]#011Speed: 218.87 samples/sec#011loss=7.969577\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172520.884658, \"EndTime\": 1663172524.1612587, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3275.984287261963, \"count\": 1, \"min\": 3275.984287261963, \"max\": 3275.984287261963}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.2497337881495 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=29, train loss <loss>=7.995310306549072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] Epoch[30] Batch[0] avg_epoch_loss=7.974600\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=7.974600315093994\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:06 INFO 139830709036864] Epoch[30] Batch[5] avg_epoch_loss=7.990697\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=7.990697065989177\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:06 INFO 139830709036864] Epoch[30] Batch [5]#011Speed: 226.33 samples/sec#011loss=7.990697\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] Epoch[30] Batch[10] avg_epoch_loss=8.017782\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=8.050283050537109\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] Epoch[30] Batch [10]#011Speed: 226.23 samples/sec#011loss=8.050283\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172524.1613302, \"EndTime\": 1663172527.7261128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3564.267873764038, \"count\": 1, \"min\": 3564.267873764038, \"max\": 3564.267873764038}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.64102474059112 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=30, train loss <loss>=8.017781604420055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:07 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:08 INFO 139830709036864] Epoch[31] Batch[0] avg_epoch_loss=7.951256\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=7.951255798339844\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:09 INFO 139830709036864] Epoch[31] Batch[5] avg_epoch_loss=7.960603\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=7.960602521896362\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:09 INFO 139830709036864] Epoch[31] Batch [5]#011Speed: 214.32 samples/sec#011loss=7.960603\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] Epoch[31] Batch[10] avg_epoch_loss=8.015528\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=8.08143949508667\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] Epoch[31] Batch [10]#011Speed: 208.86 samples/sec#011loss=8.081439\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172527.7261798, \"EndTime\": 1663172531.3797555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3653.146982192993, \"count\": 1, \"min\": 3653.146982192993, \"max\": 3653.146982192993}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.40885165017576 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=31, train loss <loss>=8.015528418801047\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:11 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:12 INFO 139830709036864] Epoch[32] Batch[0] avg_epoch_loss=7.875741\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=7.8757405281066895\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:13 INFO 139830709036864] Epoch[32] Batch[5] avg_epoch_loss=7.978420\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=7.97842001914978\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:13 INFO 139830709036864] Epoch[32] Batch [5]#011Speed: 226.24 samples/sec#011loss=7.978420\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] Epoch[32] Batch[10] avg_epoch_loss=7.942426\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=7.899232292175293\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] Epoch[32] Batch [10]#011Speed: 221.26 samples/sec#011loss=7.899232\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172531.3798313, \"EndTime\": 1663172534.8724763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3492.1796321868896, \"count\": 1, \"min\": 3492.1796321868896, \"max\": 3492.1796321868896}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.5559639616514 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=32, train loss <loss>=7.94242559779774\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:14 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_f54370c3-0e18-4aa5-abe2-55ede8ee4afc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172534.8725522, \"EndTime\": 1663172534.9761083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.09004783630371, \"count\": 1, \"min\": 103.09004783630371, \"max\": 103.09004783630371}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:15 INFO 139830709036864] Epoch[33] Batch[0] avg_epoch_loss=8.072701\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=8.072701454162598\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:17 INFO 139830709036864] Epoch[33] Batch[5] avg_epoch_loss=7.925499\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=7.925499439239502\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:17 INFO 139830709036864] Epoch[33] Batch [5]#011Speed: 220.69 samples/sec#011loss=7.925499\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172534.976185, \"EndTime\": 1663172538.2770526, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3300.7988929748535, \"count\": 1, \"min\": 3300.7988929748535, \"max\": 3300.7988929748535}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.2524520069827 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=33, train loss <loss>=7.688914203643799\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:18 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_6b6d862e-7d71-483b-8717-a59456b0b56a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172538.2771397, \"EndTime\": 1663172538.3606553, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 82.8549861907959, \"count\": 1, \"min\": 82.8549861907959, \"max\": 82.8549861907959}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:19 INFO 139830709036864] Epoch[34] Batch[0] avg_epoch_loss=7.853134\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=7.853133678436279\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:20 INFO 139830709036864] Epoch[34] Batch[5] avg_epoch_loss=7.952691\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=7.952690839767456\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:20 INFO 139830709036864] Epoch[34] Batch [5]#011Speed: 211.67 samples/sec#011loss=7.952691\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] Epoch[34] Batch[10] avg_epoch_loss=7.956049\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=7.960079097747803\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] Epoch[34] Batch [10]#011Speed: 224.69 samples/sec#011loss=7.960079\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172538.3607435, \"EndTime\": 1663172541.9406786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3579.8628330230713, \"count\": 1, \"min\": 3579.8628330230713, \"max\": 3579.8628330230713}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.40262724520193 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=34, train loss <loss>=7.956049138849432\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:21 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:22 INFO 139830709036864] Epoch[35] Batch[0] avg_epoch_loss=7.857094\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=7.857093811035156\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:23 INFO 139830709036864] Epoch[35] Batch[5] avg_epoch_loss=7.880775\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=7.880774577458699\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:23 INFO 139830709036864] Epoch[35] Batch [5]#011Speed: 230.65 samples/sec#011loss=7.880775\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172541.9407692, \"EndTime\": 1663172545.074053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3132.858991622925, \"count\": 1, \"min\": 3132.858991622925, \"max\": 3132.858991622925}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=202.3628224315025 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=35, train loss <loss>=7.878260469436645\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] Epoch[36] Batch[0] avg_epoch_loss=7.868706\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=7.868706226348877\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:27 INFO 139830709036864] Epoch[36] Batch[5] avg_epoch_loss=7.919698\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=7.919697841008504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:27 INFO 139830709036864] Epoch[36] Batch [5]#011Speed: 213.51 samples/sec#011loss=7.919698\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:28 INFO 139830709036864] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172545.0741374, \"EndTime\": 1663172548.424813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3350.1033782958984, \"count\": 1, \"min\": 3350.1033782958984, \"max\": 3350.1033782958984}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:28 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.49514542102813 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:28 INFO 139830709036864] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=36, train loss <loss>=7.921827745437622\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:28 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:29 INFO 139830709036864] Epoch[37] Batch[0] avg_epoch_loss=7.891516\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=7.891516208648682\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:30 INFO 139830709036864] Epoch[37] Batch[5] avg_epoch_loss=7.986883\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=7.986882925033569\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:30 INFO 139830709036864] Epoch[37] Batch [5]#011Speed: 219.76 samples/sec#011loss=7.986883\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:31 INFO 139830709036864] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172548.424899, \"EndTime\": 1663172551.7544434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3328.9780616760254, \"count\": 1, \"min\": 3328.9780616760254, \"max\": 3328.9780616760254}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:31 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.03496060375585 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:31 INFO 139830709036864] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=37, train loss <loss>=7.941430377960205\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:31 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:32 INFO 139830709036864] Epoch[38] Batch[0] avg_epoch_loss=7.983438\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=7.983438014984131\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:33 INFO 139830709036864] Epoch[38] Batch[5] avg_epoch_loss=7.849670\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=7.849670171737671\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:33 INFO 139830709036864] Epoch[38] Batch [5]#011Speed: 230.74 samples/sec#011loss=7.849670\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:34 INFO 139830709036864] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172551.7545238, \"EndTime\": 1663172554.8611665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3105.9606075286865, \"count\": 1, \"min\": 3105.9606075286865, \"max\": 3105.9606075286865}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:34 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=199.27991834377497 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:34 INFO 139830709036864] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=38, train loss <loss>=7.845744228363037\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:34 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:35 INFO 139830709036864] Epoch[39] Batch[0] avg_epoch_loss=7.860209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=7.860208511352539\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:36 INFO 139830709036864] Epoch[39] Batch[5] avg_epoch_loss=7.875240\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=7.875240087509155\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:36 INFO 139830709036864] Epoch[39] Batch [5]#011Speed: 215.66 samples/sec#011loss=7.875240\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] Epoch[39] Batch[10] avg_epoch_loss=7.903400\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=7.937192726135254\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] Epoch[39] Batch [10]#011Speed: 206.94 samples/sec#011loss=7.937193\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172554.8613434, \"EndTime\": 1663172558.5349514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3672.9679107666016, \"count\": 1, \"min\": 3672.9679107666016, \"max\": 3672.9679107666016}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.7584137984126 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=39, train loss <loss>=7.903400377793745\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:38 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:39 INFO 139830709036864] Epoch[40] Batch[0] avg_epoch_loss=7.863130\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=7.863129615783691\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:40 INFO 139830709036864] Epoch[40] Batch[5] avg_epoch_loss=7.784535\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=7.7845345338185625\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:40 INFO 139830709036864] Epoch[40] Batch [5]#011Speed: 212.39 samples/sec#011loss=7.784535\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] Epoch[40] Batch[10] avg_epoch_loss=7.847755\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=7.923618507385254\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] Epoch[40] Batch [10]#011Speed: 223.48 samples/sec#011loss=7.923619\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172558.5350342, \"EndTime\": 1663172562.1391304, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3603.499174118042, \"count\": 1, \"min\": 3603.499174118042, \"max\": 3603.499174118042}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.9268519111508 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=40, train loss <loss>=7.847754521803423\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] Epoch[41] Batch[0] avg_epoch_loss=7.884875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=7.884875297546387\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:44 INFO 139830709036864] Epoch[41] Batch[5] avg_epoch_loss=7.832531\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=7.832530577977498\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:44 INFO 139830709036864] Epoch[41] Batch [5]#011Speed: 224.79 samples/sec#011loss=7.832531\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] Epoch[41] Batch[10] avg_epoch_loss=7.820164\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=7.805324268341065\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] Epoch[41] Batch [10]#011Speed: 226.41 samples/sec#011loss=7.805324\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172562.1392565, \"EndTime\": 1663172565.5990329, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3459.1805934906006, \"count\": 1, \"min\": 3459.1805934906006, \"max\": 3459.1805934906006}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.08044789343077 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=41, train loss <loss>=7.820164073597301\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:45 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:46 INFO 139830709036864] Epoch[42] Batch[0] avg_epoch_loss=7.876230\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=7.876229763031006\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:47 INFO 139830709036864] Epoch[42] Batch[5] avg_epoch_loss=7.835729\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=7.835729440053304\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:47 INFO 139830709036864] Epoch[42] Batch [5]#011Speed: 219.40 samples/sec#011loss=7.835729\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:48 INFO 139830709036864] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172565.599088, \"EndTime\": 1663172568.5751283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2975.496768951416, \"count\": 1, \"min\": 2975.496768951416, \"max\": 2975.496768951416}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:48 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.22575124681686 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:48 INFO 139830709036864] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=42, train loss <loss>=7.810309198167589\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:48 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:49 INFO 139830709036864] Epoch[43] Batch[0] avg_epoch_loss=7.831722\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=7.831721782684326\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:50 INFO 139830709036864] Epoch[43] Batch[5] avg_epoch_loss=7.791209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=7.791208744049072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:50 INFO 139830709036864] Epoch[43] Batch [5]#011Speed: 217.41 samples/sec#011loss=7.791209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:51 INFO 139830709036864] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172568.5753884, \"EndTime\": 1663172571.823026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3247.048854827881, \"count\": 1, \"min\": 3247.048854827881, \"max\": 3247.048854827881}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:51 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.46486362023717 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:51 INFO 139830709036864] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=43, train loss <loss>=7.840094470977784\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:51 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:52 INFO 139830709036864] Epoch[44] Batch[0] avg_epoch_loss=7.900668\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=7.900667667388916\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:53 INFO 139830709036864] Epoch[44] Batch[5] avg_epoch_loss=7.843318\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=7.843318223953247\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:53 INFO 139830709036864] Epoch[44] Batch [5]#011Speed: 216.33 samples/sec#011loss=7.843318\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172571.8231082, \"EndTime\": 1663172575.1735258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3349.605083465576, \"count\": 1, \"min\": 3349.605083465576, \"max\": 3349.605083465576}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.68710897875178 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=44, train loss <loss>=7.849705553054809\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] Epoch[45] Batch[0] avg_epoch_loss=7.654684\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=7.654683589935303\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:57 INFO 139830709036864] Epoch[45] Batch[5] avg_epoch_loss=7.847231\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=7.847230672836304\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:57 INFO 139830709036864] Epoch[45] Batch [5]#011Speed: 235.00 samples/sec#011loss=7.847231\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172575.1736047, \"EndTime\": 1663172578.2748451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3100.6219387054443, \"count\": 1, \"min\": 3100.6219387054443, \"max\": 3100.6219387054443}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.43648465514 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=45, train loss <loss>=7.806414985656739\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] Epoch[46] Batch[0] avg_epoch_loss=7.830466\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:22:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=7.830465793609619\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:00 INFO 139830709036864] Epoch[46] Batch[5] avg_epoch_loss=7.737312\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=7.7373119195302325\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:00 INFO 139830709036864] Epoch[46] Batch [5]#011Speed: 217.80 samples/sec#011loss=7.737312\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] Epoch[46] Batch[10] avg_epoch_loss=7.791648\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=7.856850433349609\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] Epoch[46] Batch [10]#011Speed: 215.65 samples/sec#011loss=7.856850\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172578.274937, \"EndTime\": 1663172581.877002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3601.438045501709, \"count\": 1, \"min\": 3601.438045501709, \"max\": 3601.438045501709}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.47797611344112 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=46, train loss <loss>=7.791647607629949\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:01 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:02 INFO 139830709036864] Epoch[47] Batch[0] avg_epoch_loss=7.675807\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=7.675806522369385\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:04 INFO 139830709036864] Epoch[47] Batch[5] avg_epoch_loss=7.745726\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=7.7457263469696045\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:04 INFO 139830709036864] Epoch[47] Batch [5]#011Speed: 209.67 samples/sec#011loss=7.745726\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] Epoch[47] Batch[10] avg_epoch_loss=7.770718\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=7.800707912445068\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] Epoch[47] Batch [10]#011Speed: 203.48 samples/sec#011loss=7.800708\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172581.8770754, \"EndTime\": 1663172585.6017325, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3724.238395690918, \"count\": 1, \"min\": 3724.238395690918, \"max\": 3724.238395690918}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=177.2114628181151 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=47, train loss <loss>=7.77071796764027\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:05 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:06 INFO 139830709036864] Epoch[48] Batch[0] avg_epoch_loss=7.854286\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=7.854285717010498\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:07 INFO 139830709036864] Epoch[48] Batch[5] avg_epoch_loss=7.804211\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=7.804210901260376\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:07 INFO 139830709036864] Epoch[48] Batch [5]#011Speed: 227.88 samples/sec#011loss=7.804211\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] Epoch[48] Batch[10] avg_epoch_loss=7.823555\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=7.8467686653137205\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] Epoch[48] Batch [10]#011Speed: 221.64 samples/sec#011loss=7.846769\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172585.601819, \"EndTime\": 1663172589.0510175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3448.688745498657, \"count\": 1, \"min\": 3448.688745498657, \"max\": 3448.688745498657}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.34099284129857 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=48, train loss <loss>=7.823555339466441\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] Epoch[49] Batch[0] avg_epoch_loss=7.791245\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=7.791245460510254\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:11 INFO 139830709036864] Epoch[49] Batch[5] avg_epoch_loss=7.802270\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=7.802270253499349\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:11 INFO 139830709036864] Epoch[49] Batch [5]#011Speed: 216.70 samples/sec#011loss=7.802270\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] Epoch[49] Batch[10] avg_epoch_loss=7.774207\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=7.740530300140381\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] Epoch[49] Batch [10]#011Speed: 209.30 samples/sec#011loss=7.740530\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172589.0510962, \"EndTime\": 1663172592.7010548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3649.37162399292, \"count\": 1, \"min\": 3649.37162399292, \"max\": 3649.37162399292}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=176.46332972344823 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=49, train loss <loss>=7.774206638336182\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:12 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:13 INFO 139830709036864] Epoch[50] Batch[0] avg_epoch_loss=7.814418\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=7.814417839050293\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:14 INFO 139830709036864] Epoch[50] Batch[5] avg_epoch_loss=7.743379\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=7.74337903658549\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:14 INFO 139830709036864] Epoch[50] Batch [5]#011Speed: 231.93 samples/sec#011loss=7.743379\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] Epoch[50] Batch[10] avg_epoch_loss=7.771675\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=7.805630874633789\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] Epoch[50] Batch [10]#011Speed: 221.12 samples/sec#011loss=7.805631\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172592.701129, \"EndTime\": 1663172596.14474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3443.142890930176, \"count\": 1, \"min\": 3443.142890930176, \"max\": 3443.142890930176}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.3551635842433 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=50, train loss <loss>=7.771675326607444\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] Epoch[51] Batch[0] avg_epoch_loss=7.673835\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=7.673835277557373\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:18 INFO 139830709036864] Epoch[51] Batch[5] avg_epoch_loss=7.693897\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=7.693897008895874\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:18 INFO 139830709036864] Epoch[51] Batch [5]#011Speed: 217.15 samples/sec#011loss=7.693897\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:19 INFO 139830709036864] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172596.1448255, \"EndTime\": 1663172599.4563575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3310.983180999756, \"count\": 1, \"min\": 3310.983180999756, \"max\": 3310.983180999756}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:19 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.50978713158025 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:19 INFO 139830709036864] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=51, train loss <loss>=7.697690820693969\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:19 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:20 INFO 139830709036864] Epoch[52] Batch[0] avg_epoch_loss=7.782658\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=7.782658100128174\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:21 INFO 139830709036864] Epoch[52] Batch[5] avg_epoch_loss=7.724645\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=7.724644819895427\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:21 INFO 139830709036864] Epoch[52] Batch [5]#011Speed: 213.70 samples/sec#011loss=7.724645\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:22 INFO 139830709036864] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172599.4564507, \"EndTime\": 1663172602.7644062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3307.2924613952637, \"count\": 1, \"min\": 3307.2924613952637, \"max\": 3307.2924613952637}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:22 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.55032139903935 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:22 INFO 139830709036864] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=52, train loss <loss>=7.743010520935059\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:22 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:23 INFO 139830709036864] Epoch[53] Batch[0] avg_epoch_loss=7.725701\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=7.725700855255127\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:24 INFO 139830709036864] Epoch[53] Batch[5] avg_epoch_loss=7.712988\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=7.712987661361694\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:24 INFO 139830709036864] Epoch[53] Batch [5]#011Speed: 217.47 samples/sec#011loss=7.712988\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172602.764496, \"EndTime\": 1663172606.0706646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3305.644750595093, \"count\": 1, \"min\": 3305.644750595093, \"max\": 3305.644750595093}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.08577895556135 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=53, train loss <loss>=7.700289869308472\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] Epoch[54] Batch[0] avg_epoch_loss=7.774191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=7.774190902709961\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:28 INFO 139830709036864] Epoch[54] Batch[5] avg_epoch_loss=7.735429\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=7.7354293664296465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:28 INFO 139830709036864] Epoch[54] Batch [5]#011Speed: 229.06 samples/sec#011loss=7.735429\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172606.0707932, \"EndTime\": 1663172609.1617131, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3090.2247428894043, \"count\": 1, \"min\": 3090.2247428894043, \"max\": 3090.2247428894043}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=202.88895734587285 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=54, train loss <loss>=7.71785192489624\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] Epoch[55] Batch[0] avg_epoch_loss=7.495501\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=7.495500564575195\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:31 INFO 139830709036864] Epoch[55] Batch[5] avg_epoch_loss=7.658391\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=7.658390601476033\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:31 INFO 139830709036864] Epoch[55] Batch [5]#011Speed: 239.58 samples/sec#011loss=7.658391\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] Epoch[55] Batch[10] avg_epoch_loss=7.565612\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=7.454277801513672\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] Epoch[55] Batch [10]#011Speed: 223.90 samples/sec#011loss=7.454278\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172609.1618068, \"EndTime\": 1663172612.5534682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3391.17169380188, \"count\": 1, \"min\": 3391.17169380188, \"max\": 3391.17169380188}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.31889248524354 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=55, train loss <loss>=7.565612056038597\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:32 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_09adc62b-18ad-4aef-bce4-b969a747b817-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172612.553589, \"EndTime\": 1663172612.6178565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.72237205505371, \"count\": 1, \"min\": 63.72237205505371, \"max\": 63.72237205505371}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:33 INFO 139830709036864] Epoch[56] Batch[0] avg_epoch_loss=7.557495\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=7.5574951171875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:34 INFO 139830709036864] Epoch[56] Batch[5] avg_epoch_loss=7.698941\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=7.698940912882487\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:34 INFO 139830709036864] Epoch[56] Batch [5]#011Speed: 212.23 samples/sec#011loss=7.698941\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] Epoch[56] Batch[10] avg_epoch_loss=7.757716\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=7.828246402740478\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] Epoch[56] Batch [10]#011Speed: 213.86 samples/sec#011loss=7.828246\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172612.6179516, \"EndTime\": 1663172616.3041232, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3686.108112335205, \"count\": 1, \"min\": 3686.108112335205, \"max\": 3686.108112335205}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=174.16192783109818 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=56, train loss <loss>=7.75771613554521\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] Epoch[57] Batch[0] avg_epoch_loss=7.818346\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=7.81834602355957\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:38 INFO 139830709036864] Epoch[57] Batch[5] avg_epoch_loss=7.643145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=7.643145163853963\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:38 INFO 139830709036864] Epoch[57] Batch [5]#011Speed: 220.22 samples/sec#011loss=7.643145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:39 INFO 139830709036864] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172616.3042061, \"EndTime\": 1663172619.5933566, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3288.7649536132812, \"count\": 1, \"min\": 3288.7649536132812, \"max\": 3288.7649536132812}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:39 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.20892493090975 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:39 INFO 139830709036864] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=57, train loss <loss>=7.631908512115478\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:39 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:40 INFO 139830709036864] Epoch[58] Batch[0] avg_epoch_loss=7.645359\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=7.645359039306641\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:41 INFO 139830709036864] Epoch[58] Batch[5] avg_epoch_loss=7.692911\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=7.692910989125569\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:41 INFO 139830709036864] Epoch[58] Batch [5]#011Speed: 220.83 samples/sec#011loss=7.692911\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] Epoch[58] Batch[10] avg_epoch_loss=7.698399\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=7.704984188079834\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] Epoch[58] Batch [10]#011Speed: 211.34 samples/sec#011loss=7.704984\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172619.5934513, \"EndTime\": 1663172623.1571338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3563.152313232422, \"count\": 1, \"min\": 3563.152313232422, \"max\": 3563.152313232422}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.71266550306066 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=58, train loss <loss>=7.698398806832054\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] Epoch[59] Batch[0] avg_epoch_loss=7.685795\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=7.685795307159424\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:45 INFO 139830709036864] Epoch[59] Batch[5] avg_epoch_loss=7.623708\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=7.6237077713012695\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:45 INFO 139830709036864] Epoch[59] Batch [5]#011Speed: 227.84 samples/sec#011loss=7.623708\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] Epoch[59] Batch[10] avg_epoch_loss=7.678643\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=7.744565677642822\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] Epoch[59] Batch [10]#011Speed: 229.67 samples/sec#011loss=7.744566\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172623.1572242, \"EndTime\": 1663172626.5835783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3425.7938861846924, \"count\": 1, \"min\": 3425.7938861846924, \"max\": 3425.7938861846924}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.73046669783028 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=59, train loss <loss>=7.678643183274702\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:46 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:47 INFO 139830709036864] Epoch[60] Batch[0] avg_epoch_loss=7.587564\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=7.587563514709473\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:48 INFO 139830709036864] Epoch[60] Batch[5] avg_epoch_loss=7.600972\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=7.600972493489583\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:48 INFO 139830709036864] Epoch[60] Batch [5]#011Speed: 217.13 samples/sec#011loss=7.600972\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:49 INFO 139830709036864] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172626.583654, \"EndTime\": 1663172629.8544583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3270.3795433044434, \"count\": 1, \"min\": 3270.3795433044434, \"max\": 3270.3795433044434}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.7058691795629 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=60, train loss <loss>=7.643753433227539\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:50 INFO 139830709036864] Epoch[61] Batch[0] avg_epoch_loss=7.552549\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=7.552549362182617\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:52 INFO 139830709036864] Epoch[61] Batch[5] avg_epoch_loss=7.609059\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=7.609058777491252\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:52 INFO 139830709036864] Epoch[61] Batch [5]#011Speed: 211.27 samples/sec#011loss=7.609059\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] Epoch[61] Batch[10] avg_epoch_loss=7.685374\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=7.776953029632568\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] Epoch[61] Batch [10]#011Speed: 226.80 samples/sec#011loss=7.776953\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172629.8545492, \"EndTime\": 1663172633.4323573, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3577.3067474365234, \"count\": 1, \"min\": 3577.3067474365234, \"max\": 3577.3067474365234}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.9743434408868 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=61, train loss <loss>=7.6853743466463955\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:53 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:54 INFO 139830709036864] Epoch[62] Batch[0] avg_epoch_loss=7.715740\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=7.715739727020264\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:55 INFO 139830709036864] Epoch[62] Batch[5] avg_epoch_loss=7.682779\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=7.682778676350911\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:55 INFO 139830709036864] Epoch[62] Batch [5]#011Speed: 214.53 samples/sec#011loss=7.682779\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:56 INFO 139830709036864] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172633.4324408, \"EndTime\": 1663172636.6542306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3221.2343215942383, \"count\": 1, \"min\": 3221.2343215942383, \"max\": 3221.2343215942383}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.213225188248 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=62, train loss <loss>=7.6801807403564455\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:57 INFO 139830709036864] Epoch[63] Batch[0] avg_epoch_loss=7.492110\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=7.492109775543213\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:58 INFO 139830709036864] Epoch[63] Batch[5] avg_epoch_loss=7.620801\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=7.620800813039144\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:58 INFO 139830709036864] Epoch[63] Batch [5]#011Speed: 231.40 samples/sec#011loss=7.620801\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:59 INFO 139830709036864] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172636.6544914, \"EndTime\": 1663172639.8750453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3220.03173828125, \"count\": 1, \"min\": 3220.03173828125, \"max\": 3220.03173828125}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:59 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.3979173277684 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:59 INFO 139830709036864] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=63, train loss <loss>=7.6350184917449955\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:23:59 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:00 INFO 139830709036864] Epoch[64] Batch[0] avg_epoch_loss=7.682069\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=7.682069301605225\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:01 INFO 139830709036864] Epoch[64] Batch[5] avg_epoch_loss=7.570805\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=7.570805311203003\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:01 INFO 139830709036864] Epoch[64] Batch [5]#011Speed: 226.27 samples/sec#011loss=7.570805\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] Epoch[64] Batch[10] avg_epoch_loss=7.558126\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=7.54291181564331\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] Epoch[64] Batch [10]#011Speed: 218.17 samples/sec#011loss=7.542912\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172639.8751662, \"EndTime\": 1663172643.3772397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3501.338481903076, \"count\": 1, \"min\": 3501.338481903076, \"max\": 3501.338481903076}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.0645783489229 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=64, train loss <loss>=7.558126449584961\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:03 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_5e0da1eb-12a4-41ca-99c7-b35b5542b0d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172643.3773274, \"EndTime\": 1663172643.4674897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 89.67328071594238, \"count\": 1, \"min\": 89.67328071594238, \"max\": 89.67328071594238}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:04 INFO 139830709036864] Epoch[65] Batch[0] avg_epoch_loss=7.681652\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=7.681652069091797\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:05 INFO 139830709036864] Epoch[65] Batch[5] avg_epoch_loss=7.625831\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=7.625831127166748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:05 INFO 139830709036864] Epoch[65] Batch [5]#011Speed: 207.42 samples/sec#011loss=7.625831\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:06 INFO 139830709036864] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172643.467573, \"EndTime\": 1663172646.8556588, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3388.0138397216797, \"count\": 1, \"min\": 3388.0138397216797, \"max\": 3388.0138397216797}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:06 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.35155035653122 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:06 INFO 139830709036864] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=65, train loss <loss>=7.586342239379883\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:06 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:07 INFO 139830709036864] Epoch[66] Batch[0] avg_epoch_loss=7.533454\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=7.533453941345215\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:08 INFO 139830709036864] Epoch[66] Batch[5] avg_epoch_loss=7.577646\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=7.577645540237427\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:08 INFO 139830709036864] Epoch[66] Batch [5]#011Speed: 223.25 samples/sec#011loss=7.577646\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172646.8557606, \"EndTime\": 1663172650.11211, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3255.866765975952, \"count\": 1, \"min\": 3255.866765975952, \"max\": 3255.866765975952}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.63305942364153 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=66, train loss <loss>=7.593391799926758\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] Epoch[67] Batch[0] avg_epoch_loss=7.618899\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=7.618898868560791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:12 INFO 139830709036864] Epoch[67] Batch[5] avg_epoch_loss=7.606723\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=7.606722513834636\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:12 INFO 139830709036864] Epoch[67] Batch [5]#011Speed: 233.65 samples/sec#011loss=7.606723\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172650.1122997, \"EndTime\": 1663172653.2206628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3107.839345932007, \"count\": 1, \"min\": 3107.839345932007, \"max\": 3107.839345932007}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.98331001535627 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=67, train loss <loss>=7.595109844207764\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] Epoch[68] Batch[0] avg_epoch_loss=7.483190\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=7.483189582824707\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:15 INFO 139830709036864] Epoch[68] Batch[5] avg_epoch_loss=7.518954\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=7.518953959147136\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:15 INFO 139830709036864] Epoch[68] Batch [5]#011Speed: 232.63 samples/sec#011loss=7.518954\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172653.2207367, \"EndTime\": 1663172656.3558998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3134.615182876587, \"count\": 1, \"min\": 3134.615182876587, \"max\": 3134.615182876587}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.3169905731519 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=68, train loss <loss>=7.584035110473633\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] Epoch[69] Batch[0] avg_epoch_loss=7.624057\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=7.624056816101074\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:18 INFO 139830709036864] Epoch[69] Batch[5] avg_epoch_loss=7.435710\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=7.435709794362386\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:18 INFO 139830709036864] Epoch[69] Batch [5]#011Speed: 212.57 samples/sec#011loss=7.435710\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] Epoch[69] Batch[10] avg_epoch_loss=7.535558\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=7.655376148223877\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] Epoch[69] Batch [10]#011Speed: 212.42 samples/sec#011loss=7.655376\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172656.3559928, \"EndTime\": 1663172659.9904828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3633.999824523926, \"count\": 1, \"min\": 3633.999824523926, \"max\": 3633.999824523926}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.31654829794738 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=69, train loss <loss>=7.5355581370267\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:19 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:20 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_c6d43bfd-68b4-4d55-a0b7-3a68076a8071-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172659.990568, \"EndTime\": 1663172660.070784, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 79.61034774780273, \"count\": 1, \"min\": 79.61034774780273, \"max\": 79.61034774780273}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:20 INFO 139830709036864] Epoch[70] Batch[0] avg_epoch_loss=7.481985\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=7.481985092163086\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:22 INFO 139830709036864] Epoch[70] Batch[5] avg_epoch_loss=7.570907\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=7.570906559626262\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:22 INFO 139830709036864] Epoch[70] Batch [5]#011Speed: 219.15 samples/sec#011loss=7.570907\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:23 INFO 139830709036864] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172660.0708807, \"EndTime\": 1663172663.3660645, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3295.109987258911, \"count\": 1, \"min\": 3295.109987258911, \"max\": 3295.109987258911}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:23 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.00465375534927 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:23 INFO 139830709036864] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=70, train loss <loss>=7.5818572521209715\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:23 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:24 INFO 139830709036864] Epoch[71] Batch[0] avg_epoch_loss=7.488380\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=7.488380432128906\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:25 INFO 139830709036864] Epoch[71] Batch[5] avg_epoch_loss=7.522750\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=7.522749662399292\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:25 INFO 139830709036864] Epoch[71] Batch [5]#011Speed: 222.17 samples/sec#011loss=7.522750\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] Epoch[71] Batch[10] avg_epoch_loss=7.541166\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=7.5632647514343265\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] Epoch[71] Batch [10]#011Speed: 213.17 samples/sec#011loss=7.563265\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172663.3661695, \"EndTime\": 1663172666.956684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3589.8990631103516, \"count\": 1, \"min\": 3589.8990631103516, \"max\": 3589.8990631103516}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.72744228015165 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=71, train loss <loss>=7.541165611960671\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:26 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:27 INFO 139830709036864] Epoch[72] Batch[0] avg_epoch_loss=7.334085\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=7.334085464477539\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:29 INFO 139830709036864] Epoch[72] Batch[5] avg_epoch_loss=7.565987\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=7.56598695119222\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:29 INFO 139830709036864] Epoch[72] Batch [5]#011Speed: 212.41 samples/sec#011loss=7.565987\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] Epoch[72] Batch[10] avg_epoch_loss=7.533637\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=7.494817924499512\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] Epoch[72] Batch [10]#011Speed: 212.23 samples/sec#011loss=7.494818\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172666.957019, \"EndTime\": 1663172670.6455908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3687.8061294555664, \"count\": 1, \"min\": 3687.8061294555664, \"max\": 3687.8061294555664}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=174.89487725152438 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=72, train loss <loss>=7.533637393604625\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:30 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_2727044f-ae2c-4438-ac0c-90b3fa0c4c31-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172670.6456718, \"EndTime\": 1663172670.7165735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 70.31893730163574, \"count\": 1, \"min\": 70.31893730163574, \"max\": 70.31893730163574}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:31 INFO 139830709036864] Epoch[73] Batch[0] avg_epoch_loss=7.514066\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=7.514065742492676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:32 INFO 139830709036864] Epoch[73] Batch[5] avg_epoch_loss=7.483476\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=7.483476003011067\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:32 INFO 139830709036864] Epoch[73] Batch [5]#011Speed: 224.19 samples/sec#011loss=7.483476\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:33 INFO 139830709036864] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172670.7166495, \"EndTime\": 1663172673.9970574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3280.336618423462, \"count\": 1, \"min\": 3280.336618423462, \"max\": 3280.336618423462}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:33 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.33982719551045 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:33 INFO 139830709036864] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=73, train loss <loss>=7.491155910491943\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:33 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:34 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_4d0cb2bf-2b03-4a37-b58f-d2dba23144b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172673.9971416, \"EndTime\": 1663172674.101361, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.71661186218262, \"count\": 1, \"min\": 103.71661186218262, \"max\": 103.71661186218262}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:34 INFO 139830709036864] Epoch[74] Batch[0] avg_epoch_loss=7.469277\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=7.469277381896973\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:36 INFO 139830709036864] Epoch[74] Batch[5] avg_epoch_loss=7.557147\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=7.557147264480591\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:36 INFO 139830709036864] Epoch[74] Batch [5]#011Speed: 212.21 samples/sec#011loss=7.557147\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:37 INFO 139830709036864] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172674.1014478, \"EndTime\": 1663172677.4375193, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3336.0049724578857, \"count\": 1, \"min\": 3336.0049724578857, \"max\": 3336.0049724578857}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:37 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.7429407125336 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:37 INFO 139830709036864] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=74, train loss <loss>=7.506667995452881\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:37 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:38 INFO 139830709036864] Epoch[75] Batch[0] avg_epoch_loss=7.488448\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=7.488448143005371\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:39 INFO 139830709036864] Epoch[75] Batch[5] avg_epoch_loss=7.542011\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=7.542011022567749\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:39 INFO 139830709036864] Epoch[75] Batch [5]#011Speed: 214.44 samples/sec#011loss=7.542011\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] Epoch[75] Batch[10] avg_epoch_loss=7.556191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=7.573207569122315\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] Epoch[75] Batch [10]#011Speed: 213.11 samples/sec#011loss=7.573208\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172677.4376132, \"EndTime\": 1663172681.1021893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3664.1058921813965, \"count\": 1, \"min\": 3664.1058921813965, \"max\": 3664.1058921813965}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.12248559226506 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=75, train loss <loss>=7.556191271001643\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] Epoch[76] Batch[0] avg_epoch_loss=7.600861\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=7.600861072540283\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:43 INFO 139830709036864] Epoch[76] Batch[5] avg_epoch_loss=7.508089\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=7.5080885887146\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:43 INFO 139830709036864] Epoch[76] Batch [5]#011Speed: 214.20 samples/sec#011loss=7.508089\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:44 INFO 139830709036864] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172681.1022625, \"EndTime\": 1663172684.452238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3349.4927883148193, \"count\": 1, \"min\": 3349.4927883148193, \"max\": 3349.4927883148193}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:44 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.2936580160804 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:44 INFO 139830709036864] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=76, train loss <loss>=7.520621681213379\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:44 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:45 INFO 139830709036864] Epoch[77] Batch[0] avg_epoch_loss=7.530699\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=7.530699253082275\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:46 INFO 139830709036864] Epoch[77] Batch[5] avg_epoch_loss=7.459608\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=7.4596076011657715\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:46 INFO 139830709036864] Epoch[77] Batch [5]#011Speed: 216.42 samples/sec#011loss=7.459608\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172684.4523695, \"EndTime\": 1663172687.7890592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3335.9525203704834, \"count\": 1, \"min\": 3335.9525203704834, \"max\": 3335.9525203704834}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.25270205428592 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=77, train loss <loss>=7.4453342914581295\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:47 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_f2088f20-432a-4d37-b167-d8733fa0e2cc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172687.7891378, \"EndTime\": 1663172687.8884327, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 98.71125221252441, \"count\": 1, \"min\": 98.71125221252441, \"max\": 98.71125221252441}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:48 INFO 139830709036864] Epoch[78] Batch[0] avg_epoch_loss=7.674996\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=7.6749958992004395\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:49 INFO 139830709036864] Epoch[78] Batch[5] avg_epoch_loss=7.546014\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=7.546013911565145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:49 INFO 139830709036864] Epoch[78] Batch [5]#011Speed: 236.68 samples/sec#011loss=7.546014\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] Epoch[78] Batch[10] avg_epoch_loss=7.562128\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=7.581464385986328\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] Epoch[78] Batch [10]#011Speed: 216.19 samples/sec#011loss=7.581464\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172687.8885062, \"EndTime\": 1663172691.3750763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3486.5078926086426, \"count\": 1, \"min\": 3486.5078926086426, \"max\": 3486.5078926086426}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.279146388913 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=78, train loss <loss>=7.5621277635747735\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:51 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:52 INFO 139830709036864] Epoch[79] Batch[0] avg_epoch_loss=7.413379\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=7.413378715515137\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:53 INFO 139830709036864] Epoch[79] Batch[5] avg_epoch_loss=7.461383\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=7.4613831837972\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:53 INFO 139830709036864] Epoch[79] Batch [5]#011Speed: 212.69 samples/sec#011loss=7.461383\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] Epoch[79] Batch[10] avg_epoch_loss=7.566620\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=7.692904758453369\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] Epoch[79] Batch [10]#011Speed: 209.32 samples/sec#011loss=7.692905\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172691.3751638, \"EndTime\": 1663172695.05157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3675.6579875946045, \"count\": 1, \"min\": 3675.6579875946045, \"max\": 3675.6579875946045}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.91416778386235 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=79, train loss <loss>=7.566620263186368\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] Epoch[80] Batch[0] avg_epoch_loss=7.389602\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=7.389602184295654\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:57 INFO 139830709036864] Epoch[80] Batch[5] avg_epoch_loss=7.435055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=7.4350550174713135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:57 INFO 139830709036864] Epoch[80] Batch [5]#011Speed: 229.93 samples/sec#011loss=7.435055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] Epoch[80] Batch[10] avg_epoch_loss=7.511668\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=7.60360279083252\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] Epoch[80] Batch [10]#011Speed: 213.29 samples/sec#011loss=7.603603\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172695.0516496, \"EndTime\": 1663172698.5491977, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3497.0474243164062, \"count\": 1, \"min\": 3497.0474243164062, \"max\": 3497.0474243164062}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.58665218545235 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=80, train loss <loss>=7.511667641726407\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:58 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:59 INFO 139830709036864] Epoch[81] Batch[0] avg_epoch_loss=7.524211\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:24:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=7.524211406707764\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:00 INFO 139830709036864] Epoch[81] Batch[5] avg_epoch_loss=7.499105\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=7.499105056126912\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:00 INFO 139830709036864] Epoch[81] Batch [5]#011Speed: 209.44 samples/sec#011loss=7.499105\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:01 INFO 139830709036864] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172698.549283, \"EndTime\": 1663172701.9197671, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3370.0761795043945, \"count\": 1, \"min\": 3370.0761795043945, \"max\": 3370.0761795043945}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:01 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.85572496046515 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:01 INFO 139830709036864] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=81, train loss <loss>=7.514064645767212\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:01 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:02 INFO 139830709036864] Epoch[82] Batch[0] avg_epoch_loss=7.496273\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=7.496273040771484\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:04 INFO 139830709036864] Epoch[82] Batch[5] avg_epoch_loss=7.419744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=7.419743617375691\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:04 INFO 139830709036864] Epoch[82] Batch [5]#011Speed: 208.53 samples/sec#011loss=7.419744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] Epoch[82] Batch[10] avg_epoch_loss=7.498482\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=7.5929670333862305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] Epoch[82] Batch [10]#011Speed: 204.38 samples/sec#011loss=7.592967\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172701.9198487, \"EndTime\": 1663172705.646604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3726.2744903564453, \"count\": 1, \"min\": 3726.2744903564453, \"max\": 3726.2744903564453}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=175.23621368592475 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=82, train loss <loss>=7.498481533744118\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:05 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:06 INFO 139830709036864] Epoch[83] Batch[0] avg_epoch_loss=7.556422\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=7.556421756744385\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:07 INFO 139830709036864] Epoch[83] Batch[5] avg_epoch_loss=7.479436\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=7.47943639755249\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:07 INFO 139830709036864] Epoch[83] Batch [5]#011Speed: 236.54 samples/sec#011loss=7.479436\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] Epoch[83] Batch[10] avg_epoch_loss=7.258096\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=6.992488193511963\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] Epoch[83] Batch [10]#011Speed: 216.25 samples/sec#011loss=6.992488\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172705.6466913, \"EndTime\": 1663172709.095606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3448.0385780334473, \"count\": 1, \"min\": 3448.0385780334473, \"max\": 3448.0385780334473}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.95125085200036 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=83, train loss <loss>=7.258096304806796\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_82340094-62df-4a7d-b68f-31eabbea3a8a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172709.095776, \"EndTime\": 1663172709.178172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 81.73012733459473, \"count\": 1, \"min\": 81.73012733459473, \"max\": 81.73012733459473}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] Epoch[84] Batch[0] avg_epoch_loss=7.414524\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=7.414524078369141\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:11 INFO 139830709036864] Epoch[84] Batch[5] avg_epoch_loss=7.406610\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=7.406609773635864\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:11 INFO 139830709036864] Epoch[84] Batch [5]#011Speed: 213.48 samples/sec#011loss=7.406610\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:12 INFO 139830709036864] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172709.1782644, \"EndTime\": 1663172712.513406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3335.0744247436523, \"count\": 1, \"min\": 3335.0744247436523, \"max\": 3335.0744247436523}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:12 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.49679162952108 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:12 INFO 139830709036864] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=84, train loss <loss>=7.459721612930298\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:12 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:13 INFO 139830709036864] Epoch[85] Batch[0] avg_epoch_loss=7.413301\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=7.413301467895508\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:14 INFO 139830709036864] Epoch[85] Batch[5] avg_epoch_loss=7.437451\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=7.437451124191284\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:14 INFO 139830709036864] Epoch[85] Batch [5]#011Speed: 212.98 samples/sec#011loss=7.437451\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] Epoch[85] Batch[10] avg_epoch_loss=7.410329\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=7.377782821655273\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] Epoch[85] Batch [10]#011Speed: 220.53 samples/sec#011loss=7.377783\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172712.5135012, \"EndTime\": 1663172716.1396258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3625.6141662597656, \"count\": 1, \"min\": 3625.6141662597656, \"max\": 3625.6141662597656}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.962177555016 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=85, train loss <loss>=7.410329168493098\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] Epoch[86] Batch[0] avg_epoch_loss=7.600268\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=7.6002678871154785\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:18 INFO 139830709036864] Epoch[86] Batch[5] avg_epoch_loss=7.525185\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=7.525185028711955\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:18 INFO 139830709036864] Epoch[86] Batch [5]#011Speed: 223.88 samples/sec#011loss=7.525185\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] Epoch[86] Batch[10] avg_epoch_loss=7.512218\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=7.496658420562744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] Epoch[86] Batch [10]#011Speed: 220.85 samples/sec#011loss=7.496658\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172716.139717, \"EndTime\": 1663172719.64949, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3509.272575378418, \"count\": 1, \"min\": 3509.272575378418, \"max\": 3509.272575378418}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.79281865001187 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=86, train loss <loss>=7.512218388644132\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:19 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:20 INFO 139830709036864] Epoch[87] Batch[0] avg_epoch_loss=7.545498\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=7.545498371124268\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:21 INFO 139830709036864] Epoch[87] Batch[5] avg_epoch_loss=7.440545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=7.440544605255127\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:21 INFO 139830709036864] Epoch[87] Batch [5]#011Speed: 224.53 samples/sec#011loss=7.440545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:22 INFO 139830709036864] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172719.6495638, \"EndTime\": 1663172722.888848, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3238.8551235198975, \"count\": 1, \"min\": 3238.8551235198975, \"max\": 3238.8551235198975}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:22 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.28342655521996 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:22 INFO 139830709036864] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=87, train loss <loss>=7.4263163089752195\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:22 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:23 INFO 139830709036864] Epoch[88] Batch[0] avg_epoch_loss=7.210216\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=7.210216045379639\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:25 INFO 139830709036864] Epoch[88] Batch[5] avg_epoch_loss=7.418961\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=7.418961366017659\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:25 INFO 139830709036864] Epoch[88] Batch [5]#011Speed: 211.87 samples/sec#011loss=7.418961\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172722.8889437, \"EndTime\": 1663172726.2126296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3323.133707046509, \"count\": 1, \"min\": 3323.133707046509, \"max\": 3323.133707046509}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.35353229988306 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=88, train loss <loss>=7.438539791107178\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] Epoch[89] Batch[0] avg_epoch_loss=7.505658\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=7.505658149719238\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:28 INFO 139830709036864] Epoch[89] Batch[5] avg_epoch_loss=7.449228\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=7.449227809906006\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:28 INFO 139830709036864] Epoch[89] Batch [5]#011Speed: 222.90 samples/sec#011loss=7.449228\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:29 INFO 139830709036864] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172726.2128325, \"EndTime\": 1663172729.4889646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3275.654077529907, \"count\": 1, \"min\": 3275.654077529907, \"max\": 3275.654077529907}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:29 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.43585396074192 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:29 INFO 139830709036864] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=89, train loss <loss>=7.454714441299439\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:29 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:30 INFO 139830709036864] Epoch[90] Batch[0] avg_epoch_loss=7.337136\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=7.337136268615723\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:31 INFO 139830709036864] Epoch[90] Batch[5] avg_epoch_loss=7.407128\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=7.4071275393168134\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:31 INFO 139830709036864] Epoch[90] Batch [5]#011Speed: 224.51 samples/sec#011loss=7.407128\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:32 INFO 139830709036864] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172729.4890597, \"EndTime\": 1663172732.7398849, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3250.3809928894043, \"count\": 1, \"min\": 3250.3809928894043, \"max\": 3250.3809928894043}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:32 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.66253147165207 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:32 INFO 139830709036864] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=90, train loss <loss>=7.4015624046325685\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:32 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:33 INFO 139830709036864] Epoch[91] Batch[0] avg_epoch_loss=7.487899\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=7.487898826599121\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:34 INFO 139830709036864] Epoch[91] Batch[5] avg_epoch_loss=7.431502\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=7.431501547495524\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:34 INFO 139830709036864] Epoch[91] Batch [5]#011Speed: 229.35 samples/sec#011loss=7.431502\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:35 INFO 139830709036864] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172732.7399871, \"EndTime\": 1663172735.9126205, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3172.056198120117, \"count\": 1, \"min\": 3172.056198120117, \"max\": 3172.056198120117}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.61164816864013 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=91, train loss <loss>=7.456924629211426\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:36 INFO 139830709036864] Epoch[92] Batch[0] avg_epoch_loss=7.307192\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=7.307192325592041\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:37 INFO 139830709036864] Epoch[92] Batch[5] avg_epoch_loss=7.463826\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=7.463825623194377\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:37 INFO 139830709036864] Epoch[92] Batch [5]#011Speed: 223.40 samples/sec#011loss=7.463826\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] Epoch[92] Batch[10] avg_epoch_loss=7.124942\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=6.71828064918518\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] Epoch[92] Batch [10]#011Speed: 208.75 samples/sec#011loss=6.718281\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172735.9127119, \"EndTime\": 1663172739.4859529, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3572.643995285034, \"count\": 1, \"min\": 3572.643995285034, \"max\": 3572.643995285034}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.97268462475793 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=92, train loss <loss>=7.124941544099287\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:39 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_00e41e91-9956-46ae-ab13-1a55c012d27e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172739.4860368, \"EndTime\": 1663172739.5894995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.00898551940918, \"count\": 1, \"min\": 103.00898551940918, \"max\": 103.00898551940918}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:40 INFO 139830709036864] Epoch[93] Batch[0] avg_epoch_loss=7.491554\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=7.491553783416748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:41 INFO 139830709036864] Epoch[93] Batch[5] avg_epoch_loss=7.422204\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=7.42220401763916\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:41 INFO 139830709036864] Epoch[93] Batch [5]#011Speed: 216.78 samples/sec#011loss=7.422204\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:42 INFO 139830709036864] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172739.5895765, \"EndTime\": 1663172742.88582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3296.1838245391846, \"count\": 1, \"min\": 3296.1838245391846, \"max\": 3296.1838245391846}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.94363518559157 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=93, train loss <loss>=7.356595420837403\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:42 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:43 INFO 139830709036864] Epoch[94] Batch[0] avg_epoch_loss=7.338215\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=7.338215351104736\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:44 INFO 139830709036864] Epoch[94] Batch[5] avg_epoch_loss=7.433355\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=7.433355093002319\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:44 INFO 139830709036864] Epoch[94] Batch [5]#011Speed: 228.12 samples/sec#011loss=7.433355\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] Epoch[94] Batch[10] avg_epoch_loss=7.448517\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=7.46671199798584\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] Epoch[94] Batch [10]#011Speed: 216.64 samples/sec#011loss=7.466712\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172742.885904, \"EndTime\": 1663172746.3773205, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3490.8761978149414, \"count\": 1, \"min\": 3490.8761978149414, \"max\": 3490.8761978149414}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.78658430811083 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=94, train loss <loss>=7.448517322540283\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:46 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:47 INFO 139830709036864] Epoch[95] Batch[0] avg_epoch_loss=7.293775\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=7.293774604797363\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:48 INFO 139830709036864] Epoch[95] Batch[5] avg_epoch_loss=7.370212\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=7.370211760203044\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:48 INFO 139830709036864] Epoch[95] Batch [5]#011Speed: 218.20 samples/sec#011loss=7.370212\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:49 INFO 139830709036864] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172746.377403, \"EndTime\": 1663172749.6715364, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3293.679714202881, \"count\": 1, \"min\": 3293.679714202881, \"max\": 3293.679714202881}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.2317760949189 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=95, train loss <loss>=7.376613903045654\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:50 INFO 139830709036864] Epoch[96] Batch[0] avg_epoch_loss=7.485833\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=7.485833168029785\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:51 INFO 139830709036864] Epoch[96] Batch[5] avg_epoch_loss=7.366273\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=7.366272846857707\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:51 INFO 139830709036864] Epoch[96] Batch [5]#011Speed: 211.31 samples/sec#011loss=7.366273\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172749.6716306, \"EndTime\": 1663172753.05823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3386.1124515533447, \"count\": 1, \"min\": 3386.1124515533447, \"max\": 3386.1124515533447}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.73186308754433 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=96, train loss <loss>=7.390703010559082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] Epoch[97] Batch[0] avg_epoch_loss=7.517479\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=7.517479419708252\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:55 INFO 139830709036864] Epoch[97] Batch[5] avg_epoch_loss=7.454300\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=7.4542999267578125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:55 INFO 139830709036864] Epoch[97] Batch [5]#011Speed: 210.82 samples/sec#011loss=7.454300\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:56 INFO 139830709036864] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172753.058312, \"EndTime\": 1663172756.4215124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3362.736701965332, \"count\": 1, \"min\": 3362.736701965332, \"max\": 3362.736701965332}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.93356589886483 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=97, train loss <loss>=7.427826595306397\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:57 INFO 139830709036864] Epoch[98] Batch[0] avg_epoch_loss=7.512976\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=7.512975692749023\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:58 INFO 139830709036864] Epoch[98] Batch[5] avg_epoch_loss=7.381495\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=7.381494760513306\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:25:58 INFO 139830709036864] Epoch[98] Batch [5]#011Speed: 215.45 samples/sec#011loss=7.381495\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] Epoch[98] Batch[10] avg_epoch_loss=7.390056\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=7.400328540802002\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] Epoch[98] Batch [10]#011Speed: 213.09 samples/sec#011loss=7.400329\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172756.4216263, \"EndTime\": 1663172760.0270412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3604.8285961151123, \"count\": 1, \"min\": 3604.8285961151123, \"max\": 3604.8285961151123}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.58575990587423 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=98, train loss <loss>=7.39005556973544\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] Epoch[99] Batch[0] avg_epoch_loss=7.417359\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=7.417358875274658\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:02 INFO 139830709036864] Epoch[99] Batch[5] avg_epoch_loss=7.442504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=7.442504167556763\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:02 INFO 139830709036864] Epoch[99] Batch [5]#011Speed: 213.24 samples/sec#011loss=7.442504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] Epoch[99] Batch[10] avg_epoch_loss=7.443191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=7.444015026092529\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] Epoch[99] Batch [10]#011Speed: 208.07 samples/sec#011loss=7.444015\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172760.027116, \"EndTime\": 1663172763.671677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3644.0556049346924, \"count\": 1, \"min\": 3644.0556049346924, \"max\": 3644.0556049346924}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.8365970498875 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=99, train loss <loss>=7.443190921436656\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:03 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:04 INFO 139830709036864] Epoch[100] Batch[0] avg_epoch_loss=7.267867\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=7.267867088317871\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:05 INFO 139830709036864] Epoch[100] Batch[5] avg_epoch_loss=7.344033\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=7.344033241271973\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:05 INFO 139830709036864] Epoch[100] Batch [5]#011Speed: 214.79 samples/sec#011loss=7.344033\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] Epoch[100] Batch[10] avg_epoch_loss=7.428886\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=7.530709648132325\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] Epoch[100] Batch [10]#011Speed: 207.60 samples/sec#011loss=7.530710\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172763.6717598, \"EndTime\": 1663172767.374726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3702.5370597839355, \"count\": 1, \"min\": 3702.5370597839355, \"max\": 3702.5370597839355}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.87110044619314 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=100, train loss <loss>=7.428886153481224\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:07 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:08 INFO 139830709036864] Epoch[101] Batch[0] avg_epoch_loss=7.403082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=7.403082370758057\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:09 INFO 139830709036864] Epoch[101] Batch[5] avg_epoch_loss=7.419431\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=7.419431289037068\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:09 INFO 139830709036864] Epoch[101] Batch [5]#011Speed: 230.58 samples/sec#011loss=7.419431\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] Epoch[101] Batch[10] avg_epoch_loss=7.407871\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=7.393997859954834\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] Epoch[101] Batch [10]#011Speed: 217.64 samples/sec#011loss=7.393998\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172767.3748038, \"EndTime\": 1663172770.8611426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3485.844135284424, \"count\": 1, \"min\": 3485.844135284424, \"max\": 3485.844135284424}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.03497209815626 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=101, train loss <loss>=7.407870639454234\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:10 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:11 INFO 139830709036864] Epoch[102] Batch[0] avg_epoch_loss=7.302063\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=7.30206298828125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:12 INFO 139830709036864] Epoch[102] Batch[5] avg_epoch_loss=7.307561\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=7.3075612386067705\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:12 INFO 139830709036864] Epoch[102] Batch [5]#011Speed: 225.98 samples/sec#011loss=7.307561\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172770.8612356, \"EndTime\": 1663172774.0178406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3156.0721397399902, \"count\": 1, \"min\": 3156.0721397399902, \"max\": 3156.0721397399902}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.07200139355442 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=102, train loss <loss>=7.330554676055908\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] Epoch[103] Batch[0] avg_epoch_loss=7.334827\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=7.334826946258545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:16 INFO 139830709036864] Epoch[103] Batch[5] avg_epoch_loss=7.322458\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=7.3224577109018965\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:16 INFO 139830709036864] Epoch[103] Batch [5]#011Speed: 219.55 samples/sec#011loss=7.322458\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] Epoch[103] Batch[10] avg_epoch_loss=7.376491\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=7.441331958770752\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] Epoch[103] Batch [10]#011Speed: 216.01 samples/sec#011loss=7.441332\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172774.0179346, \"EndTime\": 1663172777.6015964, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3583.015203475952, \"count\": 1, \"min\": 3583.015203475952, \"max\": 3583.015203475952}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.7543316270054 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=103, train loss <loss>=7.376491459933194\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:17 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:18 INFO 139830709036864] Epoch[104] Batch[0] avg_epoch_loss=7.222394\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=7.222393989562988\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:19 INFO 139830709036864] Epoch[104] Batch[5] avg_epoch_loss=7.342873\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=7.342873175938924\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:19 INFO 139830709036864] Epoch[104] Batch [5]#011Speed: 228.12 samples/sec#011loss=7.342873\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:20 INFO 139830709036864] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172777.601683, \"EndTime\": 1663172780.7974155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3195.310592651367, \"count\": 1, \"min\": 3195.310592651367, \"max\": 3195.310592651367}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:20 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.5303746094515 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:20 INFO 139830709036864] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=104, train loss <loss>=7.361433124542236\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:20 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:21 INFO 139830709036864] Epoch[105] Batch[0] avg_epoch_loss=7.381451\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=7.38145112991333\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:22 INFO 139830709036864] Epoch[105] Batch[5] avg_epoch_loss=7.309637\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=7.309637149175008\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:22 INFO 139830709036864] Epoch[105] Batch [5]#011Speed: 219.53 samples/sec#011loss=7.309637\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172780.7974966, \"EndTime\": 1663172784.0915942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3293.33758354187, \"count\": 1, \"min\": 3293.33758354187, \"max\": 3293.33758354187}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.41354996537152 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=105, train loss <loss>=7.317074632644653\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] Epoch[106] Batch[0] avg_epoch_loss=7.379404\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=7.379404067993164\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:26 INFO 139830709036864] Epoch[106] Batch[5] avg_epoch_loss=7.347507\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=7.347506682078044\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:26 INFO 139830709036864] Epoch[106] Batch [5]#011Speed: 211.40 samples/sec#011loss=7.347507\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:27 INFO 139830709036864] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172784.0916798, \"EndTime\": 1663172787.4797592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3387.6020908355713, \"count\": 1, \"min\": 3387.6020908355713, \"max\": 3387.6020908355713}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:27 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.32649106898108 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:27 INFO 139830709036864] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=106, train loss <loss>=7.341216564178467\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:27 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:28 INFO 139830709036864] Epoch[107] Batch[0] avg_epoch_loss=7.296531\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=7.2965312004089355\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:29 INFO 139830709036864] Epoch[107] Batch[5] avg_epoch_loss=7.295869\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=7.295869032541911\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:29 INFO 139830709036864] Epoch[107] Batch [5]#011Speed: 231.05 samples/sec#011loss=7.295869\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] Epoch[107] Batch[10] avg_epoch_loss=7.303211\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=7.312020587921142\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] Epoch[107] Batch [10]#011Speed: 217.16 samples/sec#011loss=7.312021\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172787.4798515, \"EndTime\": 1663172790.954024, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3473.724126815796, \"count\": 1, \"min\": 3473.724126815796, \"max\": 3473.724126815796}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.97612139271195 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=107, train loss <loss>=7.30321064862338\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:30 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:31 INFO 139830709036864] Epoch[108] Batch[0] avg_epoch_loss=7.230755\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=7.230754852294922\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:32 INFO 139830709036864] Epoch[108] Batch[5] avg_epoch_loss=7.337534\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=7.337534268697103\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:32 INFO 139830709036864] Epoch[108] Batch [5]#011Speed: 227.16 samples/sec#011loss=7.337534\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172790.954108, \"EndTime\": 1663172794.207167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3252.629280090332, \"count\": 1, \"min\": 3252.629280090332, \"max\": 3252.629280090332}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.0650035563525 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=108, train loss <loss>=7.315160465240479\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] Epoch[109] Batch[0] avg_epoch_loss=7.327415\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=7.327414512634277\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:36 INFO 139830709036864] Epoch[109] Batch[5] avg_epoch_loss=7.314746\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=7.314746459325154\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:36 INFO 139830709036864] Epoch[109] Batch [5]#011Speed: 216.54 samples/sec#011loss=7.314746\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] Epoch[109] Batch[10] avg_epoch_loss=7.313729\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=7.3125073432922365\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] Epoch[109] Batch [10]#011Speed: 210.85 samples/sec#011loss=7.312507\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172794.2072878, \"EndTime\": 1663172797.8254116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3617.570400238037, \"count\": 1, \"min\": 3617.570400238037, \"max\": 3617.570400238037}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=177.18396198281891 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=109, train loss <loss>=7.313728679310191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:37 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:38 INFO 139830709036864] Epoch[110] Batch[0] avg_epoch_loss=7.181063\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=7.181062698364258\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:39 INFO 139830709036864] Epoch[110] Batch[5] avg_epoch_loss=7.350423\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=7.350423415501912\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:39 INFO 139830709036864] Epoch[110] Batch [5]#011Speed: 216.08 samples/sec#011loss=7.350423\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] Epoch[110] Batch[10] avg_epoch_loss=7.326440\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=7.297659111022949\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] Epoch[110] Batch [10]#011Speed: 220.55 samples/sec#011loss=7.297659\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172797.8255072, \"EndTime\": 1663172801.3756096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3549.5636463165283, \"count\": 1, \"min\": 3549.5636463165283, \"max\": 3549.5636463165283}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.56535456721687 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=110, train loss <loss>=7.326439640738747\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] Epoch[111] Batch[0] avg_epoch_loss=7.042868\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=7.042868137359619\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:43 INFO 139830709036864] Epoch[111] Batch[5] avg_epoch_loss=7.172780\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=7.1727800369262695\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:43 INFO 139830709036864] Epoch[111] Batch [5]#011Speed: 221.50 samples/sec#011loss=7.172780\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:44 INFO 139830709036864] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172801.375707, \"EndTime\": 1663172804.6307228, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3254.4264793395996, \"count\": 1, \"min\": 3254.4264793395996, \"max\": 3254.4264793395996}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:44 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.34073923435793 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:44 INFO 139830709036864] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=111, train loss <loss>=7.219316101074218\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:44 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:45 INFO 139830709036864] Epoch[112] Batch[0] avg_epoch_loss=7.176834\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=7.1768341064453125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:46 INFO 139830709036864] Epoch[112] Batch[5] avg_epoch_loss=7.238017\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=7.238016605377197\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:46 INFO 139830709036864] Epoch[112] Batch [5]#011Speed: 213.20 samples/sec#011loss=7.238017\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:47 INFO 139830709036864] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172804.6308033, \"EndTime\": 1663172807.9652174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3333.9083194732666, \"count\": 1, \"min\": 3333.9083194732666, \"max\": 3333.9083194732666}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:47 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.4579458633205 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:47 INFO 139830709036864] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=112, train loss <loss>=7.234643125534058\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:47 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:48 INFO 139830709036864] Epoch[113] Batch[0] avg_epoch_loss=7.287022\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=7.287021636962891\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:50 INFO 139830709036864] Epoch[113] Batch[5] avg_epoch_loss=7.272953\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=7.272952556610107\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:50 INFO 139830709036864] Epoch[113] Batch [5]#011Speed: 212.30 samples/sec#011loss=7.272953\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172807.96538, \"EndTime\": 1663172811.3145247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3348.531723022461, \"count\": 1, \"min\": 3348.531723022461, \"max\": 3348.531723022461}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.83511200722174 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=113, train loss <loss>=7.247667551040649\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] Epoch[114] Batch[0] avg_epoch_loss=7.386736\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=7.3867363929748535\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:53 INFO 139830709036864] Epoch[114] Batch[5] avg_epoch_loss=7.269572\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=7.269572337468465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:53 INFO 139830709036864] Epoch[114] Batch [5]#011Speed: 228.64 samples/sec#011loss=7.269572\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:54 INFO 139830709036864] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172811.3146317, \"EndTime\": 1663172814.4954515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3180.340528488159, \"count\": 1, \"min\": 3180.340528488159, \"max\": 3180.340528488159}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:54 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=199.02765095829204 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:54 INFO 139830709036864] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=114, train loss <loss>=7.2582086563110355\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:54 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:55 INFO 139830709036864] Epoch[115] Batch[0] avg_epoch_loss=7.398054\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=7.398054122924805\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:56 INFO 139830709036864] Epoch[115] Batch[5] avg_epoch_loss=7.313037\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=7.313037395477295\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:56 INFO 139830709036864] Epoch[115] Batch [5]#011Speed: 229.81 samples/sec#011loss=7.313037\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] Epoch[115] Batch[10] avg_epoch_loss=7.457334\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=7.630490112304687\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] Epoch[115] Batch [10]#011Speed: 214.21 samples/sec#011loss=7.630490\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172814.4955368, \"EndTime\": 1663172817.9878948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3491.852283477783, \"count\": 1, \"min\": 3491.852283477783, \"max\": 3491.852283477783}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.99575758999214 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=115, train loss <loss>=7.457334084944292\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:57 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:58 INFO 139830709036864] Epoch[116] Batch[0] avg_epoch_loss=7.428586\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:26:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=7.428586006164551\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:00 INFO 139830709036864] Epoch[116] Batch[5] avg_epoch_loss=7.392744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=7.3927444616953535\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:00 INFO 139830709036864] Epoch[116] Batch [5]#011Speed: 219.32 samples/sec#011loss=7.392744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] Epoch[116] Batch[10] avg_epoch_loss=7.427409\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=7.469006729125977\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] Epoch[116] Batch [10]#011Speed: 204.29 samples/sec#011loss=7.469007\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172817.987978, \"EndTime\": 1663172821.6440332, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3655.6243896484375, \"count\": 1, \"min\": 3655.6243896484375, \"max\": 3655.6243896484375}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.07488924009047 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=116, train loss <loss>=7.4274091287092725\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:01 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:02 INFO 139830709036864] Epoch[117] Batch[0] avg_epoch_loss=7.418943\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=7.418942928314209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:03 INFO 139830709036864] Epoch[117] Batch[5] avg_epoch_loss=7.340476\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=7.340475877126058\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:03 INFO 139830709036864] Epoch[117] Batch [5]#011Speed: 215.23 samples/sec#011loss=7.340476\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172821.6441336, \"EndTime\": 1663172825.0340526, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3389.369010925293, \"count\": 1, \"min\": 3389.369010925293, \"max\": 3389.369010925293}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.9172607475832 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=117, train loss <loss>=7.329168605804443\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] Epoch[118] Batch[0] avg_epoch_loss=7.467724\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=7.467723846435547\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:07 INFO 139830709036864] Epoch[118] Batch[5] avg_epoch_loss=7.346979\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=7.346979300181071\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:07 INFO 139830709036864] Epoch[118] Batch [5]#011Speed: 205.84 samples/sec#011loss=7.346979\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:08 INFO 139830709036864] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172825.0341504, \"EndTime\": 1663172828.5116389, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3476.903200149536, \"count\": 1, \"min\": 3476.903200149536, \"max\": 3476.903200149536}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:08 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=171.98603647111372 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:08 INFO 139830709036864] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=118, train loss <loss>=7.362558221817016\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:08 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:09 INFO 139830709036864] Epoch[119] Batch[0] avg_epoch_loss=7.223287\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=7.223287105560303\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:10 INFO 139830709036864] Epoch[119] Batch[5] avg_epoch_loss=7.203276\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=7.203275760014852\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:10 INFO 139830709036864] Epoch[119] Batch [5]#011Speed: 221.88 samples/sec#011loss=7.203276\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:11 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172828.5117247, \"EndTime\": 1663172831.7619107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3249.736547470093, \"count\": 1, \"min\": 3249.736547470093, \"max\": 3249.736547470093}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:11 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.00821045275237 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:11 INFO 139830709036864] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=119, train loss <loss>=7.25373911857605\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:11 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:12 INFO 139830709036864] Epoch[120] Batch[0] avg_epoch_loss=7.391869\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=7.391868591308594\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:13 INFO 139830709036864] Epoch[120] Batch[5] avg_epoch_loss=7.315643\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=7.315643310546875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:13 INFO 139830709036864] Epoch[120] Batch [5]#011Speed: 218.81 samples/sec#011loss=7.315643\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:14 INFO 139830709036864] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172831.7619963, \"EndTime\": 1663172834.9853919, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3222.834825515747, \"count\": 1, \"min\": 3222.834825515747, \"max\": 3222.834825515747}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:14 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.4742695234994 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:14 INFO 139830709036864] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=120, train loss <loss>=7.316684627532959\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:14 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:15 INFO 139830709036864] Epoch[121] Batch[0] avg_epoch_loss=7.182459\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=7.182459354400635\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:17 INFO 139830709036864] Epoch[121] Batch[5] avg_epoch_loss=7.254159\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=7.254158894220988\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:17 INFO 139830709036864] Epoch[121] Batch [5]#011Speed: 215.44 samples/sec#011loss=7.254159\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172834.9854825, \"EndTime\": 1663172838.2852924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3299.3476390838623, \"count\": 1, \"min\": 3299.3476390838623, \"max\": 3299.3476390838623}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.69503537079723 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=121, train loss <loss>=7.274156856536865\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] Epoch[122] Batch[0] avg_epoch_loss=7.116991\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=7.11699104309082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:20 INFO 139830709036864] Epoch[122] Batch[5] avg_epoch_loss=7.206060\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=7.2060597737630205\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:20 INFO 139830709036864] Epoch[122] Batch [5]#011Speed: 239.25 samples/sec#011loss=7.206060\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:21 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172838.285405, \"EndTime\": 1663172841.4213462, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3135.490655899048, \"count\": 1, \"min\": 3135.490655899048, \"max\": 3135.490655899048}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:21 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=203.14950062543002 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:21 INFO 139830709036864] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=122, train loss <loss>=7.258546495437622\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:21 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:22 INFO 139830709036864] Epoch[123] Batch[0] avg_epoch_loss=7.178809\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=7.178808689117432\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:23 INFO 139830709036864] Epoch[123] Batch[5] avg_epoch_loss=7.228220\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=7.228220383326213\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:23 INFO 139830709036864] Epoch[123] Batch [5]#011Speed: 212.92 samples/sec#011loss=7.228220\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] Epoch[123] Batch[10] avg_epoch_loss=7.236990\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=7.247514629364014\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] Epoch[123] Batch [10]#011Speed: 206.49 samples/sec#011loss=7.247515\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172841.4214396, \"EndTime\": 1663172845.1154194, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3693.5324668884277, \"count\": 1, \"min\": 3693.5324668884277, \"max\": 3693.5324668884277}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.30934490036987 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=123, train loss <loss>=7.236990495161577\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] Epoch[124] Batch[0] avg_epoch_loss=7.250818\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=7.250818252563477\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:27 INFO 139830709036864] Epoch[124] Batch[5] avg_epoch_loss=7.160362\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=7.160362323125203\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:27 INFO 139830709036864] Epoch[124] Batch [5]#011Speed: 223.08 samples/sec#011loss=7.160362\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172845.1155033, \"EndTime\": 1663172848.3891375, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3273.216485977173, \"count\": 1, \"min\": 3273.216485977173, \"max\": 3273.216485977173}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.9343059515691 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=124, train loss <loss>=7.192004251480102\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] Epoch[125] Batch[0] avg_epoch_loss=7.294297\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=7.294296741485596\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:30 INFO 139830709036864] Epoch[125] Batch[5] avg_epoch_loss=7.214274\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=7.214274485905965\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:30 INFO 139830709036864] Epoch[125] Batch [5]#011Speed: 237.02 samples/sec#011loss=7.214274\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] Epoch[125] Batch[10] avg_epoch_loss=7.218195\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=7.222899532318115\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] Epoch[125] Batch [10]#011Speed: 225.04 samples/sec#011loss=7.222900\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172848.3892174, \"EndTime\": 1663172851.7658257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3375.807046890259, \"count\": 1, \"min\": 3375.807046890259, \"max\": 3375.807046890259}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=196.6850522578593 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=125, train loss <loss>=7.218194961547852\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:31 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:32 INFO 139830709036864] Epoch[126] Batch[0] avg_epoch_loss=7.131561\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=7.131561279296875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:33 INFO 139830709036864] Epoch[126] Batch[5] avg_epoch_loss=7.162391\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=7.16239054997762\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:33 INFO 139830709036864] Epoch[126] Batch [5]#011Speed: 236.78 samples/sec#011loss=7.162391\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] Epoch[126] Batch[10] avg_epoch_loss=7.216968\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=7.282461166381836\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] Epoch[126] Batch [10]#011Speed: 217.46 samples/sec#011loss=7.282461\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172851.7659295, \"EndTime\": 1663172855.182053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3415.5426025390625, \"count\": 1, \"min\": 3415.5426025390625, \"max\": 3415.5426025390625}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.4697570145437 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=126, train loss <loss>=7.216968102888628\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] Epoch[127] Batch[0] avg_epoch_loss=7.182581\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=7.182581424713135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:37 INFO 139830709036864] Epoch[127] Batch[5] avg_epoch_loss=7.301816\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=7.301815589269002\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:37 INFO 139830709036864] Epoch[127] Batch [5]#011Speed: 225.41 samples/sec#011loss=7.301816\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] Epoch[127] Batch[10] avg_epoch_loss=7.300484\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=7.298886680603028\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] Epoch[127] Batch [10]#011Speed: 221.34 samples/sec#011loss=7.298887\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172855.1821492, \"EndTime\": 1663172858.6715324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3488.8393878936768, \"count\": 1, \"min\": 3488.8393878936768, \"max\": 3488.8393878936768}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.87492785526746 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=127, train loss <loss>=7.3004842671481045\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:38 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:39 INFO 139830709036864] Epoch[128] Batch[0] avg_epoch_loss=7.191734\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=7.1917338371276855\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:40 INFO 139830709036864] Epoch[128] Batch[5] avg_epoch_loss=7.134489\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=7.134488582611084\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:40 INFO 139830709036864] Epoch[128] Batch [5]#011Speed: 212.76 samples/sec#011loss=7.134489\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] Epoch[128] Batch[10] avg_epoch_loss=7.099906\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=7.058406829833984\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] Epoch[128] Batch [10]#011Speed: 207.52 samples/sec#011loss=7.058407\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172858.6716187, \"EndTime\": 1663172862.3467052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3674.530029296875, \"count\": 1, \"min\": 3674.530029296875, \"max\": 3674.530029296875}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.86799595110574 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=128, train loss <loss>=7.099905967712402\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:42 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_553bb0ba-703e-451c-a377-249f1b1138b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172862.3467884, \"EndTime\": 1663172862.4520361, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 104.75444793701172, \"count\": 1, \"min\": 104.75444793701172, \"max\": 104.75444793701172}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:43 INFO 139830709036864] Epoch[129] Batch[0] avg_epoch_loss=7.190360\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=7.190360069274902\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:44 INFO 139830709036864] Epoch[129] Batch[5] avg_epoch_loss=7.205246\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=7.205245892206828\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:44 INFO 139830709036864] Epoch[129] Batch [5]#011Speed: 233.80 samples/sec#011loss=7.205246\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] Epoch[129] Batch[10] avg_epoch_loss=7.312735\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=7.441721057891845\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] Epoch[129] Batch [10]#011Speed: 233.00 samples/sec#011loss=7.441721\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172862.4521248, \"EndTime\": 1663172865.8135545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3361.360788345337, \"count\": 1, \"min\": 3361.360788345337, \"max\": 3361.360788345337}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.66383008542653 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=129, train loss <loss>=7.312734603881836\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:45 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:46 INFO 139830709036864] Epoch[130] Batch[0] avg_epoch_loss=7.351243\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=7.351243495941162\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:47 INFO 139830709036864] Epoch[130] Batch[5] avg_epoch_loss=7.335086\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=7.335086186726888\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:47 INFO 139830709036864] Epoch[130] Batch [5]#011Speed: 220.09 samples/sec#011loss=7.335086\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172865.8136497, \"EndTime\": 1663172869.1291366, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3314.9657249450684, \"count\": 1, \"min\": 3314.9657249450684, \"max\": 3314.9657249450684}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.30891048720193 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=130, train loss <loss>=7.297416543960571\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] Epoch[131] Batch[0] avg_epoch_loss=7.212037\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=7.212037086486816\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:51 INFO 139830709036864] Epoch[131] Batch[5] avg_epoch_loss=7.207650\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=7.207650343577067\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:51 INFO 139830709036864] Epoch[131] Batch [5]#011Speed: 222.77 samples/sec#011loss=7.207650\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] Epoch[131] Batch[10] avg_epoch_loss=7.233735\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=7.265036487579346\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] Epoch[131] Batch [10]#011Speed: 206.30 samples/sec#011loss=7.265036\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172869.129218, \"EndTime\": 1663172872.7480159, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3618.353843688965, \"count\": 1, \"min\": 3618.353843688965, \"max\": 3618.353843688965}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.22631288896056 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=131, train loss <loss>=7.233734954487193\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:52 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:53 INFO 139830709036864] Epoch[132] Batch[0] avg_epoch_loss=7.183256\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=7.183256149291992\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:54 INFO 139830709036864] Epoch[132] Batch[5] avg_epoch_loss=7.256951\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=7.256951491038005\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:54 INFO 139830709036864] Epoch[132] Batch [5]#011Speed: 211.86 samples/sec#011loss=7.256951\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] Epoch[132] Batch[10] avg_epoch_loss=7.322593\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=7.401361751556396\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] Epoch[132] Batch [10]#011Speed: 211.50 samples/sec#011loss=7.401362\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172872.7481005, \"EndTime\": 1663172876.4119828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3663.4228229522705, \"count\": 1, \"min\": 3663.4228229522705, \"max\": 3663.4228229522705}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.52346318728087 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=132, train loss <loss>=7.322592518546364\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:57 INFO 139830709036864] Epoch[133] Batch[0] avg_epoch_loss=7.093293\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=7.093293190002441\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:58 INFO 139830709036864] Epoch[133] Batch[5] avg_epoch_loss=7.203847\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=7.203846534093221\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:27:58 INFO 139830709036864] Epoch[133] Batch [5]#011Speed: 217.62 samples/sec#011loss=7.203847\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] Epoch[133] Batch[10] avg_epoch_loss=7.190587\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=7.174675559997558\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] Epoch[133] Batch [10]#011Speed: 207.33 samples/sec#011loss=7.174676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172876.4120646, \"EndTime\": 1663172880.0324185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3619.9333667755127, \"count\": 1, \"min\": 3619.9333667755127, \"max\": 3619.9333667755127}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.11839352039217 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=133, train loss <loss>=7.190587000413374\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] Epoch[134] Batch[0] avg_epoch_loss=7.066972\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=7.066972255706787\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:02 INFO 139830709036864] Epoch[134] Batch[5] avg_epoch_loss=7.132038\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=7.132038036982219\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:02 INFO 139830709036864] Epoch[134] Batch [5]#011Speed: 217.20 samples/sec#011loss=7.132038\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172880.0325084, \"EndTime\": 1663172883.3453565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3312.3927116394043, \"count\": 1, \"min\": 3312.3927116394043, \"max\": 3312.3927116394043}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.39486261825567 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=134, train loss <loss>=7.131840991973877\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] Epoch[135] Batch[0] avg_epoch_loss=7.146398\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=7.146397590637207\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:05 INFO 139830709036864] Epoch[135] Batch[5] avg_epoch_loss=7.145528\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=7.145527601242065\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:05 INFO 139830709036864] Epoch[135] Batch [5]#011Speed: 208.24 samples/sec#011loss=7.145528\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] Epoch[135] Batch[10] avg_epoch_loss=7.106701\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=7.060108280181884\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] Epoch[135] Batch [10]#011Speed: 205.44 samples/sec#011loss=7.060108\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172883.3454492, \"EndTime\": 1663172887.0677125, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3721.79913520813, \"count\": 1, \"min\": 3721.79913520813, \"max\": 3721.79913520813}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.43328318007207 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=135, train loss <loss>=7.106700637123802\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] Epoch[136] Batch[0] avg_epoch_loss=7.158050\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=7.158050060272217\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:09 INFO 139830709036864] Epoch[136] Batch[5] avg_epoch_loss=7.136671\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=7.136670668919881\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:09 INFO 139830709036864] Epoch[136] Batch [5]#011Speed: 226.27 samples/sec#011loss=7.136671\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172887.0677865, \"EndTime\": 1663172890.3457634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3277.568578720093, \"count\": 1, \"min\": 3277.568578720093, \"max\": 3277.568578720093}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.63145109803625 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=136, train loss <loss>=7.1463165283203125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] Epoch[137] Batch[0] avg_epoch_loss=7.186785\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=7.186784744262695\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:12 INFO 139830709036864] Epoch[137] Batch[5] avg_epoch_loss=7.138543\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=7.138543446858724\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:12 INFO 139830709036864] Epoch[137] Batch [5]#011Speed: 211.53 samples/sec#011loss=7.138543\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] Epoch[137] Batch[10] avg_epoch_loss=7.132093\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=7.124352359771729\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] Epoch[137] Batch [10]#011Speed: 210.14 samples/sec#011loss=7.124352\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172890.3458576, \"EndTime\": 1663172894.0115373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3665.182590484619, \"count\": 1, \"min\": 3665.182590484619, \"max\": 3665.182590484619}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.24844774070118 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=137, train loss <loss>=7.1320929527282715\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] Epoch[138] Batch[0] avg_epoch_loss=7.302861\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=7.302861213684082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:16 INFO 139830709036864] Epoch[138] Batch[5] avg_epoch_loss=7.188494\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=7.188493887583415\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:16 INFO 139830709036864] Epoch[138] Batch [5]#011Speed: 212.43 samples/sec#011loss=7.188494\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] Epoch[138] Batch[10] avg_epoch_loss=7.208145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=7.23172664642334\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] Epoch[138] Batch [10]#011Speed: 215.84 samples/sec#011loss=7.231727\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172894.0116227, \"EndTime\": 1663172897.6704159, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3658.263921737671, \"count\": 1, \"min\": 3658.263921737671, \"max\": 3658.263921737671}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.0403879979745 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=138, train loss <loss>=7.2081451416015625\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:17 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:18 INFO 139830709036864] Epoch[139] Batch[0] avg_epoch_loss=7.150080\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=7.15008020401001\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:19 INFO 139830709036864] Epoch[139] Batch[5] avg_epoch_loss=7.151331\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=7.151331345240275\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:19 INFO 139830709036864] Epoch[139] Batch [5]#011Speed: 218.82 samples/sec#011loss=7.151331\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:20 INFO 139830709036864] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172897.6705031, \"EndTime\": 1663172900.9622753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3291.248083114624, \"count\": 1, \"min\": 3291.248083114624, \"max\": 3291.248083114624}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:20 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.4087321378163 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:20 INFO 139830709036864] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=139, train loss <loss>=7.15655665397644\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:20 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:21 INFO 139830709036864] Epoch[140] Batch[0] avg_epoch_loss=7.148386\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=7.148386478424072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:23 INFO 139830709036864] Epoch[140] Batch[5] avg_epoch_loss=7.109588\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=7.109588066736857\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:23 INFO 139830709036864] Epoch[140] Batch [5]#011Speed: 217.06 samples/sec#011loss=7.109588\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172900.9623713, \"EndTime\": 1663172904.2813365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3318.4547424316406, \"count\": 1, \"min\": 3318.4547424316406, \"max\": 3318.4547424316406}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.52531246156374 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=140, train loss <loss>=7.158549690246582\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] Epoch[141] Batch[0] avg_epoch_loss=7.141340\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=7.1413397789001465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:26 INFO 139830709036864] Epoch[141] Batch[5] avg_epoch_loss=7.137450\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=7.137449502944946\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:26 INFO 139830709036864] Epoch[141] Batch [5]#011Speed: 212.36 samples/sec#011loss=7.137450\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:27 INFO 139830709036864] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172904.2814283, \"EndTime\": 1663172907.556148, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3274.245262145996, \"count\": 1, \"min\": 3274.245262145996, \"max\": 3274.245262145996}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:27 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.32528483052593 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:27 INFO 139830709036864] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=141, train loss <loss>=7.203123903274536\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:27 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:28 INFO 139830709036864] Epoch[142] Batch[0] avg_epoch_loss=7.084398\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=7.08439826965332\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:29 INFO 139830709036864] Epoch[142] Batch[5] avg_epoch_loss=7.134860\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=7.134860038757324\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:29 INFO 139830709036864] Epoch[142] Batch [5]#011Speed: 217.09 samples/sec#011loss=7.134860\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:30 INFO 139830709036864] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172907.5562327, \"EndTime\": 1663172910.845172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3288.450002670288, \"count\": 1, \"min\": 3288.450002670288, \"max\": 3288.450002670288}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:30 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.6562495355623 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:30 INFO 139830709036864] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=142, train loss <loss>=7.106296586990356\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:30 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:31 INFO 139830709036864] Epoch[143] Batch[0] avg_epoch_loss=7.232817\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=7.232816696166992\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:32 INFO 139830709036864] Epoch[143] Batch[5] avg_epoch_loss=7.141526\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=7.141525665918986\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:32 INFO 139830709036864] Epoch[143] Batch [5]#011Speed: 215.67 samples/sec#011loss=7.141526\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] Epoch[143] Batch[10] avg_epoch_loss=7.165186\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=7.19357852935791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] Epoch[143] Batch [10]#011Speed: 210.43 samples/sec#011loss=7.193579\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172910.8453238, \"EndTime\": 1663172914.471263, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3625.459671020508, \"count\": 1, \"min\": 3625.459671020508, \"max\": 3625.459671020508}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.00593242078844 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=143, train loss <loss>=7.165186058391225\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:34 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:35 INFO 139830709036864] Epoch[144] Batch[0] avg_epoch_loss=7.101820\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=7.1018195152282715\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:36 INFO 139830709036864] Epoch[144] Batch[5] avg_epoch_loss=7.150832\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=7.150832414627075\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:36 INFO 139830709036864] Epoch[144] Batch [5]#011Speed: 212.97 samples/sec#011loss=7.150832\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] Epoch[144] Batch[10] avg_epoch_loss=7.148021\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=7.144648170471191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] Epoch[144] Batch [10]#011Speed: 211.77 samples/sec#011loss=7.144648\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172914.4713423, \"EndTime\": 1663172918.114295, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3642.4334049224854, \"count\": 1, \"min\": 3642.4334049224854, \"max\": 3642.4334049224854}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.11259169699562 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=144, train loss <loss>=7.148021394556219\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] Epoch[145] Batch[0] avg_epoch_loss=7.152900\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=7.152900218963623\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:40 INFO 139830709036864] Epoch[145] Batch[5] avg_epoch_loss=7.166426\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=7.166425625483195\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:40 INFO 139830709036864] Epoch[145] Batch [5]#011Speed: 211.35 samples/sec#011loss=7.166426\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:41 INFO 139830709036864] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172918.1143894, \"EndTime\": 1663172921.4801288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3365.1862144470215, \"count\": 1, \"min\": 3365.1862144470215, \"max\": 3365.1862144470215}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:41 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.74506003283253 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:41 INFO 139830709036864] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=145, train loss <loss>=7.151491403579712\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:41 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:42 INFO 139830709036864] Epoch[146] Batch[0] avg_epoch_loss=7.208918\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=7.20891809463501\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:43 INFO 139830709036864] Epoch[146] Batch[5] avg_epoch_loss=7.134813\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=7.134812513987224\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:43 INFO 139830709036864] Epoch[146] Batch [5]#011Speed: 210.69 samples/sec#011loss=7.134813\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:44 INFO 139830709036864] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172921.4802449, \"EndTime\": 1663172924.7600603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3279.3169021606445, \"count\": 1, \"min\": 3279.3169021606445, \"max\": 3279.3169021606445}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:44 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.7152594374893 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:44 INFO 139830709036864] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=146, train loss <loss>=7.109048128128052\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:44 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:45 INFO 139830709036864] Epoch[147] Batch[0] avg_epoch_loss=7.217316\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=7.217316150665283\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:46 INFO 139830709036864] Epoch[147] Batch[5] avg_epoch_loss=7.143970\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=7.143969615300496\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:46 INFO 139830709036864] Epoch[147] Batch [5]#011Speed: 211.08 samples/sec#011loss=7.143970\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172924.7601557, \"EndTime\": 1663172928.1435091, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3382.8840255737305, \"count\": 1, \"min\": 3382.8840255737305, \"max\": 3382.8840255737305}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.1110474329262 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=147, train loss <loss>=7.129894971847534\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] Epoch[148] Batch[0] avg_epoch_loss=6.979152\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=6.979152202606201\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:50 INFO 139830709036864] Epoch[148] Batch[5] avg_epoch_loss=7.109962\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=7.1099623044331866\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:50 INFO 139830709036864] Epoch[148] Batch [5]#011Speed: 230.58 samples/sec#011loss=7.109962\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172928.1436038, \"EndTime\": 1663172931.2983193, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3154.240131378174, \"count\": 1, \"min\": 3154.240131378174, \"max\": 3154.240131378174}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.96512164719306 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=148, train loss <loss>=7.133350229263305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] Epoch[149] Batch[0] avg_epoch_loss=7.007445\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=7.007445335388184\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:53 INFO 139830709036864] Epoch[149] Batch[5] avg_epoch_loss=7.079766\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=7.079765637715657\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:53 INFO 139830709036864] Epoch[149] Batch [5]#011Speed: 232.66 samples/sec#011loss=7.079766\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172931.2984495, \"EndTime\": 1663172934.4249377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3125.9846687316895, \"count\": 1, \"min\": 3125.9846687316895, \"max\": 3125.9846687316895}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=204.07996986045205 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=149, train loss <loss>=7.096339845657349\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:54 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_fd81d350-6697-4550-bd8b-7bfaa88498f7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172934.4251378, \"EndTime\": 1663172934.5288901, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 103.26051712036133, \"count\": 1, \"min\": 103.26051712036133, \"max\": 103.26051712036133}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:55 INFO 139830709036864] Epoch[150] Batch[0] avg_epoch_loss=7.217304\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=7.21730375289917\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:56 INFO 139830709036864] Epoch[150] Batch[5] avg_epoch_loss=7.135945\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=7.135944525400798\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:56 INFO 139830709036864] Epoch[150] Batch [5]#011Speed: 216.09 samples/sec#011loss=7.135945\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] Epoch[150] Batch[10] avg_epoch_loss=7.151618\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=7.170426368713379\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] Epoch[150] Batch [10]#011Speed: 207.18 samples/sec#011loss=7.170426\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172934.5289772, \"EndTime\": 1663172938.192225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3663.180112838745, \"count\": 1, \"min\": 3663.180112838745, \"max\": 3663.180112838745}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.3491274066704 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=150, train loss <loss>=7.15161809054288\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] Epoch[151] Batch[0] avg_epoch_loss=7.047496\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:28:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=7.0474958419799805\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:00 INFO 139830709036864] Epoch[151] Batch[5] avg_epoch_loss=7.137596\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=7.137595971425374\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:00 INFO 139830709036864] Epoch[151] Batch [5]#011Speed: 217.75 samples/sec#011loss=7.137596\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:01 INFO 139830709036864] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172938.19231, \"EndTime\": 1663172941.5014942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3308.708667755127, \"count\": 1, \"min\": 3308.708667755127, \"max\": 3308.708667755127}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:01 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.58675645172505 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:01 INFO 139830709036864] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=151, train loss <loss>=7.130845022201538\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:01 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:02 INFO 139830709036864] Epoch[152] Batch[0] avg_epoch_loss=7.097406\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=7.097406387329102\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:03 INFO 139830709036864] Epoch[152] Batch[5] avg_epoch_loss=7.070261\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=7.070261478424072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:03 INFO 139830709036864] Epoch[152] Batch [5]#011Speed: 240.62 samples/sec#011loss=7.070261\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] Epoch[152] Batch[10] avg_epoch_loss=7.174655\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=7.2999269485473635\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] Epoch[152] Batch [10]#011Speed: 213.44 samples/sec#011loss=7.299927\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172941.5019448, \"EndTime\": 1663172944.9994183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3496.917963027954, \"count\": 1, \"min\": 3496.917963027954, \"max\": 3496.917963027954}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.16010685789678 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=152, train loss <loss>=7.174654873934659\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:04 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:05 INFO 139830709036864] Epoch[153] Batch[0] avg_epoch_loss=7.232249\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=7.232248783111572\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:07 INFO 139830709036864] Epoch[153] Batch[5] avg_epoch_loss=7.126515\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=7.126514991124471\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:07 INFO 139830709036864] Epoch[153] Batch [5]#011Speed: 207.32 samples/sec#011loss=7.126515\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:08 INFO 139830709036864] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172944.9995186, \"EndTime\": 1663172948.3973744, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3397.338628768921, \"count\": 1, \"min\": 3397.338628768921, \"max\": 3397.338628768921}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:08 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.07718933260463 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:08 INFO 139830709036864] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=153, train loss <loss>=7.122073554992676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:08 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:09 INFO 139830709036864] Epoch[154] Batch[0] avg_epoch_loss=7.090739\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=7.0907392501831055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:10 INFO 139830709036864] Epoch[154] Batch[5] avg_epoch_loss=7.143157\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=7.143157005310059\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:10 INFO 139830709036864] Epoch[154] Batch [5]#011Speed: 211.37 samples/sec#011loss=7.143157\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] Epoch[154] Batch[10] avg_epoch_loss=7.223603\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=7.320138740539551\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] Epoch[154] Batch [10]#011Speed: 218.73 samples/sec#011loss=7.320139\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172948.3974679, \"EndTime\": 1663172951.9841664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3586.0586166381836, \"count\": 1, \"min\": 3586.0586166381836, \"max\": 3586.0586166381836}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.71261176121752 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=154, train loss <loss>=7.223603248596191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:11 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:12 INFO 139830709036864] Epoch[155] Batch[0] avg_epoch_loss=7.201506\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=7.2015061378479\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:14 INFO 139830709036864] Epoch[155] Batch[5] avg_epoch_loss=7.205162\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=7.2051622072855634\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:14 INFO 139830709036864] Epoch[155] Batch [5]#011Speed: 227.98 samples/sec#011loss=7.205162\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172951.9842544, \"EndTime\": 1663172955.2325408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3247.8041648864746, \"count\": 1, \"min\": 3247.8041648864746, \"max\": 3247.8041648864746}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.81731255265993 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=155, train loss <loss>=7.1558449268341064\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] Epoch[156] Batch[0] avg_epoch_loss=7.189205\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=7.189204692840576\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:17 INFO 139830709036864] Epoch[156] Batch[5] avg_epoch_loss=7.089838\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=7.089838186899821\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:17 INFO 139830709036864] Epoch[156] Batch [5]#011Speed: 228.70 samples/sec#011loss=7.089838\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172955.2326233, \"EndTime\": 1663172958.440796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3207.676410675049, \"count\": 1, \"min\": 3207.676410675049, \"max\": 3207.676410675049}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.64316190560567 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=156, train loss <loss>=7.0667815685272215\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:18 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_48308af5-2ba3-412d-927b-0997ed3e6742-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172958.440881, \"EndTime\": 1663172958.515336, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 73.85587692260742, \"count\": 1, \"min\": 73.85587692260742, \"max\": 73.85587692260742}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:19 INFO 139830709036864] Epoch[157] Batch[0] avg_epoch_loss=6.961022\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=6.96102237701416\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:20 INFO 139830709036864] Epoch[157] Batch[5] avg_epoch_loss=7.053843\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=7.053842544555664\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:20 INFO 139830709036864] Epoch[157] Batch [5]#011Speed: 210.05 samples/sec#011loss=7.053843\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] Epoch[157] Batch[10] avg_epoch_loss=7.084660\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=7.121641063690186\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] Epoch[157] Batch [10]#011Speed: 209.16 samples/sec#011loss=7.121641\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172958.5153904, \"EndTime\": 1663172962.184244, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3668.7979698181152, \"count\": 1, \"min\": 3668.7979698181152, \"max\": 3668.7979698181152}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.52102384535436 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=157, train loss <loss>=7.084660053253174\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] Epoch[158] Batch[0] avg_epoch_loss=7.205847\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=7.205846786499023\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:24 INFO 139830709036864] Epoch[158] Batch[5] avg_epoch_loss=7.049341\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=7.049340645472209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:24 INFO 139830709036864] Epoch[158] Batch [5]#011Speed: 210.16 samples/sec#011loss=7.049341\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172962.184332, \"EndTime\": 1663172965.5375333, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3352.735996246338, \"count\": 1, \"min\": 3352.735996246338, \"max\": 3352.735996246338}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.09190885577792 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=158, train loss <loss>=7.056285047531128\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:25 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_e80f17e7-c37a-4ac6-b82f-5022e2b6730b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172965.537625, \"EndTime\": 1663172965.640473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 102.38051414489746, \"count\": 1, \"min\": 102.38051414489746, \"max\": 102.38051414489746}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:26 INFO 139830709036864] Epoch[159] Batch[0] avg_epoch_loss=6.875863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=6.875863075256348\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:27 INFO 139830709036864] Epoch[159] Batch[5] avg_epoch_loss=7.035056\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=7.035056273142497\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:27 INFO 139830709036864] Epoch[159] Batch [5]#011Speed: 220.06 samples/sec#011loss=7.035056\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172965.6405506, \"EndTime\": 1663172968.857618, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3217.0040607452393, \"count\": 1, \"min\": 3217.0040607452393, \"max\": 3217.0040607452393}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.51608520983308 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=159, train loss <loss>=7.034001255035401\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:28 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_902e575c-e304-4466-be7a-0d0c94031349-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172968.8577044, \"EndTime\": 1663172968.9213202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.07244300842285, \"count\": 1, \"min\": 63.07244300842285, \"max\": 63.07244300842285}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:29 INFO 139830709036864] Epoch[160] Batch[0] avg_epoch_loss=6.893140\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=6.8931403160095215\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:31 INFO 139830709036864] Epoch[160] Batch[5] avg_epoch_loss=7.003269\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=7.00326943397522\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:31 INFO 139830709036864] Epoch[160] Batch [5]#011Speed: 211.63 samples/sec#011loss=7.003269\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172968.9214175, \"EndTime\": 1663172972.193929, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3272.4430561065674, \"count\": 1, \"min\": 3272.4430561065674, \"max\": 3272.4430561065674}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.95434484161433 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=160, train loss <loss>=7.076924180984497\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] Epoch[161] Batch[0] avg_epoch_loss=7.057040\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=7.057039737701416\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:34 INFO 139830709036864] Epoch[161] Batch[5] avg_epoch_loss=7.079097\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=7.079097350438436\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:34 INFO 139830709036864] Epoch[161] Batch [5]#011Speed: 216.02 samples/sec#011loss=7.079097\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] Epoch[161] Batch[10] avg_epoch_loss=7.102112\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=7.129729557037353\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] Epoch[161] Batch [10]#011Speed: 207.03 samples/sec#011loss=7.129730\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172972.194009, \"EndTime\": 1663172975.8391826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3644.6967124938965, \"count\": 1, \"min\": 3644.6967124938965, \"max\": 3644.6967124938965}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.07707816453257 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=161, train loss <loss>=7.10211198980158\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:36 INFO 139830709036864] Epoch[162] Batch[0] avg_epoch_loss=6.867145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=6.867144584655762\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:37 INFO 139830709036864] Epoch[162] Batch[5] avg_epoch_loss=6.955618\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=6.955617984135945\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:37 INFO 139830709036864] Epoch[162] Batch [5]#011Speed: 216.87 samples/sec#011loss=6.955618\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172975.8392975, \"EndTime\": 1663172979.165456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3325.608491897583, \"count\": 1, \"min\": 3325.608491897583, \"max\": 3325.608491897583}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.53609434130937 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=162, train loss <loss>=6.991533946990967\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_09fa0f7f-41f6-4850-8d02-042d067fe811-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172979.1655498, \"EndTime\": 1663172979.229171, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.03596496582031, \"count\": 1, \"min\": 63.03596496582031, \"max\": 63.03596496582031}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] Epoch[163] Batch[0] avg_epoch_loss=7.215578\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=7.215577602386475\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:41 INFO 139830709036864] Epoch[163] Batch[5] avg_epoch_loss=7.066510\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=7.06650972366333\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:41 INFO 139830709036864] Epoch[163] Batch [5]#011Speed: 211.34 samples/sec#011loss=7.066510\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:42 INFO 139830709036864] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172979.229264, \"EndTime\": 1663172982.5644498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3335.115671157837, \"count\": 1, \"min\": 3335.115671157837, \"max\": 3335.115671157837}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.7925998130538 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=163, train loss <loss>=7.072124099731445\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:42 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:43 INFO 139830709036864] Epoch[164] Batch[0] avg_epoch_loss=6.995858\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=6.995858192443848\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:44 INFO 139830709036864] Epoch[164] Batch[5] avg_epoch_loss=7.023161\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=7.023160934448242\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:44 INFO 139830709036864] Epoch[164] Batch [5]#011Speed: 217.43 samples/sec#011loss=7.023161\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:45 INFO 139830709036864] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172982.5645459, \"EndTime\": 1663172985.878628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3313.5650157928467, \"count\": 1, \"min\": 3313.5650157928467, \"max\": 3313.5650157928467}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:45 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.91283393968698 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:45 INFO 139830709036864] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=164, train loss <loss>=7.031378889083863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:45 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:46 INFO 139830709036864] Epoch[165] Batch[0] avg_epoch_loss=6.920573\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=6.9205732345581055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:47 INFO 139830709036864] Epoch[165] Batch[5] avg_epoch_loss=7.048642\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=7.048641840616862\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:47 INFO 139830709036864] Epoch[165] Batch [5]#011Speed: 222.44 samples/sec#011loss=7.048642\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172985.8787205, \"EndTime\": 1663172989.1427758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3263.516426086426, \"count\": 1, \"min\": 3263.516426086426, \"max\": 3263.516426086426}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.72777835529664 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=165, train loss <loss>=7.072117185592651\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] Epoch[166] Batch[0] avg_epoch_loss=7.053539\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=7.053538799285889\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:51 INFO 139830709036864] Epoch[166] Batch[5] avg_epoch_loss=7.090749\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=7.090749343236287\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:51 INFO 139830709036864] Epoch[166] Batch [5]#011Speed: 216.42 samples/sec#011loss=7.090749\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] Epoch[166] Batch[10] avg_epoch_loss=7.102482\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=7.116560268402099\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] Epoch[166] Batch [10]#011Speed: 213.84 samples/sec#011loss=7.116560\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172989.142889, \"EndTime\": 1663172992.7659607, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3622.586727142334, \"count\": 1, \"min\": 3622.586727142334, \"max\": 3622.586727142334}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=177.21528340334763 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=166, train loss <loss>=7.1024815819480205\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:52 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:53 INFO 139830709036864] Epoch[167] Batch[0] avg_epoch_loss=6.989583\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=6.9895830154418945\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:54 INFO 139830709036864] Epoch[167] Batch[5] avg_epoch_loss=7.057311\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=7.0573108196258545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:54 INFO 139830709036864] Epoch[167] Batch [5]#011Speed: 218.09 samples/sec#011loss=7.057311\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172992.7660494, \"EndTime\": 1663172996.0622478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3295.7565784454346, \"count\": 1, \"min\": 3295.7565784454346, \"max\": 3295.7565784454346}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.95589748976175 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=167, train loss <loss>=7.081169271469117\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] Epoch[168] Batch[0] avg_epoch_loss=7.091997\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=7.091997146606445\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:58 INFO 139830709036864] Epoch[168] Batch[5] avg_epoch_loss=7.056661\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=7.056660811106364\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:58 INFO 139830709036864] Epoch[168] Batch [5]#011Speed: 227.76 samples/sec#011loss=7.056661\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172996.0623293, \"EndTime\": 1663172999.2935703, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3230.7801246643066, \"count\": 1, \"min\": 3230.7801246643066, \"max\": 3230.7801246643066}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.46742240486796 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=168, train loss <loss>=7.025611877441406\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] Epoch[169] Batch[0] avg_epoch_loss=7.082835\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:29:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=7.0828351974487305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:01 INFO 139830709036864] Epoch[169] Batch[5] avg_epoch_loss=7.041010\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=7.041009744008382\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:01 INFO 139830709036864] Epoch[169] Batch [5]#011Speed: 225.70 samples/sec#011loss=7.041010\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:02 INFO 139830709036864] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663172999.2936642, \"EndTime\": 1663173002.5866406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3292.5050258636475, \"count\": 1, \"min\": 3292.5050258636475, \"max\": 3292.5050258636475}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:02 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.17271828012332 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:02 INFO 139830709036864] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=169, train loss <loss>=7.009835910797119\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:02 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:03 INFO 139830709036864] Epoch[170] Batch[0] avg_epoch_loss=7.129222\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=7.1292219161987305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:04 INFO 139830709036864] Epoch[170] Batch[5] avg_epoch_loss=7.012825\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=7.012824932734172\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:04 INFO 139830709036864] Epoch[170] Batch [5]#011Speed: 210.91 samples/sec#011loss=7.012825\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] Epoch[170] Batch[10] avg_epoch_loss=7.053407\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=7.102105808258057\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] Epoch[170] Batch [10]#011Speed: 200.96 samples/sec#011loss=7.102106\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173002.5867376, \"EndTime\": 1663173006.3147993, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3727.442741394043, \"count\": 1, \"min\": 3727.442741394043, \"max\": 3727.442741394043}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=172.76641866098421 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=170, train loss <loss>=7.053407148881392\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] Epoch[171] Batch[0] avg_epoch_loss=6.972941\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=6.972940921783447\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:08 INFO 139830709036864] Epoch[171] Batch[5] avg_epoch_loss=7.006091\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=7.006091197331746\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:08 INFO 139830709036864] Epoch[171] Batch [5]#011Speed: 229.72 samples/sec#011loss=7.006091\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173006.3148901, \"EndTime\": 1663173009.5348353, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3219.3338871002197, \"count\": 1, \"min\": 3219.3338871002197, \"max\": 3219.3338871002197}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=198.79142483818117 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=171, train loss <loss>=6.981521463394165\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:09 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_8655dffc-e017-428c-9325-a5aaba001901-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173009.534919, \"EndTime\": 1663173009.604977, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 69.4723129272461, \"count\": 1, \"min\": 69.4723129272461, \"max\": 69.4723129272461}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:10 INFO 139830709036864] Epoch[172] Batch[0] avg_epoch_loss=7.176076\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=7.176076412200928\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:11 INFO 139830709036864] Epoch[172] Batch[5] avg_epoch_loss=7.055395\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=7.055394649505615\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:11 INFO 139830709036864] Epoch[172] Batch [5]#011Speed: 228.03 samples/sec#011loss=7.055395\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] Epoch[172] Batch[10] avg_epoch_loss=7.002549\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=6.939135265350342\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] Epoch[172] Batch [10]#011Speed: 210.03 samples/sec#011loss=6.939135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173009.6050801, \"EndTime\": 1663173013.1769412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3571.794033050537, \"count\": 1, \"min\": 3571.794033050537, \"max\": 3571.794033050537}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.61469910722408 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=172, train loss <loss>=7.002549474889582\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] Epoch[173] Batch[0] avg_epoch_loss=6.936001\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=6.936000823974609\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:15 INFO 139830709036864] Epoch[173] Batch[5] avg_epoch_loss=6.933142\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=6.933141549428304\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:15 INFO 139830709036864] Epoch[173] Batch [5]#011Speed: 232.94 samples/sec#011loss=6.933142\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173013.1770263, \"EndTime\": 1663173016.375017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3197.564125061035, \"count\": 1, \"min\": 3197.564125061035, \"max\": 3197.564125061035}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.2021537085745 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=173, train loss <loss>=6.865892553329468\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:16 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_2cbcbf7a-c283-434d-a318-b7fd92ba6161-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173016.3751116, \"EndTime\": 1663173016.4455276, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 69.6115493774414, \"count\": 1, \"min\": 69.6115493774414, \"max\": 69.6115493774414}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:17 INFO 139830709036864] Epoch[174] Batch[0] avg_epoch_loss=7.210780\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=7.210780143737793\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:18 INFO 139830709036864] Epoch[174] Batch[5] avg_epoch_loss=7.038161\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=7.038160800933838\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:18 INFO 139830709036864] Epoch[174] Batch [5]#011Speed: 231.17 samples/sec#011loss=7.038161\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:19 INFO 139830709036864] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173016.4456317, \"EndTime\": 1663173019.6956575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3249.9537467956543, \"count\": 1, \"min\": 3249.9537467956543, \"max\": 3249.9537467956543}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:19 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.84161397072336 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:19 INFO 139830709036864] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=174, train loss <loss>=7.012381553649902\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:19 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:20 INFO 139830709036864] Epoch[175] Batch[0] avg_epoch_loss=7.181988\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=7.18198823928833\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:21 INFO 139830709036864] Epoch[175] Batch[5] avg_epoch_loss=7.077231\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=7.077231486638387\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:21 INFO 139830709036864] Epoch[175] Batch [5]#011Speed: 217.59 samples/sec#011loss=7.077231\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:22 INFO 139830709036864] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173019.6957397, \"EndTime\": 1663173022.8988538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3202.4378776550293, \"count\": 1, \"min\": 3202.4378776550293, \"max\": 3202.4378776550293}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:22 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.65355569442204 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:22 INFO 139830709036864] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=175, train loss <loss>=7.049249792098999\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:22 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:23 INFO 139830709036864] Epoch[176] Batch[0] avg_epoch_loss=6.992812\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=6.992811679840088\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:24 INFO 139830709036864] Epoch[176] Batch[5] avg_epoch_loss=6.922140\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=6.9221400419871015\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:24 INFO 139830709036864] Epoch[176] Batch [5]#011Speed: 223.01 samples/sec#011loss=6.922140\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] Epoch[176] Batch[10] avg_epoch_loss=6.998878\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=7.090964508056641\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] Epoch[176] Batch [10]#011Speed: 209.61 samples/sec#011loss=7.090965\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173022.8989487, \"EndTime\": 1663173026.4831183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3583.512544631958, \"count\": 1, \"min\": 3583.512544631958, \"max\": 3583.512544631958}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.1456663038172 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=176, train loss <loss>=6.998878435655073\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:26 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:27 INFO 139830709036864] Epoch[177] Batch[0] avg_epoch_loss=6.872063\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=6.872063159942627\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:28 INFO 139830709036864] Epoch[177] Batch[5] avg_epoch_loss=6.963692\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=6.9636924266815186\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:28 INFO 139830709036864] Epoch[177] Batch [5]#011Speed: 220.58 samples/sec#011loss=6.963692\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] Epoch[177] Batch[10] avg_epoch_loss=7.068630\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=7.1945549011230465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] Epoch[177] Batch [10]#011Speed: 225.27 samples/sec#011loss=7.194555\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173026.4832423, \"EndTime\": 1663173029.941069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3457.3042392730713, \"count\": 1, \"min\": 3457.3042392730713, \"max\": 3457.3042392730713}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.28910065259726 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=177, train loss <loss>=7.068629915064031\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:29 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:30 INFO 139830709036864] Epoch[178] Batch[0] avg_epoch_loss=6.871480\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=6.871480464935303\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:31 INFO 139830709036864] Epoch[178] Batch[5] avg_epoch_loss=6.968864\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=6.968864361445109\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:31 INFO 139830709036864] Epoch[178] Batch [5]#011Speed: 229.94 samples/sec#011loss=6.968864\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] Epoch[178] Batch[10] avg_epoch_loss=7.042219\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=7.130245208740234\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] Epoch[178] Batch [10]#011Speed: 226.79 samples/sec#011loss=7.130245\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173029.9411702, \"EndTime\": 1663173033.364174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3421.7333793640137, \"count\": 1, \"min\": 3421.7333793640137, \"max\": 3421.7333793640137}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.66269511889317 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=178, train loss <loss>=7.042219292033803\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:33 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:34 INFO 139830709036864] Epoch[179] Batch[0] avg_epoch_loss=7.019489\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=7.019489288330078\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:35 INFO 139830709036864] Epoch[179] Batch[5] avg_epoch_loss=7.077139\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=7.077138741811116\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:35 INFO 139830709036864] Epoch[179] Batch [5]#011Speed: 234.24 samples/sec#011loss=7.077139\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] Epoch[179] Batch[10] avg_epoch_loss=7.036659\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=6.988082218170166\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] Epoch[179] Batch [10]#011Speed: 208.77 samples/sec#011loss=6.988082\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173033.3642626, \"EndTime\": 1663173036.93782, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3573.0552673339844, \"count\": 1, \"min\": 3573.0552673339844, \"max\": 3573.0552673339844}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=181.62855487539733 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=179, train loss <loss>=7.036658503792503\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:36 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:37 INFO 139830709036864] Epoch[180] Batch[0] avg_epoch_loss=7.085111\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=7.085110664367676\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:39 INFO 139830709036864] Epoch[180] Batch[5] avg_epoch_loss=6.989616\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=6.989616473515828\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:39 INFO 139830709036864] Epoch[180] Batch [5]#011Speed: 219.96 samples/sec#011loss=6.989616\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173036.9379485, \"EndTime\": 1663173040.206558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3268.146753311157, \"count\": 1, \"min\": 3268.146753311157, \"max\": 3268.146753311157}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.2103518244485 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=180, train loss <loss>=6.949004316329956\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] Epoch[181] Batch[0] avg_epoch_loss=6.962777\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=6.962777137756348\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:42 INFO 139830709036864] Epoch[181] Batch[5] avg_epoch_loss=7.012238\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=7.012238423029582\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:42 INFO 139830709036864] Epoch[181] Batch [5]#011Speed: 211.51 samples/sec#011loss=7.012238\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:43 INFO 139830709036864] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173040.2066455, \"EndTime\": 1663173043.459527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3252.4068355560303, \"count\": 1, \"min\": 3252.4068355560303, \"max\": 3252.4068355560303}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:43 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.00271082705734 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:43 INFO 139830709036864] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=181, train loss <loss>=6.984568357467651\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:43 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:44 INFO 139830709036864] Epoch[182] Batch[0] avg_epoch_loss=6.997552\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=6.997552394866943\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:45 INFO 139830709036864] Epoch[182] Batch[5] avg_epoch_loss=7.018257\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:45 INFO 139830709036864] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=7.018256505330403\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:45 INFO 139830709036864] Epoch[182] Batch [5]#011Speed: 217.32 samples/sec#011loss=7.018257\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] Epoch[182] Batch[10] avg_epoch_loss=6.772504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=6.477601051330566\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] Epoch[182] Batch [10]#011Speed: 222.04 samples/sec#011loss=6.477601\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173043.4596648, \"EndTime\": 1663173047.022218, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3562.072992324829, \"count\": 1, \"min\": 3562.072992324829, \"max\": 3562.072992324829}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.5597429048756 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] #progress_metric: host=algo-1, completed 45.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=182, train loss <loss>=6.772504026239568\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_8b417089-889a-4441-b830-e0ef23ed2e8e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173047.0223033, \"EndTime\": 1663173047.1251698, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 102.38146781921387, \"count\": 1, \"min\": 102.38146781921387, \"max\": 102.38146781921387}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] Epoch[183] Batch[0] avg_epoch_loss=6.885545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=6.885544776916504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:49 INFO 139830709036864] Epoch[183] Batch[5] avg_epoch_loss=6.882827\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=6.882827123006185\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:49 INFO 139830709036864] Epoch[183] Batch [5]#011Speed: 212.19 samples/sec#011loss=6.882827\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] Epoch[183] Batch[10] avg_epoch_loss=6.951234\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=7.033321952819824\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] Epoch[183] Batch [10]#011Speed: 207.00 samples/sec#011loss=7.033322\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173047.125247, \"EndTime\": 1663173050.7925017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3667.187213897705, \"count\": 1, \"min\": 3667.187213897705, \"max\": 3667.187213897705}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.05921627642758 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=183, train loss <loss>=6.951233863830566\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:50 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:51 INFO 139830709036864] Epoch[184] Batch[0] avg_epoch_loss=6.917450\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=6.917449951171875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:52 INFO 139830709036864] Epoch[184] Batch[5] avg_epoch_loss=6.960184\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:52 INFO 139830709036864] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=6.960184256235759\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:52 INFO 139830709036864] Epoch[184] Batch [5]#011Speed: 213.56 samples/sec#011loss=6.960184\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] Epoch[184] Batch[10] avg_epoch_loss=6.977504\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=6.998288536071778\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] Epoch[184] Batch [10]#011Speed: 209.63 samples/sec#011loss=6.998289\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173050.7925932, \"EndTime\": 1663173054.4615235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3668.3967113494873, \"count\": 1, \"min\": 3668.3967113494873, \"max\": 3668.3967113494873}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=176.63773896123394 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] #progress_metric: host=algo-1, completed 46.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=184, train loss <loss>=6.977504383433949\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:54 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:55 INFO 139830709036864] Epoch[185] Batch[0] avg_epoch_loss=6.886838\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=6.886838436126709\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:56 INFO 139830709036864] Epoch[185] Batch[5] avg_epoch_loss=7.024072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=7.024072011311849\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:56 INFO 139830709036864] Epoch[185] Batch [5]#011Speed: 218.01 samples/sec#011loss=7.024072\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:57 INFO 139830709036864] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173054.46161, \"EndTime\": 1663173057.7831264, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3320.964574813843, \"count\": 1, \"min\": 3320.964574813843, \"max\": 3320.964574813843}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:57 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.48972171567627 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:57 INFO 139830709036864] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=185, train loss <loss>=6.953533267974853\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:57 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:58 INFO 139830709036864] Epoch[186] Batch[0] avg_epoch_loss=7.004133\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=7.004133224487305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:59 INFO 139830709036864] Epoch[186] Batch[5] avg_epoch_loss=6.980707\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=6.980707327524821\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:30:59 INFO 139830709036864] Epoch[186] Batch [5]#011Speed: 232.02 samples/sec#011loss=6.980707\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] Epoch[186] Batch[10] avg_epoch_loss=6.963502\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=6.942855739593506\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] Epoch[186] Batch [10]#011Speed: 224.39 samples/sec#011loss=6.942856\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173057.783253, \"EndTime\": 1663173061.2338965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3450.111150741577, \"count\": 1, \"min\": 3450.111150741577, \"max\": 3450.111150741577}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.65379083232543 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] #progress_metric: host=algo-1, completed 46.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=186, train loss <loss>=6.963502060283314\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] Epoch[187] Batch[0] avg_epoch_loss=6.939450\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=6.939450263977051\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:03 INFO 139830709036864] Epoch[187] Batch[5] avg_epoch_loss=6.992608\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=6.9926075140635175\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:03 INFO 139830709036864] Epoch[187] Batch [5]#011Speed: 218.61 samples/sec#011loss=6.992608\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] Epoch[187] Batch[10] avg_epoch_loss=7.072968\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=7.169401168823242\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] Epoch[187] Batch [10]#011Speed: 209.89 samples/sec#011loss=7.169401\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173061.2339804, \"EndTime\": 1663173064.8699217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3635.371685028076, \"count\": 1, \"min\": 3635.371685028076, \"max\": 3635.371685028076}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.9450420767665 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=187, train loss <loss>=7.072968266227028\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:04 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:05 INFO 139830709036864] Epoch[188] Batch[0] avg_epoch_loss=7.123612\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=7.123611927032471\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:06 INFO 139830709036864] Epoch[188] Batch[5] avg_epoch_loss=6.975511\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=6.975510676701863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:06 INFO 139830709036864] Epoch[188] Batch [5]#011Speed: 239.05 samples/sec#011loss=6.975511\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] Epoch[188] Batch[10] avg_epoch_loss=6.804145\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=6.5985054016113285\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] Epoch[188] Batch [10]#011Speed: 208.96 samples/sec#011loss=6.598505\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173064.8699944, \"EndTime\": 1663173068.329522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3458.9967727661133, \"count\": 1, \"min\": 3458.9967727661133, \"max\": 3458.9967727661133}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.2218410229466 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] #progress_metric: host=algo-1, completed 47.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=188, train loss <loss>=6.804144642569802\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] Epoch[189] Batch[0] avg_epoch_loss=6.896316\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=6.896316051483154\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:10 INFO 139830709036864] Epoch[189] Batch[5] avg_epoch_loss=6.903937\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=6.903937101364136\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:10 INFO 139830709036864] Epoch[189] Batch [5]#011Speed: 226.07 samples/sec#011loss=6.903937\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:11 INFO 139830709036864] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173068.329607, \"EndTime\": 1663173071.5504835, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3220.4620838165283, \"count\": 1, \"min\": 3220.4620838165283, \"max\": 3220.4620838165283}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:11 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.78533386930687 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:11 INFO 139830709036864] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=189, train loss <loss>=6.920023679733276\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:11 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:12 INFO 139830709036864] Epoch[190] Batch[0] avg_epoch_loss=7.035294\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=7.0352935791015625\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:13 INFO 139830709036864] Epoch[190] Batch[5] avg_epoch_loss=6.939544\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=6.939543803532918\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:13 INFO 139830709036864] Epoch[190] Batch [5]#011Speed: 223.40 samples/sec#011loss=6.939544\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] Epoch[190] Batch[10] avg_epoch_loss=6.974845\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=7.017206859588623\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] Epoch[190] Batch [10]#011Speed: 221.13 samples/sec#011loss=7.017207\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173071.5505729, \"EndTime\": 1663173075.075773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3524.7557163238525, \"count\": 1, \"min\": 3524.7557163238525, \"max\": 3524.7557163238525}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.83546459070382 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] #progress_metric: host=algo-1, completed 47.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=190, train loss <loss>=6.9748451926491475\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] Epoch[191] Batch[0] avg_epoch_loss=7.037447\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=7.037447452545166\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:17 INFO 139830709036864] Epoch[191] Batch[5] avg_epoch_loss=6.968467\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=6.968466758728027\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:17 INFO 139830709036864] Epoch[191] Batch [5]#011Speed: 214.66 samples/sec#011loss=6.968467\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] Epoch[191] Batch[10] avg_epoch_loss=6.982302\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=6.998905086517334\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] Epoch[191] Batch [10]#011Speed: 208.20 samples/sec#011loss=6.998905\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173075.0758681, \"EndTime\": 1663173078.720092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3643.7082290649414, \"count\": 1, \"min\": 3643.7082290649414, \"max\": 3643.7082290649414}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=177.01143225162613 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=191, train loss <loss>=6.982302362268621\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:18 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:19 INFO 139830709036864] Epoch[192] Batch[0] avg_epoch_loss=6.762613\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=6.762612819671631\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:20 INFO 139830709036864] Epoch[192] Batch[5] avg_epoch_loss=6.922755\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=6.922754844029744\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:20 INFO 139830709036864] Epoch[192] Batch [5]#011Speed: 218.04 samples/sec#011loss=6.922755\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] Epoch[192] Batch[10] avg_epoch_loss=6.945388\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=6.972548770904541\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] Epoch[192] Batch [10]#011Speed: 215.77 samples/sec#011loss=6.972549\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173078.7201796, \"EndTime\": 1663173082.2903242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3569.6895122528076, \"count\": 1, \"min\": 3569.6895122528076, \"max\": 3569.6895122528076}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=193.00706316077714 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] #progress_metric: host=algo-1, completed 48.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=192, train loss <loss>=6.945388447154652\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] Epoch[193] Batch[0] avg_epoch_loss=7.006184\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=7.006183624267578\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:24 INFO 139830709036864] Epoch[193] Batch[5] avg_epoch_loss=6.933223\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:24 INFO 139830709036864] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=6.933223327000936\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:24 INFO 139830709036864] Epoch[193] Batch [5]#011Speed: 224.19 samples/sec#011loss=6.933223\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] Epoch[193] Batch[10] avg_epoch_loss=6.726062\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=6.477467727661133\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] Epoch[193] Batch [10]#011Speed: 210.80 samples/sec#011loss=6.477468\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173082.290414, \"EndTime\": 1663173085.8411963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3550.3103733062744, \"count\": 1, \"min\": 3550.3103733062744, \"max\": 3550.3103733062744}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.76245329926738 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=193, train loss <loss>=6.726061690937389\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:25 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/state_0043ae74-d481-400d-bea3-c1d5eb2f62df-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173085.8413434, \"EndTime\": 1663173085.916545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 74.61667060852051, \"count\": 1, \"min\": 74.61667060852051, \"max\": 74.61667060852051}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:26 INFO 139830709036864] Epoch[194] Batch[0] avg_epoch_loss=6.969564\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=6.969563961029053\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:28 INFO 139830709036864] Epoch[194] Batch[5] avg_epoch_loss=6.962342\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=6.962342023849487\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:28 INFO 139830709036864] Epoch[194] Batch [5]#011Speed: 211.66 samples/sec#011loss=6.962342\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173085.9166327, \"EndTime\": 1663173089.2283947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3311.6960525512695, \"count\": 1, \"min\": 3311.6960525512695, \"max\": 3311.6960525512695}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.3415494623155 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] #progress_metric: host=algo-1, completed 48.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=194, train loss <loss>=6.9542279720306395\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] Epoch[195] Batch[0] avg_epoch_loss=6.905396\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=6.9053955078125\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:31 INFO 139830709036864] Epoch[195] Batch[5] avg_epoch_loss=6.926903\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:31 INFO 139830709036864] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=6.926903486251831\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:31 INFO 139830709036864] Epoch[195] Batch [5]#011Speed: 225.43 samples/sec#011loss=6.926903\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173089.228477, \"EndTime\": 1663173092.3931184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3163.959741592407, \"count\": 1, \"min\": 3163.959741592407, \"max\": 3163.959741592407}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.15654779370155 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=195, train loss <loss>=6.948329448699951\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] Epoch[196] Batch[0] avg_epoch_loss=6.942802\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=6.9428019523620605\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:34 INFO 139830709036864] Epoch[196] Batch[5] avg_epoch_loss=6.947825\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=6.947824875513713\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:34 INFO 139830709036864] Epoch[196] Batch [5]#011Speed: 220.04 samples/sec#011loss=6.947825\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:35 INFO 139830709036864] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173092.3932035, \"EndTime\": 1663173095.6479044, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3254.138469696045, \"count\": 1, \"min\": 3254.138469696045, \"max\": 3254.138469696045}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.43597472322344 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 49.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=196, train loss <loss>=6.911467504501343\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:36 INFO 139830709036864] Epoch[197] Batch[0] avg_epoch_loss=6.892122\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=6.892122268676758\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:37 INFO 139830709036864] Epoch[197] Batch[5] avg_epoch_loss=6.921652\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=6.9216523965199785\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:37 INFO 139830709036864] Epoch[197] Batch [5]#011Speed: 210.55 samples/sec#011loss=6.921652\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:38 INFO 139830709036864] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173095.647989, \"EndTime\": 1663173098.9804823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3332.0114612579346, \"count\": 1, \"min\": 3332.0114612579346, \"max\": 3332.0114612579346}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:38 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.364456053538 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:38 INFO 139830709036864] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=197, train loss <loss>=6.947029447555542\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:38 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:39 INFO 139830709036864] Epoch[198] Batch[0] avg_epoch_loss=7.037418\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=7.037417888641357\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:40 INFO 139830709036864] Epoch[198] Batch[5] avg_epoch_loss=6.936390\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=6.936390399932861\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:40 INFO 139830709036864] Epoch[198] Batch [5]#011Speed: 228.17 samples/sec#011loss=6.936390\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] Epoch[198] Batch[10] avg_epoch_loss=7.087552\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=7.268945503234863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] Epoch[198] Batch [10]#011Speed: 219.52 samples/sec#011loss=7.268946\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173098.9805741, \"EndTime\": 1663173102.438306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3457.2832584381104, \"count\": 1, \"min\": 3457.2832584381104, \"max\": 3457.2832584381104}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.18429357107817 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 49.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=198, train loss <loss>=7.087551810524681\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:42 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:43 INFO 139830709036864] Epoch[199] Batch[0] avg_epoch_loss=6.931969\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=6.931969165802002\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:44 INFO 139830709036864] Epoch[199] Batch[5] avg_epoch_loss=6.988956\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=6.988956054051717\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:44 INFO 139830709036864] Epoch[199] Batch [5]#011Speed: 211.38 samples/sec#011loss=6.988956\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] Epoch[199] Batch[10] avg_epoch_loss=7.095804\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=7.224020767211914\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] Epoch[199] Batch [10]#011Speed: 209.91 samples/sec#011loss=7.224021\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173102.4383821, \"EndTime\": 1663173106.1091378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3670.2709197998047, \"count\": 1, \"min\": 3670.2709197998047, \"max\": 3670.2709197998047}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=182.81415202027654 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=199, train loss <loss>=7.095803650942716\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] Epoch[200] Batch[0] avg_epoch_loss=6.909010\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=6.90900993347168\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:48 INFO 139830709036864] Epoch[200] Batch[5] avg_epoch_loss=6.958545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=6.958544969558716\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:48 INFO 139830709036864] Epoch[200] Batch [5]#011Speed: 210.99 samples/sec#011loss=6.958545\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] Epoch[200] Batch[10] avg_epoch_loss=7.022531\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=7.099314022064209\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] Epoch[200] Batch [10]#011Speed: 214.22 samples/sec#011loss=7.099314\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173106.1092231, \"EndTime\": 1663173109.7661443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3656.503677368164, \"count\": 1, \"min\": 3656.503677368164, \"max\": 3656.503677368164}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=176.39212883875595 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] #progress_metric: host=algo-1, completed 50.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=200, train loss <loss>=7.022530902515758\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:49 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:50 INFO 139830709036864] Epoch[201] Batch[0] avg_epoch_loss=6.971398\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:50 INFO 139830709036864] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=6.97139835357666\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:51 INFO 139830709036864] Epoch[201] Batch[5] avg_epoch_loss=6.897973\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=6.897972583770752\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:51 INFO 139830709036864] Epoch[201] Batch [5]#011Speed: 211.43 samples/sec#011loss=6.897973\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173109.7662292, \"EndTime\": 1663173113.149306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3382.65061378479, \"count\": 1, \"min\": 3382.65061378479, \"max\": 3382.65061378479}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.12394488832342 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=201, train loss <loss>=6.926392030715943\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] Epoch[202] Batch[0] avg_epoch_loss=6.938714\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=6.938714027404785\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:55 INFO 139830709036864] Epoch[202] Batch[5] avg_epoch_loss=6.955336\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=6.955335696538289\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:55 INFO 139830709036864] Epoch[202] Batch [5]#011Speed: 228.16 samples/sec#011loss=6.955336\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173113.149399, \"EndTime\": 1663173116.298572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3148.5559940338135, \"count\": 1, \"min\": 3148.5559940338135, \"max\": 3148.5559940338135}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.1911876281753 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] #progress_metric: host=algo-1, completed 50.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=202, train loss <loss>=6.935204648971558\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] Epoch[203] Batch[0] avg_epoch_loss=6.935771\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=6.935770511627197\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:58 INFO 139830709036864] Epoch[203] Batch[5] avg_epoch_loss=6.961000\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=6.9609997272491455\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:58 INFO 139830709036864] Epoch[203] Batch [5]#011Speed: 217.74 samples/sec#011loss=6.961000\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] Epoch[203] Batch[10] avg_epoch_loss=6.969050\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=6.9787109375\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] Epoch[203] Batch [10]#011Speed: 230.08 samples/sec#011loss=6.978711\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173116.2986574, \"EndTime\": 1663173119.7822013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3483.057737350464, \"count\": 1, \"min\": 3483.057737350464, \"max\": 3483.057737350464}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.46213565212534 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=203, train loss <loss>=6.96905027736317\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:31:59 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:00 INFO 139830709036864] Epoch[204] Batch[0] avg_epoch_loss=7.032307\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=7.032306671142578\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:01 INFO 139830709036864] Epoch[204] Batch[5] avg_epoch_loss=7.005002\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=7.00500210126241\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:01 INFO 139830709036864] Epoch[204] Batch [5]#011Speed: 225.59 samples/sec#011loss=7.005002\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:02 INFO 139830709036864] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173119.782294, \"EndTime\": 1663173122.9815915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3198.8022327423096, \"count\": 1, \"min\": 3198.8022327423096, \"max\": 3198.8022327423096}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:02 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=195.06557232508186 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:02 INFO 139830709036864] #progress_metric: host=algo-1, completed 51.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=204, train loss <loss>=6.967676830291748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:02 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:03 INFO 139830709036864] Epoch[205] Batch[0] avg_epoch_loss=6.919812\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:03 INFO 139830709036864] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=6.919812202453613\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:05 INFO 139830709036864] Epoch[205] Batch[5] avg_epoch_loss=6.936256\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=6.936255772908528\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:05 INFO 139830709036864] Epoch[205] Batch [5]#011Speed: 212.91 samples/sec#011loss=6.936256\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:06 INFO 139830709036864] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173122.9816751, \"EndTime\": 1663173126.3697515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3387.641429901123, \"count\": 1, \"min\": 3387.641429901123, \"max\": 3387.641429901123}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:06 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.4392811868218 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:06 INFO 139830709036864] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=205, train loss <loss>=6.917666864395142\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:06 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:07 INFO 139830709036864] Epoch[206] Batch[0] avg_epoch_loss=6.910383\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:07 INFO 139830709036864] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=6.9103827476501465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:08 INFO 139830709036864] Epoch[206] Batch[5] avg_epoch_loss=6.938790\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=6.938790400822957\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:08 INFO 139830709036864] Epoch[206] Batch [5]#011Speed: 211.47 samples/sec#011loss=6.938790\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:09 INFO 139830709036864] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173126.3698354, \"EndTime\": 1663173129.6910117, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3320.7032680511475, \"count\": 1, \"min\": 3320.7032680511475, \"max\": 3320.7032680511475}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:09 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.01330463539975 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:09 INFO 139830709036864] #progress_metric: host=algo-1, completed 51.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:09 INFO 139830709036864] #quality_metric: host=algo-1, epoch=206, train loss <loss>=6.918647527694702\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:09 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:10 INFO 139830709036864] Epoch[207] Batch[0] avg_epoch_loss=6.924132\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=6.924132347106934\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:11 INFO 139830709036864] Epoch[207] Batch[5] avg_epoch_loss=6.899329\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=6.899329423904419\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:11 INFO 139830709036864] Epoch[207] Batch [5]#011Speed: 210.86 samples/sec#011loss=6.899329\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:12 INFO 139830709036864] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173129.6910872, \"EndTime\": 1663173132.9997544, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3308.1042766571045, \"count\": 1, \"min\": 3308.1042766571045, \"max\": 3308.1042766571045}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:12 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=175.3186358147104 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:13 INFO 139830709036864] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=207, train loss <loss>=6.892419147491455\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:13 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:13 INFO 139830709036864] Epoch[208] Batch[0] avg_epoch_loss=7.015460\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=7.015460014343262\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:15 INFO 139830709036864] Epoch[208] Batch[5] avg_epoch_loss=6.923293\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=6.923292716344197\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:15 INFO 139830709036864] Epoch[208] Batch [5]#011Speed: 211.71 samples/sec#011loss=6.923293\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] Epoch[208] Batch[10] avg_epoch_loss=6.904080\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=6.881024646759033\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] Epoch[208] Batch [10]#011Speed: 216.87 samples/sec#011loss=6.881025\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173132.999873, \"EndTime\": 1663173136.6345866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3634.239435195923, \"count\": 1, \"min\": 3634.239435195923, \"max\": 3634.239435195923}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.5719962240213 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] #progress_metric: host=algo-1, completed 52.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=208, train loss <loss>=6.9040799574418505\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:16 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:17 INFO 139830709036864] Epoch[209] Batch[0] avg_epoch_loss=7.030630\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:17 INFO 139830709036864] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=7.030629634857178\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:18 INFO 139830709036864] Epoch[209] Batch[5] avg_epoch_loss=6.926220\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=6.926220337549846\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:18 INFO 139830709036864] Epoch[209] Batch [5]#011Speed: 235.84 samples/sec#011loss=6.926220\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] Epoch[209] Batch[10] avg_epoch_loss=6.824627\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=6.702714538574218\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] Epoch[209] Batch [10]#011Speed: 214.14 samples/sec#011loss=6.702715\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173136.634696, \"EndTime\": 1663173140.0978217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3462.6965522766113, \"count\": 1, \"min\": 3462.6965522766113, \"max\": 3462.6965522766113}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.44154684952076 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=209, train loss <loss>=6.824626792560924\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] Epoch[210] Batch[0] avg_epoch_loss=6.778630\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=6.778630256652832\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:22 INFO 139830709036864] Epoch[210] Batch[5] avg_epoch_loss=6.859282\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=6.859281937281291\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:22 INFO 139830709036864] Epoch[210] Batch [5]#011Speed: 224.11 samples/sec#011loss=6.859282\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173140.0978994, \"EndTime\": 1663173143.3449574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3246.6437816619873, \"count\": 1, \"min\": 3246.6437816619873, \"max\": 3246.6437816619873}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.11064088269262 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] #progress_metric: host=algo-1, completed 52.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=210, train loss <loss>=6.879302930831909\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] Epoch[211] Batch[0] avg_epoch_loss=6.945312\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=6.945311546325684\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:25 INFO 139830709036864] Epoch[211] Batch[5] avg_epoch_loss=6.960257\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=6.960256735483806\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:25 INFO 139830709036864] Epoch[211] Batch [5]#011Speed: 210.97 samples/sec#011loss=6.960257\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:26 INFO 139830709036864] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173143.3450518, \"EndTime\": 1663173146.6037517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3258.2297325134277, \"count\": 1, \"min\": 3258.2297325134277, \"max\": 3258.2297325134277}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:26 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=185.67646590741546 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:26 INFO 139830709036864] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:26 INFO 139830709036864] #quality_metric: host=algo-1, epoch=211, train loss <loss>=6.942903280258179\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:26 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:27 INFO 139830709036864] Epoch[212] Batch[0] avg_epoch_loss=6.935693\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=6.935693264007568\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:28 INFO 139830709036864] Epoch[212] Batch[5] avg_epoch_loss=6.880263\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=6.880263090133667\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:28 INFO 139830709036864] Epoch[212] Batch [5]#011Speed: 215.64 samples/sec#011loss=6.880263\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:29 INFO 139830709036864] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173146.6038377, \"EndTime\": 1663173149.9234161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3319.084644317627, \"count\": 1, \"min\": 3319.084644317627, \"max\": 3319.084644317627}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:29 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.09192941102444 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:29 INFO 139830709036864] #progress_metric: host=algo-1, completed 53.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=212, train loss <loss>=6.880237054824829\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:29 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:30 INFO 139830709036864] Epoch[213] Batch[0] avg_epoch_loss=6.765332\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=6.765332221984863\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:32 INFO 139830709036864] Epoch[213] Batch[5] avg_epoch_loss=6.878245\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=6.87824543317159\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:32 INFO 139830709036864] Epoch[213] Batch [5]#011Speed: 213.18 samples/sec#011loss=6.878245\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] Epoch[213] Batch[10] avg_epoch_loss=6.894370\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=6.91371898651123\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] Epoch[213] Batch [10]#011Speed: 207.50 samples/sec#011loss=6.913719\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173149.9235132, \"EndTime\": 1663173153.5969164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3672.8153228759766, \"count\": 1, \"min\": 3672.8153228759766, \"max\": 3672.8153228759766}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.8749876586244 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=213, train loss <loss>=6.894369775598699\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:33 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:34 INFO 139830709036864] Epoch[214] Batch[0] avg_epoch_loss=6.921603\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=6.921603202819824\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:35 INFO 139830709036864] Epoch[214] Batch[5] avg_epoch_loss=6.899408\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=6.8994081020355225\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:35 INFO 139830709036864] Epoch[214] Batch [5]#011Speed: 218.26 samples/sec#011loss=6.899408\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:36 INFO 139830709036864] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173153.5970135, \"EndTime\": 1663173156.9186041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3321.035861968994, \"count\": 1, \"min\": 3321.035861968994, \"max\": 3321.035861968994}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:36 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.79973477400262 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:36 INFO 139830709036864] #progress_metric: host=algo-1, completed 53.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=214, train loss <loss>=6.902569341659546\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:36 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:37 INFO 139830709036864] Epoch[215] Batch[0] avg_epoch_loss=6.962015\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:37 INFO 139830709036864] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=6.962014675140381\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:39 INFO 139830709036864] Epoch[215] Batch[5] avg_epoch_loss=6.801540\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=6.801540056864421\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:39 INFO 139830709036864] Epoch[215] Batch [5]#011Speed: 211.69 samples/sec#011loss=6.801540\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] Epoch[215] Batch[10] avg_epoch_loss=6.853673\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=6.916233158111572\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] Epoch[215] Batch [10]#011Speed: 205.82 samples/sec#011loss=6.916233\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173156.9186952, \"EndTime\": 1663173160.6280158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3708.8096141815186, \"count\": 1, \"min\": 3708.8096141815186, \"max\": 3708.8096141815186}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.11570728988022 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] #quality_metric: host=algo-1, epoch=215, train loss <loss>=6.853673284704035\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:40 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:41 INFO 139830709036864] Epoch[216] Batch[0] avg_epoch_loss=6.738820\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=6.7388200759887695\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:42 INFO 139830709036864] Epoch[216] Batch[5] avg_epoch_loss=6.894726\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=6.894725799560547\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:42 INFO 139830709036864] Epoch[216] Batch [5]#011Speed: 218.14 samples/sec#011loss=6.894726\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:43 INFO 139830709036864] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173160.6281033, \"EndTime\": 1663173163.909478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3280.9319496154785, \"count\": 1, \"min\": 3280.9319496154785, \"max\": 3280.9319496154785}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:43 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.31549379615677 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:43 INFO 139830709036864] #progress_metric: host=algo-1, completed 54.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:43 INFO 139830709036864] #quality_metric: host=algo-1, epoch=216, train loss <loss>=6.8949426174163815\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:43 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:44 INFO 139830709036864] Epoch[217] Batch[0] avg_epoch_loss=6.863791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:44 INFO 139830709036864] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=6.863790988922119\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:46 INFO 139830709036864] Epoch[217] Batch[5] avg_epoch_loss=6.839752\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:46 INFO 139830709036864] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=6.839751720428467\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:46 INFO 139830709036864] Epoch[217] Batch [5]#011Speed: 215.69 samples/sec#011loss=6.839752\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] Epoch[217] Batch[10] avg_epoch_loss=6.896913\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=6.965506935119629\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] Epoch[217] Batch [10]#011Speed: 208.74 samples/sec#011loss=6.965507\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173163.9095726, \"EndTime\": 1663173167.5790305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3668.9577102661133, \"count\": 1, \"min\": 3668.9577102661133, \"max\": 3668.9577102661133}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=176.06301650502823 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] #quality_metric: host=algo-1, epoch=217, train loss <loss>=6.896913181651723\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:47 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:48 INFO 139830709036864] Epoch[218] Batch[0] avg_epoch_loss=6.891999\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:48 INFO 139830709036864] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=6.891999244689941\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:49 INFO 139830709036864] Epoch[218] Batch[5] avg_epoch_loss=6.852408\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:49 INFO 139830709036864] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=6.852407693862915\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:49 INFO 139830709036864] Epoch[218] Batch [5]#011Speed: 223.72 samples/sec#011loss=6.852408\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] Epoch[218] Batch[10] avg_epoch_loss=6.911572\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=6.982569694519043\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] Epoch[218] Batch [10]#011Speed: 218.90 samples/sec#011loss=6.982570\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173167.5791385, \"EndTime\": 1663173171.091269, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3511.535167694092, \"count\": 1, \"min\": 3511.535167694092, \"max\": 3511.535167694092}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.80685598082846 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] #progress_metric: host=algo-1, completed 54.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=218, train loss <loss>=6.9115722396157\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] Epoch[219] Batch[0] avg_epoch_loss=6.901961\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:51 INFO 139830709036864] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=6.901961326599121\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:53 INFO 139830709036864] Epoch[219] Batch[5] avg_epoch_loss=6.882997\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:53 INFO 139830709036864] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=6.882997035980225\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:53 INFO 139830709036864] Epoch[219] Batch [5]#011Speed: 211.91 samples/sec#011loss=6.882997\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:54 INFO 139830709036864] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173171.0913446, \"EndTime\": 1663173174.4282296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3336.3823890686035, \"count\": 1, \"min\": 3336.3823890686035, \"max\": 3336.3823890686035}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:54 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=190.318819995794 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:54 INFO 139830709036864] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:54 INFO 139830709036864] #quality_metric: host=algo-1, epoch=219, train loss <loss>=6.864545202255249\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:54 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:55 INFO 139830709036864] Epoch[220] Batch[0] avg_epoch_loss=6.903473\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:55 INFO 139830709036864] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=6.903473377227783\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:56 INFO 139830709036864] Epoch[220] Batch[5] avg_epoch_loss=6.921448\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:56 INFO 139830709036864] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=6.921447515487671\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:56 INFO 139830709036864] Epoch[220] Batch [5]#011Speed: 213.94 samples/sec#011loss=6.921448\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:57 INFO 139830709036864] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173174.4283152, \"EndTime\": 1663173177.688193, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3259.410858154297, \"count\": 1, \"min\": 3259.410858154297, \"max\": 3259.410858154297}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:57 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=192.3586512834084 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:57 INFO 139830709036864] #progress_metric: host=algo-1, completed 55.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:57 INFO 139830709036864] #quality_metric: host=algo-1, epoch=220, train loss <loss>=6.888004016876221\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:57 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:58 INFO 139830709036864] Epoch[221] Batch[0] avg_epoch_loss=6.921305\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:58 INFO 139830709036864] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=6.921305179595947\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:59 INFO 139830709036864] Epoch[221] Batch[5] avg_epoch_loss=6.897135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:59 INFO 139830709036864] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=6.897135257720947\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:32:59 INFO 139830709036864] Epoch[221] Batch [5]#011Speed: 218.97 samples/sec#011loss=6.897135\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:00 INFO 139830709036864] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173177.6882765, \"EndTime\": 1663173180.8582778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3169.4185733795166, \"count\": 1, \"min\": 3169.4185733795166, \"max\": 3169.4185733795166}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:00 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=197.18871583842628 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:00 INFO 139830709036864] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:00 INFO 139830709036864] #quality_metric: host=algo-1, epoch=221, train loss <loss>=6.895331716537475\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:00 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:01 INFO 139830709036864] Epoch[222] Batch[0] avg_epoch_loss=6.921718\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:01 INFO 139830709036864] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=6.921718120574951\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:02 INFO 139830709036864] Epoch[222] Batch[5] avg_epoch_loss=6.829559\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:02 INFO 139830709036864] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=6.829558690388997\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:02 INFO 139830709036864] Epoch[222] Batch [5]#011Speed: 216.61 samples/sec#011loss=6.829559\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] Epoch[222] Batch[10] avg_epoch_loss=6.982788\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=222, batch=10 train loss <loss>=7.166662788391113\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] Epoch[222] Batch [10]#011Speed: 211.26 samples/sec#011loss=7.166663\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173180.8583724, \"EndTime\": 1663173184.4949963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3635.9312534332275, \"count\": 1, \"min\": 3635.9312534332275, \"max\": 3635.9312534332275}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=184.2654224339832 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] #progress_metric: host=algo-1, completed 55.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] #quality_metric: host=algo-1, epoch=222, train loss <loss>=6.982787825844505\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:04 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:05 INFO 139830709036864] Epoch[223] Batch[0] avg_epoch_loss=6.915812\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:05 INFO 139830709036864] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=6.9158124923706055\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:06 INFO 139830709036864] Epoch[223] Batch[5] avg_epoch_loss=6.919192\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:06 INFO 139830709036864] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=6.919192393620809\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:06 INFO 139830709036864] Epoch[223] Batch [5]#011Speed: 211.50 samples/sec#011loss=6.919192\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] Epoch[223] Batch[10] avg_epoch_loss=6.868508\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=6.807686614990234\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] Epoch[223] Batch [10]#011Speed: 203.06 samples/sec#011loss=6.807687\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173184.495087, \"EndTime\": 1663173188.25107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3755.4688453674316, \"count\": 1, \"min\": 3755.4688453674316, \"max\": 3755.4688453674316}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=178.93376040174599 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=223, train loss <loss>=6.8685079487887295\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] Epoch[224] Batch[0] avg_epoch_loss=6.764020\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:08 INFO 139830709036864] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=6.7640204429626465\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:10 INFO 139830709036864] Epoch[224] Batch[5] avg_epoch_loss=6.885503\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:10 INFO 139830709036864] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=6.88550345102946\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:10 INFO 139830709036864] Epoch[224] Batch [5]#011Speed: 225.46 samples/sec#011loss=6.885503\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:11 INFO 139830709036864] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173188.2511442, \"EndTime\": 1663173191.4724982, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3220.903158187866, \"count\": 1, \"min\": 3220.903158187866, \"max\": 3220.903158187866}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:11 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=191.23347779206114 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:11 INFO 139830709036864] #progress_metric: host=algo-1, completed 56.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:11 INFO 139830709036864] #quality_metric: host=algo-1, epoch=224, train loss <loss>=6.884218740463257\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:11 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:12 INFO 139830709036864] Epoch[225] Batch[0] avg_epoch_loss=6.906710\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:12 INFO 139830709036864] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=6.906710147857666\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:13 INFO 139830709036864] Epoch[225] Batch[5] avg_epoch_loss=6.849782\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:13 INFO 139830709036864] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=6.8497819900512695\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:13 INFO 139830709036864] Epoch[225] Batch [5]#011Speed: 224.98 samples/sec#011loss=6.849782\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:14 INFO 139830709036864] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173191.4727445, \"EndTime\": 1663173194.7268927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3253.5223960876465, \"count\": 1, \"min\": 3253.5223960876465, \"max\": 3253.5223960876465}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:14 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=194.2328161705333 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:14 INFO 139830709036864] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:14 INFO 139830709036864] #quality_metric: host=algo-1, epoch=225, train loss <loss>=6.844298887252807\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:14 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:15 INFO 139830709036864] Epoch[226] Batch[0] avg_epoch_loss=6.799358\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:15 INFO 139830709036864] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=6.799358367919922\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:16 INFO 139830709036864] Epoch[226] Batch[5] avg_epoch_loss=6.844766\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:16 INFO 139830709036864] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=6.844765901565552\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:16 INFO 139830709036864] Epoch[226] Batch [5]#011Speed: 215.92 samples/sec#011loss=6.844766\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] Epoch[226] Batch[10] avg_epoch_loss=6.891282\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=226, batch=10 train loss <loss>=6.947101306915283\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] Epoch[226] Batch [10]#011Speed: 207.94 samples/sec#011loss=6.947101\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173194.7271545, \"EndTime\": 1663173198.3883953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3660.72416305542, \"count\": 1, \"min\": 3660.72416305542, \"max\": 3660.72416305542}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=180.0135396047245 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] #progress_metric: host=algo-1, completed 56.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] #quality_metric: host=algo-1, epoch=226, train loss <loss>=6.891281994906339\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:18 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:19 INFO 139830709036864] Epoch[227] Batch[0] avg_epoch_loss=6.871437\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:19 INFO 139830709036864] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=6.871436595916748\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:20 INFO 139830709036864] Epoch[227] Batch[5] avg_epoch_loss=6.766613\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:20 INFO 139830709036864] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=6.766613086064656\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:20 INFO 139830709036864] Epoch[227] Batch [5]#011Speed: 215.44 samples/sec#011loss=6.766613\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:21 INFO 139830709036864] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173198.3884735, \"EndTime\": 1663173201.731406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3342.4134254455566, \"count\": 1, \"min\": 3342.4134254455566, \"max\": 3342.4134254455566}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:21 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.37981847975547 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:21 INFO 139830709036864] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:21 INFO 139830709036864] #quality_metric: host=algo-1, epoch=227, train loss <loss>=6.8002711772918705\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:21 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:22 INFO 139830709036864] Epoch[228] Batch[0] avg_epoch_loss=6.760168\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:22 INFO 139830709036864] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=6.760167598724365\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:23 INFO 139830709036864] Epoch[228] Batch[5] avg_epoch_loss=6.856387\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:23 INFO 139830709036864] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=6.856386582056682\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:23 INFO 139830709036864] Epoch[228] Batch [5]#011Speed: 211.40 samples/sec#011loss=6.856387\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] Epoch[228] Batch[10] avg_epoch_loss=6.874444\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=228, batch=10 train loss <loss>=6.896112060546875\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] Epoch[228] Batch [10]#011Speed: 225.11 samples/sec#011loss=6.896112\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173201.7315598, \"EndTime\": 1663173205.2982934, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3565.988779067993, \"count\": 1, \"min\": 3565.988779067993, \"max\": 3565.988779067993}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=188.72159144166736 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] #progress_metric: host=algo-1, completed 57.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=228, train loss <loss>=6.874443617734042\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] Epoch[229] Batch[0] avg_epoch_loss=6.985917\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:25 INFO 139830709036864] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=6.985916614532471\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:27 INFO 139830709036864] Epoch[229] Batch[5] avg_epoch_loss=6.888800\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:27 INFO 139830709036864] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=6.888799508412679\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:27 INFO 139830709036864] Epoch[229] Batch [5]#011Speed: 236.04 samples/sec#011loss=6.888800\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] Epoch[229] Batch[10] avg_epoch_loss=6.820191\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=6.737860584259034\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] Epoch[229] Batch [10]#011Speed: 215.68 samples/sec#011loss=6.737861\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173205.2983673, \"EndTime\": 1663173208.7801845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3481.322765350342, \"count\": 1, \"min\": 3481.322765350342, \"max\": 3481.322765350342}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=189.2895635264027 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] #quality_metric: host=algo-1, epoch=229, train loss <loss>=6.820190906524658\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:28 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:29 INFO 139830709036864] Epoch[230] Batch[0] avg_epoch_loss=6.979769\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:29 INFO 139830709036864] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=6.979769229888916\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:30 INFO 139830709036864] Epoch[230] Batch[5] avg_epoch_loss=6.906406\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:30 INFO 139830709036864] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=6.906406005223592\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:30 INFO 139830709036864] Epoch[230] Batch [5]#011Speed: 211.63 samples/sec#011loss=6.906406\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] Epoch[230] Batch[10] avg_epoch_loss=6.863762\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=6.812589263916015\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] Epoch[230] Batch [10]#011Speed: 205.70 samples/sec#011loss=6.812589\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173208.7802627, \"EndTime\": 1663173212.4725845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3691.8861865997314, \"count\": 1, \"min\": 3691.8861865997314, \"max\": 3691.8861865997314}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=186.34856598749982 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] #progress_metric: host=algo-1, completed 57.75 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] #quality_metric: host=algo-1, epoch=230, train loss <loss>=6.863762031901967\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:32 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:33 INFO 139830709036864] Epoch[231] Batch[0] avg_epoch_loss=6.922048\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:33 INFO 139830709036864] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=6.922048091888428\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:34 INFO 139830709036864] Epoch[231] Batch[5] avg_epoch_loss=6.881747\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:34 INFO 139830709036864] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=6.881746927897136\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:34 INFO 139830709036864] Epoch[231] Batch [5]#011Speed: 232.17 samples/sec#011loss=6.881747\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] Epoch[231] Batch[10] avg_epoch_loss=6.872713\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=6.861871719360352\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] Epoch[231] Batch [10]#011Speed: 218.45 samples/sec#011loss=6.861872\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173212.4726682, \"EndTime\": 1663173215.943726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3470.6568717956543, \"count\": 1, \"min\": 3470.6568717956543, \"max\": 3470.6568717956543}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=187.5659001228713 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] #quality_metric: host=algo-1, epoch=231, train loss <loss>=6.872712742198598\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:35 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:36 INFO 139830709036864] Epoch[232] Batch[0] avg_epoch_loss=6.810552\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:36 INFO 139830709036864] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=6.810551643371582\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:38 INFO 139830709036864] Epoch[232] Batch[5] avg_epoch_loss=6.835977\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:38 INFO 139830709036864] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=6.835976521174113\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:38 INFO 139830709036864] Epoch[232] Batch [5]#011Speed: 211.60 samples/sec#011loss=6.835977\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173215.943812, \"EndTime\": 1663173219.291604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3347.3618030548096, \"count\": 1, \"min\": 3347.3618030548096, \"max\": 3347.3618030548096}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=183.12200183299464 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] #progress_metric: host=algo-1, completed 58.25 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=232, train loss <loss>=6.845485067367553\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] Epoch[233] Batch[0] avg_epoch_loss=6.825451\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:39 INFO 139830709036864] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=6.825450897216797\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:41 INFO 139830709036864] Epoch[233] Batch[5] avg_epoch_loss=6.791605\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:41 INFO 139830709036864] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=6.7916050752003985\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:41 INFO 139830709036864] Epoch[233] Batch [5]#011Speed: 216.19 samples/sec#011loss=6.791605\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173219.2916982, \"EndTime\": 1663173222.6377866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3345.6201553344727, \"count\": 1, \"min\": 3345.6201553344727, \"max\": 3345.6201553344727}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] #throughput_metric: host=algo-1, train throughput=179.6307035662668 records/second\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] #quality_metric: host=algo-1, epoch=233, train loss <loss>=6.816254091262818\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] Loading parameters from best epoch (193)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173222.6378798, \"EndTime\": 1663173222.6707804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 32.34124183654785, \"count\": 1, \"min\": 32.34124183654785, \"max\": 32.34124183654785}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] stopping training now\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] Final loss: 6.726061690937389 (occurred at epoch 193)\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] #quality_metric: host=algo-1, train final_loss <loss>=6.726061690937389\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 WARNING 139830709036864] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:42 INFO 139830709036864] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173222.670854, \"EndTime\": 1663173223.658016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 986.2155914306641, \"count\": 1, \"min\": 986.2155914306641, \"max\": 986.2155914306641}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173223.6580887, \"EndTime\": 1663173223.9417202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 1269.9708938598633, \"count\": 1, \"min\": 1269.9708938598633, \"max\": 1269.9708938598633}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173223.9417846, \"EndTime\": 1663173223.983002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 41.17846488952637, \"count\": 1, \"min\": 41.17846488952637, \"max\": 41.17846488952637}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] #memory_usage::<batchbuffer> = 25.01220703125 mb\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:43 INFO 139830709036864] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173223.9830627, \"EndTime\": 1663173223.989387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04744529724121094, \"count\": 1, \"min\": 0.04744529724121094, \"max\": 0.04744529724121094}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173223.989444, \"EndTime\": 1663173228.7860465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 4796.704530715942, \"count\": 1, \"min\": 4796.704530715942, \"max\": 4796.704530715942}}}\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, RMSE): 2189.495065077791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, mean_absolute_QuantileLoss): 112250.19115100436\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, mean_wQuantileLoss): 0.5341263108183277\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.1]): 0.24884985303233634\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.2]): 0.37797194786231814\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.3]): 0.5023324957824922\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.4]): 0.6118953145968827\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.5]): 0.6930426210889506\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.6]): 0.71216962875069\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.7]): 0.6866381068820039\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.8]): 0.5800033920708325\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #test_score (algo-1, wQuantileLoss[0.9]): 0.39423343729844373\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #quality_metric: host=algo-1, test RMSE <loss>=2189.495065077791\u001b[0m\n",
      "\u001b[34m[09/14/2022 16:33:48 INFO 139830709036864] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.5341263108183277\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1663173228.78615, \"EndTime\": 1663173228.8609676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 13.970136642456055, \"count\": 1, \"min\": 13.970136642456055, \"max\": 13.970136642456055}, \"totaltime\": {\"sum\": 810153.3064842224, \"count\": 1, \"min\": 810153.3064842224, \"max\": 810153.3064842224}}}\u001b[0m\n",
      "\n",
      "2022-09-14 16:33:58 Uploading - Uploading generated training model\n",
      "2022-09-14 16:34:39 Completed - Training job completed\n",
      "ProfilerReport-1663172229: NoIssuesFound\n",
      "Training seconds: 935\n",
      "Billable seconds: 935\n",
      "CPU times: user 2.46 s, sys: 252 ms, total: 2.71 s\n",
      "Wall time: 17min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c996be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "23d82fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "598c8772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "67e590ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31527/1241095361.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31527/188640761.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=series, quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8ca19b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = df.squeeze(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f60e4c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order Date\n",
       "2015-01-02     468.9000\n",
       "2015-01-03    2203.1510\n",
       "2015-01-04     119.8880\n",
       "2015-01-05       0.0000\n",
       "2015-01-06    5188.5200\n",
       "                ...    \n",
       "2018-12-26     814.5940\n",
       "2018-12-27     177.6360\n",
       "2018-12-28    1657.3508\n",
       "2018-12-29    2915.5340\n",
       "2018-12-30     713.7900\n",
       "Freq: D, Name: Sales, Length: 1459, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "70536bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 468.9 , 2203.15,  119.89, ..., 4536.94, 1403.84, 4791.35])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traget_array_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "65d6b782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[468.9,\n",
       " 2203.15,\n",
       " 119.89,\n",
       " 0.0,\n",
       " 5188.52,\n",
       " 601.02,\n",
       " 228.33,\n",
       " 469.44,\n",
       " 4.71,\n",
       " 4637.82,\n",
       " 5624.39,\n",
       " 3553.8,\n",
       " 61.96,\n",
       " 149.95,\n",
       " 299.96,\n",
       " 0.0,\n",
       " 64.86,\n",
       " 378.59,\n",
       " 2673.87,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 40.08,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1097.25,\n",
       " 426.67,\n",
       " 3.93,\n",
       " 0.0,\n",
       " 240.5,\n",
       " 290.67,\n",
       " 0.0,\n",
       " 211.65,\n",
       " 47.76,\n",
       " 1458.56,\n",
       " 506.12,\n",
       " 738.6,\n",
       " 79.56,\n",
       " 886.28,\n",
       " 3254.15,\n",
       " 598.14,\n",
       " 2126.37,\n",
       " 1771.81,\n",
       " 0.0,\n",
       " 576.73,\n",
       " 21.36,\n",
       " 9.04,\n",
       " 54.21,\n",
       " 37.78,\n",
       " 0.0,\n",
       " 95.59,\n",
       " 8.85,\n",
       " 19.44,\n",
       " 11.36,\n",
       " 55.67,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 19.46,\n",
       " 0.0,\n",
       " 16.45,\n",
       " 97.11,\n",
       " 1345.89,\n",
       " 0.0,\n",
       " 72.63,\n",
       " 531.16,\n",
       " 0.0,\n",
       " 477.99,\n",
       " 22.08,\n",
       " 862.68,\n",
       " 2784.16,\n",
       " 505.88,\n",
       " 0.0,\n",
       " 2108.55,\n",
       " 370.78,\n",
       " 471.92,\n",
       " 3960.36,\n",
       " 28106.72,\n",
       " 590.76,\n",
       " 0.0,\n",
       " 4109.82,\n",
       " 464.09,\n",
       " 945.06,\n",
       " 65.38,\n",
       " 459.15,\n",
       " 145.13,\n",
       " 0.0,\n",
       " 1493.22,\n",
       " 890.84,\n",
       " 1170.32,\n",
       " 1959.55,\n",
       " 288.06,\n",
       " 134.38,\n",
       " 370.45,\n",
       " 475.84,\n",
       " 187.43,\n",
       " 456.27,\n",
       " 247.41,\n",
       " 2743.15,\n",
       " 0.0,\n",
       " 728.75,\n",
       " 6693.75,\n",
       " 129.98,\n",
       " 638.58,\n",
       " 0.0,\n",
       " 294.72,\n",
       " 39.07,\n",
       " 0.0,\n",
       " 1020.53,\n",
       " 205.47,\n",
       " 1250.45,\n",
       " 845.36,\n",
       " 257.75,\n",
       " 643.98,\n",
       " 0.0,\n",
       " 2379.99,\n",
       " 282.57,\n",
       " 0.0,\n",
       " 2578.48,\n",
       " 768.84,\n",
       " 1048.74,\n",
       " 19.54,\n",
       " 0.0,\n",
       " 705.56,\n",
       " 5179.74,\n",
       " 260.98,\n",
       " 0.0,\n",
       " 1444.1,\n",
       " 2408.72,\n",
       " 296.3,\n",
       " 191.9,\n",
       " 1694.56,\n",
       " 2963.64,\n",
       " 898.38,\n",
       " 310.88,\n",
       " 0.0,\n",
       " 289.28,\n",
       " 91.68,\n",
       " 944.96,\n",
       " 91.62,\n",
       " 320.1,\n",
       " 4264.33,\n",
       " 180.93,\n",
       " 323.76,\n",
       " 116.28,\n",
       " 119.54,\n",
       " 1325.72,\n",
       " 2157.39,\n",
       " 221.97,\n",
       " 0.0,\n",
       " 1924.91,\n",
       " 773.7,\n",
       " 4407.1,\n",
       " 330.51,\n",
       " 0.0,\n",
       " 1650.69,\n",
       " 254.46,\n",
       " 1374.0,\n",
       " 1038.47,\n",
       " 262.89,\n",
       " 100.36,\n",
       " 494.71,\n",
       " 0.0,\n",
       " 1970.98,\n",
       " 14.52,\n",
       " 212.94,\n",
       " 942.97,\n",
       " 763.79,\n",
       " 3356.49,\n",
       " 139.8,\n",
       " 0.0,\n",
       " 1828.95,\n",
       " 4107.44,\n",
       " 1975.5,\n",
       " 109.5,\n",
       " 4.27,\n",
       " 792.76,\n",
       " 0.0,\n",
       " 616.14,\n",
       " 1645.79,\n",
       " 46.68,\n",
       " 739.62,\n",
       " 87.16,\n",
       " 180.32,\n",
       " 1561.06,\n",
       " 1021.17,\n",
       " 1091.35,\n",
       " 281.4,\n",
       " 241.19,\n",
       " 0.0,\n",
       " 3523.88,\n",
       " 237.36,\n",
       " 1302.45,\n",
       " 548.4,\n",
       " 351.22,\n",
       " 292.59,\n",
       " 9.51,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 273.04,\n",
       " 461.33,\n",
       " 2285.79,\n",
       " 1961.48,\n",
       " 1348.34,\n",
       " 2613.18,\n",
       " 0.0,\n",
       " 8341.29,\n",
       " 5039.99,\n",
       " 580.06,\n",
       " 193.15,\n",
       " 0.0,\n",
       " 1367.84,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 14.56,\n",
       " 0.0,\n",
       " 1958.75,\n",
       " 1799.97,\n",
       " 2501.26,\n",
       " 763.07,\n",
       " 3955.63,\n",
       " 14228.43,\n",
       " 147.39,\n",
       " 333.58,\n",
       " 1095.12,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 337.5,\n",
       " 853.09,\n",
       " 500.34,\n",
       " 0.0,\n",
       " 1439.11,\n",
       " 1838.72,\n",
       " 0.0,\n",
       " 23.1,\n",
       " 565.22,\n",
       " 59.74,\n",
       " 1451.65,\n",
       " 366.81,\n",
       " 2070.13,\n",
       " 0.0,\n",
       " 832.32,\n",
       " 121.24,\n",
       " 92.52,\n",
       " 40.54,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 394.48,\n",
       " 5463.01,\n",
       " 265.52,\n",
       " 3605.68,\n",
       " 4043.59,\n",
       " 249.36,\n",
       " 1574.09,\n",
       " 1522.65,\n",
       " 1275.41,\n",
       " 5023.72,\n",
       " 133.44,\n",
       " 33.55,\n",
       " 820.52,\n",
       " 0.0,\n",
       " 9338.64,\n",
       " 5335.97,\n",
       " 2679.65,\n",
       " 1240.27,\n",
       " 10662.34,\n",
       " 211.96,\n",
       " 354.89,\n",
       " 1326.79,\n",
       " 773.44,\n",
       " 490.29,\n",
       " 8109.07,\n",
       " 987.53,\n",
       " 54.83,\n",
       " 0.0,\n",
       " 741.96,\n",
       " 0.0,\n",
       " 2343.8,\n",
       " 491.55,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1855.23,\n",
       " 2049.39,\n",
       " 2002.56,\n",
       " 573.34,\n",
       " 2212.18,\n",
       " 4525.48,\n",
       " 206.72,\n",
       " 194.32,\n",
       " 136.07,\n",
       " 3134.92,\n",
       " 641.1,\n",
       " 1501.1,\n",
       " 1356.6,\n",
       " 223.81,\n",
       " 0.0,\n",
       " 56.49,\n",
       " 103.88,\n",
       " 447.88,\n",
       " 22.32,\n",
       " 1510.58,\n",
       " 2813.81,\n",
       " 0.0,\n",
       " 5362.03,\n",
       " 9.94,\n",
       " 2043.4,\n",
       " 918.31,\n",
       " 1915.24,\n",
       " 1642.8,\n",
       " 0.0,\n",
       " 2022.36,\n",
       " 451.62,\n",
       " 127.95,\n",
       " 1381.16,\n",
       " 7836.98,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1034.73,\n",
       " 1780.82,\n",
       " 1355.93,\n",
       " 11544.27,\n",
       " 3406.86,\n",
       " 7226.09,\n",
       " 907.56,\n",
       " 1249.62,\n",
       " 1103.3,\n",
       " 1363.28,\n",
       " 5572.86,\n",
       " 4415.7,\n",
       " 1705.99,\n",
       " 203.66,\n",
       " 2280.83,\n",
       " 649.04,\n",
       " 219.15,\n",
       " 0.0,\n",
       " 129.57,\n",
       " 0.0,\n",
       " 1872.44,\n",
       " 734.85,\n",
       " 0.0,\n",
       " 1960.01,\n",
       " 1545.73,\n",
       " 4904.66,\n",
       " 58.53,\n",
       " 901.65,\n",
       " 4938.4,\n",
       " 293.15,\n",
       " 7641.66,\n",
       " 1264.47,\n",
       " 4065.6,\n",
       " 45.53,\n",
       " 0.0,\n",
       " 3436.76,\n",
       " 6962.44,\n",
       " 1933.44,\n",
       " 798.97,\n",
       " 942.11,\n",
       " 1950.2,\n",
       " 0.0,\n",
       " 2041.41,\n",
       " 2966.39,\n",
       " 2682.33,\n",
       " 2072.1,\n",
       " 2172.65,\n",
       " 5253.27,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1642.17,\n",
       " 0.0,\n",
       " 450.94,\n",
       " 834.2,\n",
       " 0.0,\n",
       " 1011.7,\n",
       " 2367.56,\n",
       " 1158.41,\n",
       " 3106.11,\n",
       " 7130.59,\n",
       " 622.28,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 350.38,\n",
       " 0.0,\n",
       " 301.74,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 43.66,\n",
       " 13.12,\n",
       " 0.0,\n",
       " 182.72,\n",
       " 3573.25,\n",
       " 4297.64,\n",
       " 0.0,\n",
       " 2161.64,\n",
       " 99.26,\n",
       " 1932.1,\n",
       " 0.0,\n",
       " 899.57,\n",
       " 299.22,\n",
       " 167.23,\n",
       " 0.0,\n",
       " 1631.9,\n",
       " 1290.48,\n",
       " 0.0,\n",
       " 1269.27,\n",
       " 5528.55,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1079.96,\n",
       " 1602.38,\n",
       " 160.43,\n",
       " 0.0,\n",
       " 105.84,\n",
       " 0.0,\n",
       " 316.78,\n",
       " 2591.1,\n",
       " 79.36,\n",
       " 37.78,\n",
       " 0.0,\n",
       " 25.87,\n",
       " 0.0,\n",
       " 1159.27,\n",
       " 551.26,\n",
       " 0.0,\n",
       " 1768.22,\n",
       " 492.84,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2198.76,\n",
       " 0.0,\n",
       " 823.98,\n",
       " 0.0,\n",
       " 1874.9,\n",
       " 275.22,\n",
       " 2800.85,\n",
       " 1972.64,\n",
       " 1384.86,\n",
       " 707.9,\n",
       " 16.78,\n",
       " 7230.21,\n",
       " 48.84,\n",
       " 0.0,\n",
       " 5203.89,\n",
       " 102.52,\n",
       " 1270.38,\n",
       " 2481.42,\n",
       " 633.3,\n",
       " 412.95,\n",
       " 0.0,\n",
       " 131.16,\n",
       " 83.7,\n",
       " 243.34,\n",
       " 1559.08,\n",
       " 571.22,\n",
       " 200.72,\n",
       " 264.46,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1484.6,\n",
       " 1121.36,\n",
       " 1192.31,\n",
       " 1138.29,\n",
       " 0.0,\n",
       " 1071.28,\n",
       " 505.58,\n",
       " 0.0,\n",
       " 5185.81,\n",
       " 3192.68,\n",
       " 35.21,\n",
       " 0.0,\n",
       " 4161.97,\n",
       " 2745.11,\n",
       " 2006.96,\n",
       " 329.96,\n",
       " 1194.73,\n",
       " 893.09,\n",
       " 152.92,\n",
       " 0.0,\n",
       " 25.99,\n",
       " 920.95,\n",
       " 2090.11,\n",
       " 2719.59,\n",
       " 4044.36,\n",
       " 7.97,\n",
       " 1571.7,\n",
       " 295.27,\n",
       " 0.0,\n",
       " 1969.27,\n",
       " 2974.06,\n",
       " 0.0,\n",
       " 1533.2,\n",
       " 121.65,\n",
       " 72.65,\n",
       " 934.1,\n",
       " 851.09,\n",
       " 834.87,\n",
       " 897.32,\n",
       " 238.38,\n",
       " 3141.2,\n",
       " 69.91,\n",
       " 255.97,\n",
       " 47.37,\n",
       " 10.86,\n",
       " 0.0,\n",
       " 169.54,\n",
       " 72.33,\n",
       " 3862.1,\n",
       " 1304.96,\n",
       " 610.31,\n",
       " 1739.42,\n",
       " 982.56,\n",
       " 0.0,\n",
       " 794.15,\n",
       " 1119.79,\n",
       " 681.1,\n",
       " 5178.14,\n",
       " 31.54,\n",
       " 2432.54,\n",
       " 1555.24,\n",
       " 140.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1379.81,\n",
       " 494.87,\n",
       " 919.67,\n",
       " 0.0,\n",
       " 11.48,\n",
       " 6061.5,\n",
       " 1128.9,\n",
       " 51.07,\n",
       " 734.14,\n",
       " 3443.9,\n",
       " 0.0,\n",
       " 1883.09,\n",
       " 446.35,\n",
       " 829.54,\n",
       " 107.98,\n",
       " 2018.95,\n",
       " 836.96,\n",
       " 0.0,\n",
       " 857.08,\n",
       " 1592.2,\n",
       " 0.0,\n",
       " 1126.16,\n",
       " 432.93,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 324.11,\n",
       " 807.54,\n",
       " 907.44,\n",
       " 305.62,\n",
       " 59.77,\n",
       " 0.0,\n",
       " 968.21,\n",
       " 2922.67,\n",
       " 0.0,\n",
       " 3073.07,\n",
       " 1338.3,\n",
       " 2033.26,\n",
       " 860.92,\n",
       " 0.0,\n",
       " 3029.39,\n",
       " 448.08,\n",
       " 525.72,\n",
       " 2.02,\n",
       " 2693.17,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 208.27,\n",
       " 525.95,\n",
       " 2395.02,\n",
       " 958.86,\n",
       " 29.97,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 443.8,\n",
       " 3712.16,\n",
       " 0.0,\n",
       " 129.09,\n",
       " 2563.3,\n",
       " 0.0,\n",
       " 3981.97,\n",
       " 145.5,\n",
       " 21.12,\n",
       " 760.11,\n",
       " 89.64,\n",
       " 222.12,\n",
       " 12197.0,\n",
       " 2672.73,\n",
       " 586.9,\n",
       " 0.0,\n",
       " 497.59,\n",
       " 3378.93,\n",
       " 381.64,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2827.52,\n",
       " 66.63,\n",
       " 1202.28,\n",
       " 7194.78,\n",
       " 178.53,\n",
       " 0.0,\n",
       " 639.97,\n",
       " 6491.29,\n",
       " 246.5,\n",
       " 0.0,\n",
       " 2601.8,\n",
       " 364.07,\n",
       " 785.55,\n",
       " 478.01,\n",
       " 757.99,\n",
       " 48.81,\n",
       " 797.12,\n",
       " 1587.11,\n",
       " 4878.39,\n",
       " 0.0,\n",
       " 2133.54,\n",
       " 4620.29,\n",
       " 34.02,\n",
       " 2109.42,\n",
       " 3262.42,\n",
       " 2535.85,\n",
       " 31.12,\n",
       " 11525.01,\n",
       " 3924.32,\n",
       " 500.38,\n",
       " 1836.05,\n",
       " 6982.06,\n",
       " 1783.5,\n",
       " 0.0,\n",
       " 4180.61,\n",
       " 3033.76,\n",
       " 3722.27,\n",
       " 2382.96,\n",
       " 1058.36,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1018.1,\n",
       " 77.24,\n",
       " 90.93,\n",
       " 169.54,\n",
       " 154.44,\n",
       " 0.0,\n",
       " 43.29,\n",
       " 851.47,\n",
       " 1415.73,\n",
       " 1855.01,\n",
       " 3536.9,\n",
       " 3080.38,\n",
       " 371.66,\n",
       " 0.0,\n",
       " 3219.55,\n",
       " 824.97,\n",
       " 77.88,\n",
       " 378.49,\n",
       " 2135.09,\n",
       " 720.09,\n",
       " 0.0,\n",
       " 15.13,\n",
       " 2262.3,\n",
       " 831.14,\n",
       " 1425.41,\n",
       " 1395.33,\n",
       " 0.0,\n",
       " 104.51,\n",
       " 244.24,\n",
       " 2122.04,\n",
       " 3211.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1328.35,\n",
       " 191.97,\n",
       " 3373.56,\n",
       " 1941.95,\n",
       " 196.11,\n",
       " 1962.13,\n",
       " 1011.9,\n",
       " 542.18,\n",
       " 1590.89,\n",
       " 2867.91,\n",
       " 2512.71,\n",
       " 702.17,\n",
       " 4072.33,\n",
       " 3885.99,\n",
       " 0.0,\n",
       " 1040.78,\n",
       " 2861.15,\n",
       " 3613.57,\n",
       " 614.5,\n",
       " 3450.19,\n",
       " 1218.76,\n",
       " 13.12,\n",
       " 1484.08,\n",
       " 3109.78,\n",
       " 1864.48,\n",
       " 2760.17,\n",
       " 1798.4,\n",
       " 747.5,\n",
       " 0.0,\n",
       " 51.02,\n",
       " 0.0,\n",
       " 1060.35,\n",
       " 174.48,\n",
       " 2052.57,\n",
       " 0.0,\n",
       " 707.82,\n",
       " 2390.76,\n",
       " 1127.99,\n",
       " 1448.85,\n",
       " 1023.39,\n",
       " 488.74,\n",
       " 5494.8,\n",
       " 4.98,\n",
       " 1918.49,\n",
       " 935.29,\n",
       " 6120.66,\n",
       " 1107.46,\n",
       " 3257.76,\n",
       " 473.31,\n",
       " 194.32,\n",
       " 7484.57,\n",
       " 4204.97,\n",
       " 561.49,\n",
       " 5148.45,\n",
       " 79.2,\n",
       " 0.0,\n",
       " 3251.34,\n",
       " 1381.34,\n",
       " 0.0,\n",
       " 161.97,\n",
       " 6285.81,\n",
       " 2161.48,\n",
       " 517.87,\n",
       " 0.0,\n",
       " 1724.38,\n",
       " 1124.65,\n",
       " 5766.15,\n",
       " 781.41,\n",
       " 453.37,\n",
       " 7365.46,\n",
       " 0.0,\n",
       " 405.34,\n",
       " 701.92,\n",
       " 102.22,\n",
       " 1040.55,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1167.06,\n",
       " 917.63,\n",
       " 1732.3,\n",
       " 31.36,\n",
       " 858.71,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 39.68,\n",
       " 0.0,\n",
       " 3107.32,\n",
       " 1601.55,\n",
       " 405.92,\n",
       " 8996.78,\n",
       " 0.0,\n",
       " 1454.9,\n",
       " 767.32,\n",
       " 157.84,\n",
       " 772.79,\n",
       " 482.58,\n",
       " 6792.19,\n",
       " 18452.97,\n",
       " 0.0,\n",
       " 4684.62,\n",
       " 146.82,\n",
       " 837.92,\n",
       " 407.07,\n",
       " 1135.64,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 392.66,\n",
       " 16.5,\n",
       " 1524.13,\n",
       " 983.17,\n",
       " 57.58,\n",
       " 0.0,\n",
       " 46.72,\n",
       " 0.0,\n",
       " 243.89,\n",
       " 147.58,\n",
       " 2095.83,\n",
       " 866.4,\n",
       " 7264.42,\n",
       " 441.2,\n",
       " 1099.01,\n",
       " 0.0,\n",
       " 847.7,\n",
       " 248.82,\n",
       " 2365.6,\n",
       " 1680.7,\n",
       " 3802.8,\n",
       " 5269.82,\n",
       " 8866.88,\n",
       " 669.37,\n",
       " 1805.99,\n",
       " 0.0,\n",
       " 2360.33,\n",
       " 1061.68,\n",
       " 138.68,\n",
       " 928.31,\n",
       " 645.71,\n",
       " 226.47,\n",
       " 0.0,\n",
       " 745.77,\n",
       " 1346.97,\n",
       " 682.25,\n",
       " 92.7,\n",
       " 125.3,\n",
       " 2659.34,\n",
       " 11.34,\n",
       " 640.76,\n",
       " 1069.22,\n",
       " 197.87,\n",
       " 315.45,\n",
       " 970.19,\n",
       " 0.0,\n",
       " 2316.52,\n",
       " 1651.3,\n",
       " 497.91,\n",
       " 1707.81,\n",
       " 5912.78,\n",
       " 3888.82,\n",
       " 944.29,\n",
       " 6.12,\n",
       " 1466.57,\n",
       " 2201.86,\n",
       " 9335.09,\n",
       " 421.08,\n",
       " 2514.54,\n",
       " 1414.13,\n",
       " 0.0,\n",
       " 271.62,\n",
       " 1765.46,\n",
       " 720.37,\n",
       " 1607.1,\n",
       " 108.42,\n",
       " 434.65,\n",
       " 0.0,\n",
       " 486.67,\n",
       " 0.0,\n",
       " 142.21,\n",
       " 255.9,\n",
       " 2994.48,\n",
       " 162.34,\n",
       " 1046.33,\n",
       " 2701.28,\n",
       " 659.82,\n",
       " 0.0,\n",
       " 197.05,\n",
       " 5226.35,\n",
       " 0.0,\n",
       " 4266.9,\n",
       " 5138.93,\n",
       " 0.0,\n",
       " 455.93,\n",
       " 648.91,\n",
       " 880.46,\n",
       " 346.92,\n",
       " 122.22,\n",
       " 1582.01,\n",
       " 2471.91,\n",
       " 2718.21,\n",
       " 1799.2,\n",
       " 10560.98,\n",
       " 277.76,\n",
       " 1311.97,\n",
       " 1975.36,\n",
       " 5305.08,\n",
       " 2137.32,\n",
       " 1063.18,\n",
       " 8090.58,\n",
       " 438.86,\n",
       " 0.0,\n",
       " 132.22,\n",
       " 811.3,\n",
       " 1294.75,\n",
       " 236.96,\n",
       " 5477.35,\n",
       " 0.0,\n",
       " 320.39,\n",
       " 1313.54,\n",
       " 830.4,\n",
       " 773.26,\n",
       " 2859.97,\n",
       " 430.44,\n",
       " 3457.1,\n",
       " 173.49,\n",
       " 147.27,\n",
       " 3000.92,\n",
       " 541.94,\n",
       " 17.12,\n",
       " 1229.55,\n",
       " 577.11,\n",
       " 0.0,\n",
       " 2475.09,\n",
       " 5310.95,\n",
       " 893.27,\n",
       " 1919.6,\n",
       " 778.71,\n",
       " 877.16,\n",
       " 191.88,\n",
       " 1352.95,\n",
       " 83.58,\n",
       " 762.14,\n",
       " 21.07,\n",
       " 1900.3,\n",
       " 4835.98,\n",
       " 51.56,\n",
       " 3882.23,\n",
       " 179.97,\n",
       " 0.0,\n",
       " 973.71,\n",
       " 3810.46,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2149.96,\n",
       " 380.2,\n",
       " 910.17,\n",
       " 2184.88,\n",
       " 2891.86,\n",
       " 1536.45,\n",
       " 89.95,\n",
       " 991.09,\n",
       " 1368.05,\n",
       " 654.59,\n",
       " 326.96,\n",
       " 4979.31,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2033.37,\n",
       " 819.15,\n",
       " 1770.78,\n",
       " 2174.16,\n",
       " 1859.16,\n",
       " 1369.48,\n",
       " 2190.81,\n",
       " 2906.4,\n",
       " 1340.24,\n",
       " 0.0,\n",
       " 2616.51,\n",
       " 1115.2,\n",
       " 4493.98,\n",
       " 561.54,\n",
       " 993.9,\n",
       " 4709.1,\n",
       " 731.52,\n",
       " 923.69,\n",
       " 1680.27,\n",
       " 204.54,\n",
       " 15.71,\n",
       " 496.11,\n",
       " 448.79,\n",
       " 14.78,\n",
       " 4225.12,\n",
       " 405.35,\n",
       " 4187.35,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4874.3,\n",
       " 767.91,\n",
       " 322.02,\n",
       " 632.19,\n",
       " 1656.87,\n",
       " 23.97,\n",
       " 743.05,\n",
       " 121.83,\n",
       " 635.6,\n",
       " 1115.2,\n",
       " 1283.51,\n",
       " 1688.47,\n",
       " 573.29,\n",
       " 661.35,\n",
       " 84.99,\n",
       " 546.92,\n",
       " 479.97,\n",
       " 1855.48,\n",
       " 2374.73,\n",
       " 1137.34,\n",
       " 2101.18,\n",
       " 425.89,\n",
       " 2043.13,\n",
       " 4751.08,\n",
       " 3839.31,\n",
       " 1232.0,\n",
       " 0.0,\n",
       " 418.06,\n",
       " 1322.14,\n",
       " 2286.05,\n",
       " 2384.0,\n",
       " 2698.17,\n",
       " 1992.99,\n",
       " ...]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series=pd.Series(traget_array_flatten).to_list()\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f841a9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520554d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180489e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
